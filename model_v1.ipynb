{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f73b370",
   "metadata": {},
   "source": [
    "Cell 0 — Notebook header, imports, environment print, config loader, seeding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5fd59725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ML ENVIRONMENT SUMMARY (runtime) ===\n",
      "                OS: nt / win32\n",
      "            Python: 3.12.11\n",
      "             Torch: 2.7.0+cu128\n",
      "    CUDA available: Yes\n",
      "      CUDA runtime: 12.8\n",
      "             cuDNN: 90701\n",
      "            Device: cuda\n",
      "       AMP enabled: Yes\n",
      "     AMP precision: bf16\n",
      "          GPU name: NVIDIA GeForce RTX 5070 Ti\n",
      "            GPU cc: 12.0\n",
      "     GPU mem (GiB): 15.92\n",
      "\n",
      "Config snapshot written to: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\resolved_config.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell 0: Header / Imports / Env print / Strict YAML config loader / Seeding ===\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, json, random\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict\n",
    "\n",
    "# Scientific stack\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# PyTorch (2.7.0+cu128; cuDNN 9.7.1; CUDA 12.8)\n",
    "import torch\n",
    "\n",
    "# We require YAML; no fallbacks.\n",
    "try:\n",
    "    import yaml  # pip install pyyaml\n",
    "except Exception as e:\n",
    "    raise ImportError(\"PyYAML is required. Install with: pip install pyyaml\") from e\n",
    "\n",
    "# ----------------- Project Layout (STRICT) -----------------\n",
    "# This notebook must live inside the 'Model_v1' folder.\n",
    "NB_ROOT = Path.cwd().resolve()\n",
    "expected_root_name = \"Model_v1\"\n",
    "if NB_ROOT.name != expected_root_name:\n",
    "    raise RuntimeError(\n",
    "        f\"This notebook must be run from inside '{expected_root_name}'\\n\"\n",
    "        f\"Current working directory is: {NB_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Mandatory config path\n",
    "CONFIG_PATH = NB_ROOT / \"configs\" / \"dti_adr_v1.yaml\"\n",
    "if not CONFIG_PATH.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Config file not found at {CONFIG_PATH}\\n\"\n",
    "        f\"Create it first under: {NB_ROOT / 'configs'}  (filename: dti_adr_v1.yaml)\"\n",
    "    )\n",
    "\n",
    "# ----------------- Load Config (YAML only) -----------------\n",
    "def load_cfg_yaml(path: Path) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        cfg = yaml.safe_load(f)\n",
    "    if not isinstance(cfg, dict):\n",
    "        raise ValueError(f\"Config at {path} is not a YAML mapping/dict.\")\n",
    "    return cfg\n",
    "\n",
    "cfg = load_cfg_yaml(CONFIG_PATH)\n",
    "\n",
    "# ----------------- Reproducibility -----------------\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed = int(cfg.get(\"run\", {}).get(\"seed\", 1337))\n",
    "set_seed(seed)\n",
    "\n",
    "# Determinism (slower but safer/reproducible)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Prefer high precision matmul on Ada; safe if not available.\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# AMP config\n",
    "amp_cfg = cfg.get(\"train\", {}).get(\"amp\", {})\n",
    "AMP_ENABLED = bool(amp_cfg.get(\"enabled\", True))\n",
    "AMP_PRECISION = str(amp_cfg.get(\"precision\", \"bf16\")).lower()\n",
    "AMP_DTYPE = (\n",
    "    torch.bfloat16 if AMP_PRECISION == \"bf16\"\n",
    "    else (torch.float16 if AMP_PRECISION == \"fp16\" else None)\n",
    ")\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "# ----------------- Pretty Environment Print -----------------\n",
    "def print_env_summary():\n",
    "    def yn(x): return \"Yes\" if x else \"No\"\n",
    "    print(\"=== ML ENVIRONMENT SUMMARY (runtime) ===\")\n",
    "    print(f\"{'OS':>18}: {os.name} / {sys.platform}\")\n",
    "    print(f\"{'Python':>18}: {sys.version.split()[0]}\")\n",
    "    print(f\"{'Torch':>18}: {torch.__version__}\")\n",
    "    print(f\"{'CUDA available':>18}: {yn(torch.cuda.is_available())}\")\n",
    "    print(f\"{'CUDA runtime':>18}: {torch.version.cuda}\")\n",
    "    print(f\"{'cuDNN':>18}: {torch.backends.cudnn.version()}\")\n",
    "    print(f\"{'Device':>18}: {DEVICE}\")\n",
    "    print(f\"{'AMP enabled':>18}: {yn(AMP_ENABLED)}\")\n",
    "    print(f\"{'AMP precision':>18}: {AMP_PRECISION if AMP_ENABLED else 'off'}\")\n",
    "    if torch.cuda.is_available():\n",
    "        props = torch.cuda.get_device_properties(0)\n",
    "        print(f\"{'GPU name':>18}: {props.name}\")\n",
    "        print(f\"{'GPU cc':>18}: {props.major}.{props.minor}\")\n",
    "        print(f\"{'GPU mem (GiB)':>18}: {props.total_memory/2**30:.2f}\")\n",
    "    # print(\"\\n=== CONFIG SNAPSHOT (key bits) ===\")\n",
    "    snap = {\n",
    "        \"run\": cfg.get(\"run\", {}),\n",
    "        \"model\": {\n",
    "            \"drug_encoder\": cfg.get(\"model\", {}).get(\"drug_encoder\"),\n",
    "            \"protein_encoder\": cfg.get(\"model\", {}).get(\"protein_encoder\"),\n",
    "            \"shared_dim\": cfg.get(\"model\", {}).get(\"shared_dim\"),\n",
    "            \"dti_head\": cfg.get(\"model\", {}).get(\"dti_head\"),\n",
    "            \"contrastive\": cfg.get(\"model\", {}).get(\"contrastive\"),\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"batch_size\": cfg.get(\"train\", {}).get(\"batch_size\"),\n",
    "            \"epochs\": cfg.get(\"train\", {}).get(\"epochs\"),\n",
    "            \"amp\": cfg.get(\"train\", {}).get(\"amp\"),\n",
    "        },\n",
    "        \"data\": {\n",
    "            \"K\": cfg.get(\"data\", {}).get(\"K\"),\n",
    "            \"adr_root\": cfg.get(\"data\", {}).get(\"adr_root\"),\n",
    "            \"dti_pairs_path\": cfg.get(\"data\", {}).get(\"dti_pairs_path\"),\n",
    "        }\n",
    "    }\n",
    "    # print(json.dumps(snap, indent=2))\n",
    "\n",
    "print_env_summary()\n",
    "\n",
    "# ----------------- Output directory & config snapshot -----------------\n",
    "OUT_DIR = Path(cfg[\"run\"][\"output_dir\"]).resolve()\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_DIR / \"resolved_config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(cfg, f, indent=2)\n",
    "print(f\"\\nConfig snapshot written to: {OUT_DIR / 'resolved_config.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb0b868",
   "metadata": {},
   "source": [
    "Cell 1 — Data inventory & schema checks (ADR artifacts + global stats, strict paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "3ad41ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ADR Artifacts ===\n",
      "IDF table        : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\1. Adr_embeddings\\idf_table.parquet  shape=(4048, 3)\n",
      "ADR index        : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\1. Adr_embeddings\\adr_index.parquet  rows=4817\n",
      "Drug index       : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\1. Adr_embeddings\\drug_index.parquet  rows=1028\n",
      "Global stats json: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\1. Adr_embeddings\\global_stats.json\n",
      "\n",
      "=== ADR split checks ===\n",
      "train | rows=   719 | preview=True | ADR-names==IDF? False | nnz=47047.0 | dens=0.016165\n",
      "  val | rows=   154 | preview=True | ADR-names==IDF? False | nnz=10324.0 | dens=0.016561\n",
      " test | rows=   155 | preview=True | ADR-names==IDF? False | nnz=11222.0 | dens=0.017885\n",
      "\n",
      "All ADR artifacts look consistent ✅\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: Data inventory & schema checks (ADR / indices / stats) ===\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "def _read_json(path: Path) -> dict:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def _read_parquet(path: Path) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_parquet(path)  # requires pyarrow or fastparquet\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(\n",
    "            f\"Failed to read parquet at {path}. \"\n",
    "            f\"Install pyarrow: pip install pyarrow\\nOriginal error: {e}\"\n",
    "        )\n",
    "\n",
    "# Resolve ADR paths\n",
    "adr_root = Path(cfg[\"data\"][\"adr_root\"]).resolve()\n",
    "adr_files = cfg[\"data\"][\"adr_files\"]\n",
    "p_idf = adr_root / adr_files[\"idf_table\"]\n",
    "p_adr_index = adr_root / adr_files[\"adr_index\"]\n",
    "p_drug_index = adr_root / adr_files[\"drug_index\"]\n",
    "p_global_stats = adr_root / adr_files[\"global_stats\"]\n",
    "\n",
    "# Check existence\n",
    "for p in [p_idf, p_adr_index, p_drug_index, p_global_stats]:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing required ADR artifact: {p}\")\n",
    "\n",
    "# Load artifacts\n",
    "idf_table = _read_parquet(p_idf)\n",
    "adr_index = _read_parquet(p_adr_index)\n",
    "drug_index = _read_parquet(p_drug_index)\n",
    "global_stats = _read_json(p_global_stats)\n",
    "\n",
    "# Sanity prints\n",
    "print(\"=== ADR Artifacts ===\")\n",
    "print(f\"IDF table        : {p_idf}  shape={idf_table.shape}\")\n",
    "print(f\"ADR index        : {p_adr_index}  rows={len(adr_index)}\")\n",
    "print(f\"Drug index       : {p_drug_index}  rows={len(drug_index)}\")\n",
    "print(f\"Global stats json: {p_global_stats}\")\n",
    "\n",
    "# Validate K (number of ADR columns to keep)\n",
    "K_cfg = int(cfg[\"data\"][\"K\"])\n",
    "K_idf = int(len(idf_table))\n",
    "if K_cfg != K_idf:\n",
    "    raise ValueError(f\"K mismatch: cfg K={K_cfg}, idf_table rows={K_idf}. \"\n",
    "                     \"Ensure config matches ADR column space.\")\n",
    "\n",
    "# Inspect per-split stats.json to ensure TF-IDF column order stability and density\n",
    "split_dirs = [adr_root / \"train\", adr_root / \"val\", adr_root / \"test\"]\n",
    "SPLIT_INFO = {}\n",
    "\n",
    "for sd in split_dirs:\n",
    "    if not sd.exists():\n",
    "        raise FileNotFoundError(f\"Missing split directory: {sd}\")\n",
    "\n",
    "    stats_path = sd / \"stats.json\"\n",
    "    tfidf_wide_path = sd / \"tfidf_wide.parquet\"\n",
    "    preview_path = sd / \"preview_top_tfidf.parquet\"\n",
    "\n",
    "    if not stats_path.exists() or not tfidf_wide_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing files under {sd}: need stats.json and tfidf_wide.parquet\")\n",
    "\n",
    "    # Load small stats for quick checks\n",
    "    s = _read_json(stats_path)\n",
    "    df_wide = _read_parquet(tfidf_wide_path)\n",
    "\n",
    "    # Column order check against IDF table\n",
    "    # Assume idf_table has a column that defines sorted ADR keys; use the first column name as ADR id\n",
    "    idf_adr_col = idf_table.columns[0]\n",
    "    idf_keys = idf_table[idf_adr_col].tolist() if idf_adr_col != 0 else idf_table.iloc[:,0].tolist()\n",
    "\n",
    "    # tfidf_wide should have the exact K ADR columns in the same order, aside from its ID column\n",
    "    wide_cols = list(df_wide.columns)\n",
    "    # Heuristic: first column may be an ID (e.g., rxcui or drug_chembl_id). Detect & strip if non-numeric/non-adr.\n",
    "    if len(wide_cols) != K_cfg and len(wide_cols) == K_cfg + 1:\n",
    "        adr_cols = wide_cols[1:]\n",
    "        id_col = wide_cols[0]\n",
    "    elif len(wide_cols) == K_cfg:\n",
    "        adr_cols = wide_cols\n",
    "        id_col = None\n",
    "    else:\n",
    "        raise ValueError(f\"{tfidf_wide_path} columns={len(wide_cols)} not matching expected K (={K_cfg}) or K+1.\")\n",
    "\n",
    "    # If the ADR column names match the IDF keys exactly, good; otherwise warn.\n",
    "    # (Some pipelines store ADR columns as positional indices 0..K-1. In that case, we skip strict name check.)\n",
    "    name_match = False\n",
    "    if isinstance(adr_cols[0], str) and isinstance(idf_keys[0], (str, int)):\n",
    "        # Attempt direct name match\n",
    "        name_match = (adr_cols == idf_keys)\n",
    "    else:\n",
    "        name_match = True  # Can't compare reliably; assume positional match\n",
    "\n",
    "    # Density stats\n",
    "    nnz = float(s.get(\"nnz\", -1))\n",
    "    density = float(s.get(\"density\", -1.0))\n",
    "    n_rows = int(s.get(\"n_rows\", len(df_wide)))\n",
    "    SPLIT_INFO[sd.name] = {\n",
    "        \"n_rows\": n_rows,\n",
    "        \"has_preview\": (sd / \"preview_top_tfidf.parquet\").exists(),\n",
    "        \"id_col\": id_col,\n",
    "        \"adr_name_match_idf\": name_match,\n",
    "        \"nnz\": nnz,\n",
    "        \"density\": density,\n",
    "    }\n",
    "\n",
    "# Summarize\n",
    "print(\"\\n=== ADR split checks ===\")\n",
    "for k, v in SPLIT_INFO.items():\n",
    "    print(f\"{k:>5} | rows={v['n_rows']:>6} | preview={v['has_preview']} | \"\n",
    "          f\"ADR-names==IDF? {v['adr_name_match_idf']} | nnz={v['nnz']} | dens={v['density']:.6f}\")\n",
    "\n",
    "print(\"\\nAll ADR artifacts look consistent ✅\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb5752a",
   "metadata": {},
   "source": [
    "Cell 2 — Load chosen Drug & Protein embeddings (dtype-safe, shape-checked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "1e80e3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded DRUG embeddings from: smiles_embeddings_chemberta.parquet\n",
      "  IDs: column='drug_chembl_id', unique=1028\n",
      "  Tensor shape: (1028, 384) (dtype=torch.float32)\n",
      "\n",
      "Loaded PROTEIN embeddings from: GVP-GNN_protein_embeddings.parquet\n",
      "  IDs: column='uniprot_id', unique=2385\n",
      "  Tensor shape: (2385, 1024) (dtype=torch.float32)\n",
      "\n",
      "=== Embedding Dimensionalities ===\n",
      "  Drug encoder 'chemberta': N=1028, D=384\n",
      "  Protein encoder 'gvp': N=2385, D=1024\n",
      "\n",
      "[Note] AMP bf16 selected. Keep model weights & losses in fp32; autocast forward only.\n",
      "\n",
      "Embedding loaders ready ✅\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: Load chosen Drug & Protein embeddings (dtype-safe, shape-checked) ===\n",
    "\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Expect config keys:\n",
    "#   cfg[\"model\"][\"drug_encoder\"] in {\"chemberta\", \"smiles2vec\", \"egnn\"}\n",
    "#   cfg[\"model\"][\"protein_encoder\"] in {\"esm\", \"gvp\"}\n",
    "\n",
    "# ---------- Helper: parquet reader (pyarrow already checked in Cell 1) ----------\n",
    "def read_parquet_strict(p: Path) -> pd.DataFrame:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(f\"Missing embedding parquet: {p}\")\n",
    "    try:\n",
    "        return pd.read_parquet(p)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to read parquet at {p}\\n{e}\")\n",
    "\n",
    "# ---------- Helper: coerce 'embedding' column → 2D float32 array ----------\n",
    "def coerce_embedding_column(df: pd.DataFrame, col: str = \"embedding\") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Coerce a DataFrame's 'embedding' column into a 2D np.ndarray[float32]\n",
    "    Handles cases where dtype=object with lists/ndarrays, or JSON-like strings.\n",
    "    \"\"\"\n",
    "    if col not in df.columns:\n",
    "        raise KeyError(f\"Expected embedding column '{col}' not found in columns: {list(df.columns)[:8]} ...\")\n",
    "\n",
    "    raw = df[col].values\n",
    "    # If first element is already a numpy array\n",
    "    if isinstance(raw[0], np.ndarray):\n",
    "        arr = np.stack(raw, axis=0)\n",
    "    else:\n",
    "        # Could be python lists or strings like \"[1.2, -0.3, ...]\"\n",
    "        processed = []\n",
    "        for i, v in enumerate(raw):\n",
    "            if isinstance(v, list):\n",
    "                processed.append(np.asarray(v, dtype=np.float32))\n",
    "                continue\n",
    "            if isinstance(v, (bytes, bytearray)):\n",
    "                v = v.decode(\"utf-8\")\n",
    "            if isinstance(v, str):\n",
    "                # Attempt to parse quick JSON-ish arrays without the cost of full json if possible\n",
    "                v_str = v.strip()\n",
    "                if v_str.startswith(\"[\") and v_str.endswith(\"]\"):\n",
    "                    try:\n",
    "                        parsed = json.loads(v_str)\n",
    "                        processed.append(np.asarray(parsed, dtype=np.float32))\n",
    "                        continue\n",
    "                    except Exception as e:\n",
    "                        raise ValueError(f\"Row {i}: failed to parse string embedding JSON: {e}\")\n",
    "                else:\n",
    "                    raise TypeError(f\"Row {i}: string embedding not in JSON list format.\")\n",
    "            # Fallback if someone stored tuples or other iterables\n",
    "            try:\n",
    "                processed.append(np.asarray(v, dtype=np.float32))\n",
    "            except Exception as e:\n",
    "                raise TypeError(f\"Row {i}: cannot coerce embedding to float32 array. Got type={type(v)}; err={e}\")\n",
    "        arr = np.stack(processed, axis=0)\n",
    "\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Embedding array must be 2D [N, D]; got shape {arr.shape}\")\n",
    "    # Ensure float32 and contiguous\n",
    "    if arr.dtype != np.float32:\n",
    "        arr = arr.astype(np.float32, copy=False)\n",
    "    return np.ascontiguousarray(arr)\n",
    "\n",
    "# ---------- Helper: pick ID column names ----------\n",
    "def detect_drug_id_column(df: pd.DataFrame) -> str:\n",
    "    # Strict expectation from your dir summary: \"drug_chembl_id\"\n",
    "    if \"drug_chembl_id\" in df.columns:\n",
    "        return \"drug_chembl_id\"\n",
    "    # Fallbacks (in case of future variants)\n",
    "    for c in [\"chembl_id\", \"drug_id\", \"id\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"Could not find a drug ID column in {list(df.columns)}\")\n",
    "\n",
    "def detect_protein_id_column(df: pd.DataFrame) -> str:\n",
    "    # ESM file uses \"id\", GVP uses \"uniprot_id\" — support both.\n",
    "    if \"uniprot_id\" in df.columns:\n",
    "        return \"uniprot_id\"\n",
    "    if \"id\" in df.columns:\n",
    "        return \"id\"\n",
    "    # Conservative fallback\n",
    "    for c in [\"protein_id\", \"target_uniprot_id\"]:\n",
    "        if c in df.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"Could not find a protein ID column in {list(df.columns)}\")\n",
    "\n",
    "# ---------- Load Drug Embeddings ----------\n",
    "drug_choice = str(cfg[\"model\"][\"drug_encoder\"]).lower()\n",
    "drug_paths = cfg[\"data\"][\"drug_embeddings\"]\n",
    "if drug_choice not in drug_paths:\n",
    "    raise KeyError(f\"Unknown drug_encoder '{drug_choice}'. Available: {list(drug_paths.keys())}\")\n",
    "\n",
    "p_drug = Path(drug_paths[drug_choice])\n",
    "if not p_drug.is_absolute():\n",
    "    p_drug = (NB_ROOT / p_drug).resolve()\n",
    "\n",
    "df_drug = read_parquet_strict(p_drug)\n",
    "drug_id_col = detect_drug_id_column(df_drug)\n",
    "drug_vecs = coerce_embedding_column(df_drug, col=\"embedding\")\n",
    "N_drug, D_drug = drug_vecs.shape\n",
    "\n",
    "# Deduplicate by ID if necessary (keep first occurrence)\n",
    "if df_drug[drug_id_col].duplicated().any():\n",
    "    keep_mask = ~df_drug[drug_id_col].duplicated()\n",
    "    df_drug = df_drug.loc[keep_mask].reset_index(drop=True)\n",
    "    drug_vecs = drug_vecs[keep_mask.values]\n",
    "    N_drug, D_drug = drug_vecs.shape\n",
    "\n",
    "# Build CPU tensors (we'll move to GPU during training to avoid long-lived CUDA mem)\n",
    "DRUG_ID_LIST = df_drug[drug_id_col].astype(str).tolist()\n",
    "DRUG_TENSOR = torch.from_numpy(drug_vecs)  # float32, CPU\n",
    "assert DRUG_TENSOR.dtype == torch.float32 and DRUG_TENSOR.ndim == 2\n",
    "\n",
    "# Fast lookup map: chembl_id -> row index\n",
    "DRUG_ID2IDX = {k: i for i, k in enumerate(DRUG_ID_LIST)}\n",
    "\n",
    "print(f\"Loaded DRUG embeddings from: {p_drug.name}\")\n",
    "print(f\"  IDs: column='{drug_id_col}', unique={len(DRUG_ID_LIST)}\")\n",
    "print(f\"  Tensor shape: {tuple(DRUG_TENSOR.shape)} (dtype={DRUG_TENSOR.dtype})\")\n",
    "\n",
    "# ---------- Load Protein Embeddings ----------\n",
    "prot_choice = str(cfg[\"model\"][\"protein_encoder\"]).lower()\n",
    "prot_paths = cfg[\"data\"][\"protein_embeddings\"]\n",
    "if prot_choice not in prot_paths:\n",
    "    raise KeyError(f\"Unknown protein_encoder '{prot_choice}'. Available: {list(prot_paths.keys())}\")\n",
    "\n",
    "p_prot = Path(prot_paths[prot_choice])\n",
    "if not p_prot.is_absolute():\n",
    "    p_prot = (NB_ROOT / p_prot).resolve()\n",
    "\n",
    "df_prot = read_parquet_strict(p_prot)\n",
    "prot_id_col = detect_protein_id_column(df_prot)\n",
    "prot_vecs = coerce_embedding_column(df_prot, col=\"embedding\")\n",
    "N_prot, D_prot = prot_vecs.shape\n",
    "\n",
    "# Deduplicate by ID if necessary\n",
    "if df_prot[prot_id_col].duplicated().any():\n",
    "    keep_mask = ~df_prot[prot_id_col].duplicated()\n",
    "    df_prot = df_prot.loc[keep_mask].reset_index(drop=True)\n",
    "    prot_vecs = prot_vecs[keep_mask.values]\n",
    "    N_prot, D_prot = prot_vecs.shape\n",
    "\n",
    "PROT_ID_LIST = df_prot[prot_id_col].astype(str).tolist()\n",
    "PROT_TENSOR = torch.from_numpy(prot_vecs)  # float32, CPU\n",
    "assert PROT_TENSOR.dtype == torch.float32 and PROT_TENSOR.ndim == 2\n",
    "\n",
    "PROT_ID2IDX = {k: i for i, k in enumerate(PROT_ID_LIST)}\n",
    "\n",
    "print(f\"\\nLoaded PROTEIN embeddings from: {p_prot.name}\")\n",
    "print(f\"  IDs: column='{prot_id_col}', unique={len(PROT_ID_LIST)}\")\n",
    "print(f\"  Tensor shape: {tuple(PROT_TENSOR.shape)} (dtype={PROT_TENSOR.dtype})\")\n",
    "\n",
    "# ---------- Shape sanity & warnings ----------\n",
    "if N_drug != len(set(DRUG_ID_LIST)):\n",
    "    raise AssertionError(\"Drug IDs are not unique after deduplication.\")\n",
    "if N_prot != len(set(PROT_ID_LIST)):\n",
    "    raise AssertionError(\"Protein IDs are not unique after deduplication.\")\n",
    "\n",
    "print(\"\\n=== Embedding Dimensionalities ===\")\n",
    "print(f\"  Drug encoder '{drug_choice}': N={N_drug}, D={D_drug}\")\n",
    "print(f\"  Protein encoder '{prot_choice}': N={N_prot}, D={D_prot}\")\n",
    "\n",
    "# Store global dims for later modules (adapters)\n",
    "DRUG_IN_DIM = int(D_drug)\n",
    "PROT_IN_DIM = int(D_prot)\n",
    "\n",
    "# ---------- AMP & dtype guidance (informational) ----------\n",
    "if AMP_ENABLED and AMP_DTYPE is torch.float16:\n",
    "    print(\"\\n[Note] AMP fp16 selected. On Ada, bf16 is often more stable than fp16.\")\n",
    "elif AMP_ENABLED and AMP_DTYPE is torch.bfloat16:\n",
    "    print(\"\\n[Note] AMP bf16 selected. Keep model weights & losses in fp32; autocast forward only.\")\n",
    "\n",
    "print(\"\\nEmbedding loaders ready ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a468f388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-standardization norms: drug mean=18.740  prot mean=0.997\n"
     ]
    }
   ],
   "source": [
    "# === Patch: sanitize + standardize embedding tensors (run after Cell 2) ===\n",
    "import torch\n",
    "\n",
    "def _sanitize_and_standardize(T: torch.Tensor, clip_sigma: float = 5.0):\n",
    "    \"\"\"\n",
    "    - Replace non-finite values with per-dim median\n",
    "    - Z-score standardize per dimension\n",
    "    - Clip to ±clip_sigma to tame extreme outliers\n",
    "    Returns standardized tensor and (mu, sd) for logging.\n",
    "    \"\"\"\n",
    "    T = T.clone().to(torch.float32, non_blocking=True)\n",
    "    # non-finite → median per column\n",
    "    nonfinite = ~torch.isfinite(T)\n",
    "    if nonfinite.any():\n",
    "        # compute column-wise medians with nan handling\n",
    "        X = T.masked_fill(nonfinite, float('nan'))\n",
    "        med = torch.nanmedian(X, dim=0).values\n",
    "        row_idx, col_idx = nonfinite.nonzero(as_tuple=True)\n",
    "        T[row_idx, col_idx] = med[col_idx]\n",
    "\n",
    "    mu = T.mean(dim=0)\n",
    "    sd = T.std(dim=0).clamp_min(1e-6)\n",
    "    Z = (T - mu) / sd\n",
    "    Z = Z.clamp_(-clip_sigma, clip_sigma).contiguous()\n",
    "    return Z, mu, sd\n",
    "\n",
    "# Apply to both embedding banks\n",
    "DRUG_TENSOR, DRUG_MU, DRUG_SD = _sanitize_and_standardize(DRUG_TENSOR)\n",
    "PROT_TENSOR, PROT_MU, PROT_SD = _sanitize_and_standardize(PROT_TENSOR)\n",
    "\n",
    "# Quick norms after standardization (should be ~sqrt(D) in L2 but z-scored per-dim)\n",
    "print(\"Post-standardization norms:\",\n",
    "      f\"drug mean={DRUG_TENSOR.norm(dim=1).mean().item():.3f} \",\n",
    "      f\"prot mean={PROT_TENSOR.norm(dim=1).mean().item():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47750d4c",
   "metadata": {},
   "source": [
    "Cell 3 — DTI pairs + ADR TF-IDF split loaders, Dataset, BalancedSampler, Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "297275d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DTI pairs (global) ===\n",
      "  drug_chembl_id target_uniprot_id  label  rxcui\n",
      "0     CHEMBL1000            O15245      0  20610\n",
      "1     CHEMBL1000            P08183      1  20610\n",
      "2     CHEMBL1000            P35367      1  20610\n",
      "Pairs total: 34741 | pos=12234 | neg=22507\n",
      "\n",
      "=== TF-IDF splits ===\n",
      "train: U=719  T.shape=(719, 4048) dtype=torch.float32\n",
      "  val: U=154    T.shape=(154, 4048) dtype=torch.float32\n",
      " test: U=155   T.shape=(155, 4048) dtype=torch.float32\n",
      "\n",
      "=== Split-aligned DTI pairs ===\n",
      "train: n=22310 | pos=8439 | neg=13871\n",
      "  val: n=4835 | pos=1861 | neg=2974\n",
      " test: n=7596 | pos=1934 | neg=5662\n",
      "\n",
      "Datasets ready ✅\n",
      "  train: 22310 rows\n",
      "    val: 4835 rows\n",
      "   test: 7596 rows\n",
      "pos_weight (scaled) = 2.4655\n",
      "\n",
      "Collate, datasets, and sampler scaffolding are ready ✅\n",
      "Next step: define adapters + heads + contrastives and wire the training loop.\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: Load DTI pairs + ADR TF-IDF, build datasets, balanced sampler, collate ===\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, Sampler\n",
    "\n",
    "# ---------- Strict paths ----------\n",
    "pairs_path = (NB_ROOT / cfg[\"data\"][\"dti_pairs_path\"]).resolve() \\\n",
    "    if not Path(cfg[\"data\"][\"dti_pairs_path\"]).is_absolute() else Path(cfg[\"data\"][\"dti_pairs_path\"])\n",
    "if not pairs_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing DTI pairs file: {pairs_path}\")\n",
    "\n",
    "# ---------- Load DTI pairs (full) ----------\n",
    "pairs_df = pd.read_parquet(pairs_path)\n",
    "required_cols = [\"drug_chembl_id\", \"target_uniprot_id\", \"label\", \"rxcui\"]\n",
    "missing = [c for c in required_cols if c not in pairs_df.columns]\n",
    "if missing:\n",
    "    raise KeyError(f\"DTI pairs parquet missing columns: {missing}\")\n",
    "\n",
    "# Normalize dtypes to string for id keys; label to int64\n",
    "pairs_df[\"drug_chembl_id\"] = pairs_df[\"drug_chembl_id\"].astype(str)\n",
    "pairs_df[\"target_uniprot_id\"] = pairs_df[\"target_uniprot_id\"].astype(str)\n",
    "pairs_df[\"rxcui\"] = pairs_df[\"rxcui\"].astype(str)\n",
    "pairs_df[\"label\"] = pairs_df[\"label\"].astype(np.int64)\n",
    "\n",
    "print(\"=== DTI pairs (global) ===\")\n",
    "print(pairs_df[required_cols].head(3))\n",
    "print(f\"Pairs total: {len(pairs_df)} | pos={(pairs_df['label']==1).sum()} | neg={(pairs_df['label']==0).sum()}\")\n",
    "\n",
    "# ---------- ADR TF-IDF loaders ----------\n",
    "def load_tfidf_split(split_name: str) -> Tuple[List[str], torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      rxcui_list: list[str] of length U_split\n",
    "      tfidf_tensor: torch.FloatTensor [U_split, K]\n",
    "    \"\"\"\n",
    "    assert split_name in (\"train\", \"val\", \"test\")\n",
    "    sd = (NB_ROOT / cfg[\"data\"][\"adr_root\"] / split_name).resolve()\n",
    "    tfidf_wide_path = sd / \"tfidf_wide.parquet\"\n",
    "    if not tfidf_wide_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing TF-IDF matrix at {tfidf_wide_path}\")\n",
    "\n",
    "    df_wide = pd.read_parquet(tfidf_wide_path)\n",
    "\n",
    "    # Detect ID column (first col often ID). Expect K or K+1 columns.\n",
    "    K = int(cfg[\"data\"][\"K\"])\n",
    "    cols = list(df_wide.columns)\n",
    "    if len(cols) == K:\n",
    "        # No explicit ID column — use drug_index.parquet to map, but your pipeline stores ID in col 0 in practice.\n",
    "        # Enforce the safer convention: require K+1 with an ID column.\n",
    "        raise ValueError(\n",
    "            f\"{tfidf_wide_path} has exactly K columns (K={K}) but no ID column to align by rxcui.\\n\"\n",
    "            f\"Expected K+1 with ID in column 0.\"\n",
    "        )\n",
    "    elif len(cols) == K + 1:\n",
    "        id_col = cols[0]\n",
    "        adr_cols = cols[1:]\n",
    "    else:\n",
    "        raise ValueError(f\"{tfidf_wide_path} columns={len(cols)} not matching expected K+1 (K={K}).\")\n",
    "\n",
    "    # Build rxcui list and TF-IDF matrix (float32, contiguous)\n",
    "    rxcui_list = df_wide[id_col].astype(str).tolist()\n",
    "    tfidf_mat = df_wide[adr_cols].to_numpy(dtype=np.float32, copy=False)\n",
    "    if tfidf_mat.ndim != 2 or tfidf_mat.shape[1] != K:\n",
    "        raise AssertionError(f\"TF-IDF shape mismatch: got {tfidf_mat.shape}, expected [U,{K}]\")\n",
    "    tfidf_tensor = torch.from_numpy(np.ascontiguousarray(tfidf_mat, dtype=np.float32))\n",
    "    return rxcui_list, tfidf_tensor\n",
    "\n",
    "R_train, T_train = load_tfidf_split(\"train\")\n",
    "R_val,   T_val   = load_tfidf_split(\"val\")\n",
    "R_test,  T_test  = load_tfidf_split(\"test\")\n",
    "\n",
    "print(\"\\n=== TF-IDF splits ===\")\n",
    "print(f\"train: U={len(R_train)}  T.shape={tuple(T_train.shape)} dtype={T_train.dtype}\")\n",
    "print(f\"  val: U={len(R_val)}    T.shape={tuple(T_val.shape)} dtype={T_val.dtype}\")\n",
    "print(f\" test: U={len(R_test)}   T.shape={tuple(T_test.shape)} dtype={T_test.dtype}\")\n",
    "\n",
    "# rxcui -> row index for each split\n",
    "R2I_train = {r: i for i, r in enumerate(R_train)}\n",
    "R2I_val   = {r: i for i, r in enumerate(R_val)}\n",
    "R2I_test  = {r: i for i, r in enumerate(R_test)}\n",
    "\n",
    "# ---------- Build split-specific DTI datasets ----------\n",
    "# We align pairs to a split if their rxcui exists in that split's TF-IDF table.\n",
    "\n",
    "def filter_pairs_for_split(pairs: pd.DataFrame, split: str) -> pd.DataFrame:\n",
    "    if split == \"train\":\n",
    "        ok = pairs[\"rxcui\"].isin(R2I_train)\n",
    "    elif split == \"val\":\n",
    "        ok = pairs[\"rxcui\"].isin(R2I_val)\n",
    "    elif split == \"test\":\n",
    "        ok = pairs[\"rxcui\"].isin(R2I_test)\n",
    "    else:\n",
    "        raise ValueError(split)\n",
    "    df = pairs.loc[ok, required_cols].copy().reset_index(drop=True)\n",
    "    # Also drop pairs whose drug/protein embeddings are missing\n",
    "    df = df[df[\"drug_chembl_id\"].isin(DRUG_ID2IDX)].copy()\n",
    "    df = df[df[\"target_uniprot_id\"].isin(PROT_ID2IDX)].copy()\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "pairs_train = filter_pairs_for_split(pairs_df, \"train\")\n",
    "pairs_val   = filter_pairs_for_split(pairs_df, \"val\")\n",
    "pairs_test  = filter_pairs_for_split(pairs_df, \"test\")\n",
    "\n",
    "def pos_neg_counts(df: pd.DataFrame) -> Tuple[int, int]:\n",
    "    pos = int((df[\"label\"] == 1).sum())\n",
    "    neg = int((df[\"label\"] == 0).sum())\n",
    "    return pos, neg\n",
    "\n",
    "print(\"\\n=== Split-aligned DTI pairs ===\")\n",
    "for name, df in [(\"train\", pairs_train), (\"val\", pairs_val), (\"test\", pairs_test)]:\n",
    "    pos, neg = pos_neg_counts(df)\n",
    "    print(f\"{name:>5}: n={len(df)} | pos={pos} | neg={neg}\")\n",
    "\n",
    "# ---------- Dataset definition ----------\n",
    "@dataclass\n",
    "class PairRow:\n",
    "    drug_idx: int\n",
    "    prot_idx: int\n",
    "    label: int\n",
    "    rxcui: str\n",
    "\n",
    "class DTIDataset(Dataset):\n",
    "    def __init__(self, df_pairs: pd.DataFrame, rxcui2idx: Dict[str, int]):\n",
    "        self.rows: List[PairRow] = []\n",
    "        self.rxcui2idx = rxcui2idx\n",
    "        # Build rows\n",
    "        for _, row in df_pairs.iterrows():\n",
    "            d_id = row[\"drug_chembl_id\"]\n",
    "            p_id = row[\"target_uniprot_id\"]\n",
    "            y    = int(row[\"label\"])\n",
    "            rx   = row[\"rxcui\"]\n",
    "            # map to indices (already filtered, but assert anyway)\n",
    "            if d_id not in DRUG_ID2IDX or p_id not in PROT_ID2IDX or rx not in rxcui2idx:\n",
    "                continue\n",
    "            self.rows.append(PairRow(\n",
    "                drug_idx=DRUG_ID2IDX[d_id],\n",
    "                prot_idx=PROT_ID2IDX[p_id],\n",
    "                label=y,\n",
    "                rxcui=rx\n",
    "            ))\n",
    "        if len(self.rows) == 0:\n",
    "            raise ValueError(\"Empty dataset after alignment. Check IDs & TF-IDF splits.\")\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> PairRow:\n",
    "        return self.rows[idx]\n",
    "\n",
    "# Instantiate datasets\n",
    "ds_train = DTIDataset(pairs_train, R2I_train)\n",
    "ds_val   = DTIDataset(pairs_val,   R2I_val)\n",
    "ds_test  = DTIDataset(pairs_test,  R2I_test)\n",
    "\n",
    "print(\"\\nDatasets ready ✅\")\n",
    "print(f\"  train: {len(ds_train)} rows\")\n",
    "print(f\"    val: {len(ds_val)} rows\")\n",
    "print(f\"   test: {len(ds_test)} rows\")\n",
    "\n",
    "# ---------- Balanced Batch Sampler ----------\n",
    "# class BalancedBatchSampler(Sampler[List[int]]):\n",
    "#     \"\"\"\n",
    "#     Yields lists of indices for balanced batches (B/2 pos, B/2 neg).\n",
    "#     With replacement where necessary.\n",
    "#     \"\"\"\n",
    "#     def __init__(self, dataset: DTIDataset, batch_size: int):\n",
    "#         assert batch_size % 2 == 0, \"Batch size must be even for balanced sampling.\"\n",
    "#         self.ds = dataset\n",
    "#         self.batch_size = batch_size\n",
    "#         # Precompute pos/neg index pools\n",
    "#         self.pos_idx = [i for i, r in enumerate(self.ds.rows) if r.label == 1]\n",
    "#         self.neg_idx = [i for i, r in enumerate(self.ds.rows) if r.label == 0]\n",
    "#         if len(self.pos_idx) == 0 or len(self.neg_idx) == 0:\n",
    "#             raise ValueError(\"Balanced sampler requires both positive and negative samples.\")\n",
    "#         self.n_batches = math.ceil(len(self.ds) / self.batch_size)\n",
    "\n",
    "#     def __len__(self) -> int:\n",
    "#         return self.n_batches\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         B2 = self.batch_size // 2\n",
    "#         for _ in range(self.n_batches):\n",
    "#             # Sample with replacement if pools smaller than needed\n",
    "#             pos = np.random.choice(self.pos_idx, size=B2, replace=(len(self.pos_idx) < B2))\n",
    "#             neg = np.random.choice(self.neg_idx, size=B2, replace=(len(self.neg_idx) < B2))\n",
    "#             batch = np.concatenate([pos, neg])\n",
    "#             np.random.shuffle(batch)\n",
    "#             yield batch.tolist()\n",
    "\n",
    "class BalancedBatchSampler(Sampler[List[int]]):\n",
    "    def __init__(self, dataset: DTIDataset, batch_size: int, pos_frac: float = 0.65):\n",
    "        assert 0.0 < pos_frac < 1.0\n",
    "        assert batch_size % 2 == 0, \"Batch size must be even.\"\n",
    "        self.ds = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.pos_frac = pos_frac\n",
    "        self.pos_idx = [i for i, r in enumerate(self.ds.rows) if r.label == 1]\n",
    "        self.neg_idx = [i for i, r in enumerate(self.ds.rows) if r.label == 0]\n",
    "        if not self.pos_idx or not self.neg_idx:\n",
    "            raise ValueError(\"Need both pos and neg.\")\n",
    "        self.n_batches = math.ceil(len(self.ds) / self.batch_size)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return self.n_batches\n",
    "\n",
    "    def __iter__(self):\n",
    "        Bp = int(round(self.batch_size * self.pos_frac))\n",
    "        Bn = self.batch_size - Bp\n",
    "        for _ in range(self.n_batches):\n",
    "            pos = np.random.choice(self.pos_idx, size=Bp, replace=(len(self.pos_idx) < Bp))\n",
    "            neg = np.random.choice(self.neg_idx, size=Bn, replace=(len(self.neg_idx) < Bn))\n",
    "            batch = np.concatenate([pos, neg])\n",
    "            np.random.shuffle(batch)\n",
    "            yield batch.tolist()\n",
    "\n",
    "# ---------- pos_weight for BCE ----------\n",
    "def compute_pos_weight(dataset: DTIDataset) -> torch.Tensor:\n",
    "    y = np.array([r.label for r in dataset.rows], dtype=np.int64)\n",
    "    n_pos = int((y == 1).sum())\n",
    "    n_neg = int((y == 0).sum())\n",
    "    if n_pos == 0:\n",
    "        raise ValueError(\"No positive samples in dataset.\")\n",
    "    pw = float(n_neg) / float(n_pos)\n",
    "    return torch.tensor(pw, dtype=torch.float32)\n",
    "\n",
    "POS_WEIGHT_TRAIN = compute_pos_weight(ds_train)\n",
    "# print(f\"\\npos_weight (train) = {POS_WEIGHT_TRAIN.item():.4f}\")\n",
    "PW_SCALE = 1.5  # try 1.2–2.0\n",
    "POS_WEIGHT_TRAIN = POS_WEIGHT_TRAIN * PW_SCALE\n",
    "print(f\"pos_weight (scaled) = {POS_WEIGHT_TRAIN.item():.4f}\")\n",
    "\n",
    "# ---------- Collate function ----------\n",
    "def collate_pairs_with_unique_drugs(\n",
    "    rows: List[PairRow],\n",
    "    split: str\n",
    ") -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x_d [B, D_drug] float32 (CPU)\n",
    "      x_p [B, D_prot] float32 (CPU)\n",
    "      y   [B] float32\n",
    "      t   [U, K] float32 (TF-IDF for unique drugs in batch for this split)\n",
    "      pair_to_u [B] int64  (maps pair i -> row in t)\n",
    "    \"\"\"\n",
    "    B = len(rows)\n",
    "    d_idx = np.fromiter((r.drug_idx for r in rows), dtype=np.int64, count=B)\n",
    "    p_idx = np.fromiter((r.prot_idx for r in rows), dtype=np.int64, count=B)\n",
    "    y_arr = np.fromiter((r.label for r in rows), dtype=np.float32, count=B)\n",
    "\n",
    "    # Unique drugs by RXCUI mapping per split\n",
    "    if split == \"train\":\n",
    "        rmap = R2I_train\n",
    "        T = T_train\n",
    "    elif split == \"val\":\n",
    "        rmap = R2I_val\n",
    "        T = T_val\n",
    "    elif split == \"test\":\n",
    "        rmap = R2I_test\n",
    "        T = T_test\n",
    "    else:\n",
    "        raise ValueError(split)\n",
    "\n",
    "    # For each pair, find the rxcui row; then build the unique set for the batch\n",
    "    rxcui_rows = [rmap[r.rxcui] for r in rows]\n",
    "    unique_rows, inverse = np.unique(np.asarray(rxcui_rows, dtype=np.int64), return_inverse=True)\n",
    "    # inverse gives pair_to_u mapping\n",
    "    pair_to_u = torch.from_numpy(inverse.astype(np.int64))\n",
    "\n",
    "    # Gather TF-IDF for U unique drugs\n",
    "    t = T[torch.from_numpy(unique_rows)]  # [U, K], still CPU float32\n",
    "\n",
    "    # Gather embedding rows for pairs\n",
    "    x_d = DRUG_TENSOR[torch.from_numpy(d_idx)]\n",
    "    x_p = PROT_TENSOR[torch.from_numpy(p_idx)]\n",
    "    y   = torch.from_numpy(y_arr)\n",
    "\n",
    "    # Final integrity checks\n",
    "    assert x_d.dtype == torch.float32 and x_p.dtype == torch.float32 and t.dtype == torch.float32\n",
    "    assert x_d.ndim == 2 and x_p.ndim == 2 and t.ndim == 2\n",
    "    assert y.ndim == 1 and pair_to_u.ndim == 1\n",
    "\n",
    "    return {\n",
    "        \"x_d\": x_d,           # [B, D_drug]\n",
    "        \"x_p\": x_p,           # [B, D_prot]\n",
    "        \"y\": y,               # [B]\n",
    "        \"t\": t,               # [U, K]\n",
    "        \"pair_to_u\": pair_to_u,  # [B]\n",
    "    }\n",
    "\n",
    "print(\"\\nCollate, datasets, and sampler scaffolding are ready ✅\\n\"\n",
    "      \"Next step: define adapters + heads + contrastives and wire the training loop.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3633a3",
   "metadata": {},
   "source": [
    "Cell 4 — Adapters, Heads, Contrastives scaffolding, AMP-guarded forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "76b8c41a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiTaskModel initialized on cuda\n",
      "Shared dim d = 512, ADR K = 4048, head = cosine\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: Adapters, Heads, Contrastives scaffolding, AMP-guarded forward ===\n",
    "\n",
    "import math\n",
    "from typing import Dict, Optional, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -------------------- Utilities --------------------\n",
    "def l2_normalize_rows(x: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Row-wise L2 normalization. x: [N, D] -> normalized [N, D]\n",
    "    \"\"\"\n",
    "    return x / (x.norm(p=2, dim=1, keepdim=True) + eps)\n",
    "\n",
    "def first_occurrence_indices(pair_to_u: torch.Tensor, B: Optional[int] = None) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Given pair_to_u [B], returns indices 'pos' of length U such that\n",
    "    u_unique[j] = u[pos[j]] aligns with t[j] for ADR head.\n",
    "    Picks the first pair position that maps to each unique u index.\n",
    "    \"\"\"\n",
    "    if B is None:\n",
    "        B = pair_to_u.numel()\n",
    "    U = int(pair_to_u.max().item() + 1)\n",
    "    pos = torch.full((U,), -1, dtype=torch.long, device=pair_to_u.device)\n",
    "    # linear scan; first seen wins\n",
    "    for i in range(B):\n",
    "        u_idx = int(pair_to_u[i].item())\n",
    "        if pos[u_idx] == -1:\n",
    "            pos[u_idx] = i\n",
    "            if (pos != -1).all():\n",
    "                break\n",
    "    if (pos == -1).any():\n",
    "        # Defensive fallback: fill remaining with the earliest valid index (keeps computation defined)\n",
    "        fill_idx = int((pair_to_u == 0).nonzero(as_tuple=True)[0][0].item()) if (pair_to_u == 0).any() else 0\n",
    "        pos[pos == -1] = fill_idx\n",
    "    return pos  # shape [U]\n",
    "\n",
    "# -------------------- Modules --------------------\n",
    "class Adapter(nn.Module):\n",
    "    \"\"\"\n",
    "    Two-layer MLP + LayerNorm projector into shared space of dim d.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, d: int, hidden_ratio: int = 2, p_drop: float = 0.10):\n",
    "        super().__init__()\n",
    "        h = hidden_ratio * d\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, h),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(h, d),\n",
    "            nn.LayerNorm(d)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.net(x)\n",
    "\n",
    "class DTIHeadCosine(nn.Module):\n",
    "    \"\"\"\n",
    "    Cosine similarity with learnable scale -> logit.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.tensor(math.log(10.0), dtype=torch.float32))  # init scale≈10\n",
    "\n",
    "    def forward(self, u: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        # u, v: [B, d]\n",
    "        u_n = l2_normalize_rows(u)\n",
    "        v_n = l2_normalize_rows(v)\n",
    "        scale = self.logit_scale.exp().clamp(1e-3, 1e3)\n",
    "        # cosine in [-1,1]; scale to logits\n",
    "        logits = scale * (u_n * v_n).sum(dim=1)\n",
    "        return logits  # [B]\n",
    "\n",
    "class DTIHeadBilinear(nn.Module):\n",
    "    \"\"\"\n",
    "    Bilinear u^T W v + b -> logit.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int):\n",
    "        super().__init__()\n",
    "        self.W = nn.Parameter(torch.empty(d, d))\n",
    "        self.b = nn.Parameter(torch.zeros(1))\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, u: torch.Tensor, v: torch.Tensor) -> torch.Tensor:\n",
    "        # [B,d] @ [d,d] -> [B,d], then dot with v\n",
    "        uW = torch.matmul(u, self.W)  # [B, d]\n",
    "        logits = (uW * v).sum(dim=1) + self.b  # [B]\n",
    "        return logits\n",
    "\n",
    "class ADRHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear(d -> K) with Softplus to enforce non-negative TF-IDF predictions.\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, K: int):\n",
    "        super().__init__()\n",
    "        self.out = nn.Linear(d, K)\n",
    "\n",
    "    def forward(self, u_unique: torch.Tensor) -> torch.Tensor:\n",
    "        y = self.out(u_unique)  # [U, K]\n",
    "        return F.softplus(y)    # non-negative\n",
    "\n",
    "# -------------------- Main Multi-Task Model --------------------\n",
    "class MultiTaskModel(nn.Module):\n",
    "    \"\"\"\n",
    "    f_d: drug adapter (in_dim=D_drug -> d)\n",
    "    f_p: protein adapter (in_dim=D_prot -> d)\n",
    "    f_a: ADR adapter over TF-IDF (K -> d)  [used for contrastive DA and prototype building]\n",
    "    h_dti: binding head (cosine or bilinear)\n",
    "    h_adr: ADR regression head (Softplus)\n",
    "    \"\"\"\n",
    "    def __init__(self, Dd: int, Dp: int, K: int, cfg: dict):\n",
    "        super().__init__()\n",
    "        mcfg = cfg[\"model\"]\n",
    "        d = int(mcfg[\"shared_dim\"])\n",
    "        hidden_ratio = int(mcfg.get(\"hidden_ratio\", 2))\n",
    "        p_drop = float(mcfg.get(\"p_drop\", 0.10))\n",
    "\n",
    "        self.d = d\n",
    "        self.K = K\n",
    "\n",
    "        # Adapters\n",
    "        self.f_d = Adapter(Dd, d, hidden_ratio=hidden_ratio, p_drop=p_drop)\n",
    "        self.f_p = Adapter(Dp, d, hidden_ratio=hidden_ratio, p_drop=p_drop)\n",
    "        self.f_a = Adapter(K,  d, hidden_ratio=hidden_ratio, p_drop=p_drop)\n",
    "\n",
    "        # DTI head\n",
    "        head_type = str(mcfg.get(\"dti_head\", \"cosine\")).lower()\n",
    "        if head_type == \"cosine\":\n",
    "            self.h_dti = DTIHeadCosine(d)\n",
    "        elif head_type == \"bilinear\":\n",
    "            self.h_dti = DTIHeadBilinear(d)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown dti_head: {head_type}\")\n",
    "        self.dti_head_type = head_type\n",
    "\n",
    "        # ADR head\n",
    "        self.h_adr = ADRHead(d, K)\n",
    "\n",
    "        # Contrastive settings\n",
    "        c = mcfg.get(\"contrastive\", {})\n",
    "        self.use_dp = bool(c.get(\"use_dp\", True))\n",
    "        self.use_da = bool(c.get(\"use_da\", True))\n",
    "        self.tau    = float(c.get(\"tau\", 0.07))\n",
    "\n",
    "    @torch.inference_mode(False)\n",
    "    def forward(\n",
    "        self,\n",
    "        x_d: torch.Tensor,         # [B, Dd] float32 CPU/GPU\n",
    "        x_p: torch.Tensor,         # [B, Dp] float32 CPU/GPU\n",
    "        t: torch.Tensor,           # [U, K]  float32 CPU/GPU (TF-IDF for unique drugs in batch)\n",
    "        pair_to_u: torch.Tensor,   # [B] int64 mapping each pair -> its u index (0..U-1)\n",
    "        amp_enabled: bool = AMP_ENABLED,\n",
    "        amp_dtype: Optional[torch.dtype] = AMP_DTYPE\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          u [B,d], v [B,d], w [U,d]\n",
    "          logits [B]\n",
    "          y_adr_hat [U, K]\n",
    "          u_unique_index [U]   # indices into batch rows used to form u_unique\n",
    "        \"\"\"\n",
    "        # Move to module device (supports CPU debug)\n",
    "        dev = next(self.parameters()).device\n",
    "        x_d = x_d.to(dev, dtype=torch.float32, non_blocking=True)\n",
    "        x_p = x_p.to(dev, dtype=torch.float32, non_blocking=True)\n",
    "        t   = t.to(dev,   dtype=torch.float32, non_blocking=True)\n",
    "        pair_to_u = pair_to_u.to(dev)\n",
    "\n",
    "        # AMP guard: forward only autocast (losses kept in fp32 outside)\n",
    "        ac_dtype = amp_dtype if (amp_enabled and amp_dtype is not None) else torch.float32\n",
    "        ctx = torch.autocast(device_type=dev.type, dtype=ac_dtype) if (amp_enabled and amp_dtype is not None and dev.type == \"cuda\") else torch.cuda.amp.autocast(enabled=False)\n",
    "\n",
    "        with ctx:\n",
    "            # Adapters\n",
    "            u = self.f_d(x_d)   # [B, d]\n",
    "            v = self.f_p(x_p)   # [B, d]\n",
    "            w = self.f_a(t)     # [U, d]\n",
    "\n",
    "            # DTI head\n",
    "            logits = self.h_dti(u, v)  # [B]\n",
    "\n",
    "            # Build u_unique aligned with 't' (same U order) via first occurrences in the batch\n",
    "            pos = first_occurrence_indices(pair_to_u, B=x_d.shape[0])  # [U]\n",
    "            u_unique = u.index_select(0, pos)  # [U, d]\n",
    "\n",
    "            # ADR head\n",
    "            y_adr_hat = self.h_adr(u_unique)  # [U, K] (Softplus)\n",
    "\n",
    "        return {\n",
    "            \"u\": u, \"v\": v, \"w\": w,\n",
    "            \"logits\": logits,\n",
    "            \"y_adr_hat\": y_adr_hat,\n",
    "            \"u_unique_index\": pos,   # useful for diagnostics / contrastive DA if needed\n",
    "        }\n",
    "\n",
    "# -------------------- Contrastive losses (InfoNCE) --------------------\n",
    "def info_nce_from_pairs(\n",
    "    u: torch.Tensor, v: torch.Tensor, labels: torch.Tensor, tau: float = 0.07\n",
    ") -> Tuple[torch.Tensor, int]:\n",
    "    \"\"\"\n",
    "    Drug–Protein InfoNCE on positive pairs only.\n",
    "    Args:\n",
    "      u, v: [B, d]\n",
    "      labels: [B] in {0,1}\n",
    "    Returns:\n",
    "      loss, n_pos_used\n",
    "    \"\"\"\n",
    "    dev = u.device\n",
    "    pos_mask = (labels.to(dev) == 1)\n",
    "    if pos_mask.sum() <= 1:\n",
    "        # Not enough positives to build a batchwise contrast — return 0\n",
    "        return u.new_zeros(()), int(pos_mask.sum().item())\n",
    "    u_pos = u[pos_mask]\n",
    "    v_pos = v[pos_mask]\n",
    "    # Normalize rows\n",
    "    u_pos = l2_normalize_rows(u_pos)\n",
    "    v_pos = l2_normalize_rows(v_pos)\n",
    "    # Similarity matrices\n",
    "    logits_uv = (u_pos @ v_pos.T) / tau   # [P, P]\n",
    "    logits_vu = (v_pos @ u_pos.T) / tau   # [P, P]\n",
    "    targets = torch.arange(u_pos.size(0), device=dev, dtype=torch.long)\n",
    "    loss = F.cross_entropy(logits_uv, targets) + F.cross_entropy(logits_vu, targets)\n",
    "    return loss * 0.5, int(pos_mask.sum().item())\n",
    "\n",
    "def info_nce_drug_adr(\n",
    "    u_unique: torch.Tensor, w: torch.Tensor, tau: float = 0.07\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Drug–ADR InfoNCE: diagonal positives between u_unique and w (same U).\n",
    "    \"\"\"\n",
    "    if u_unique.size(0) <= 1:\n",
    "        return u_unique.new_zeros(())\n",
    "    u_n = l2_normalize_rows(u_unique)\n",
    "    w_n = l2_normalize_rows(w)\n",
    "    logits_uw = (u_n @ w_n.T) / tau   # [U, U]\n",
    "    logits_wu = (w_n @ u_n.T) / tau   # [U, U]\n",
    "    targets = torch.arange(u_n.size(0), device=u_n.device, dtype=torch.long)\n",
    "    loss = F.cross_entropy(logits_uw, targets) + F.cross_entropy(logits_wu, targets)\n",
    "    return loss * 0.5\n",
    "\n",
    "# -------------------- Instantiate model (moves later to DEVICE) --------------------\n",
    "K = int(cfg[\"data\"][\"K\"])\n",
    "model = MultiTaskModel(DRUG_IN_DIM, PROT_IN_DIM, K, cfg)\n",
    "model = model.to(DEVICE)\n",
    "print(model.__class__.__name__, \"initialized on\", DEVICE)\n",
    "print(f\"Shared dim d = {model.d}, ADR K = {model.K}, head = {model.dti_head_type}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f64ae2",
   "metadata": {},
   "source": [
    "Cell 5 — Loss assembly, optimizer/scheduler, and a single-batch dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "98335efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dry Run: Loss breakdown ===\n",
      "L_dti (BCE pos-w): 1.315807\n",
      "L_adr (regress) : 0.301591\n",
      "L_dp (InfoNCE)  : 0.000000\n",
      "L_da (InfoNCE)  : 0.000000\n",
      "--> L_total     : 1.315807\n",
      "\n",
      "=== Sanity: tensor shapes/dtypes ===\n",
      "logits: (8,) torch.float32  | y: (8,) torch.float32\n",
      "y_adr_hat: (8, 4048) torch.float32  | t_targets: (8, 4048) torch.float32\n",
      "u: (8, 512), v: (8, 512), w: (8, 512)  | pos: (8,)\n",
      "AMP enabled: True / bf16\n",
      "Dry run forward+loss ✅  (no weights updated)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fahmid\\AppData\\Local\\Temp\\ipykernel_31012\\2768861038.py:102: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_scaler)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: Loss assembly, optimizer/scheduler, and a single-batch dry run ===\n",
    "\n",
    "import math\n",
    "from typing import Dict, Tuple, Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# ---------------------- Loss builders ----------------------\n",
    "def build_dti_bce_loss(pos_weight: torch.Tensor) -> nn.Module:\n",
    "    \"\"\"\n",
    "    BCEWithLogits with pos_weight on the SAME device/dtype as logits.\n",
    "    We'll recast pos_weight at call-time to logits.dtype/device for safety.\n",
    "    \"\"\"\n",
    "    # We'll wrap a callable to inject the (device,dtype)-corrected pos_weight each step.\n",
    "    class _BCEWithDynamicPos(nn.Module):\n",
    "        def __init__(self, pw: torch.Tensor):\n",
    "            super().__init__()\n",
    "            self.register_buffer(\"pos_weight_buf\", pw.float(), persistent=False)\n",
    "\n",
    "        def forward(self, logits: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "            # Ensure fp32 loss math even under autocast (logits will be autocast dtype)\n",
    "            logits_f32 = logits.float()\n",
    "            target_f32 = target.float()\n",
    "            # Move/cast pos_weight\n",
    "            pos_w = self.pos_weight_buf.to(device=logits_f32.device, dtype=logits_f32.dtype)\n",
    "            return F.binary_cross_entropy_with_logits(logits_f32, target_f32, pos_weight=pos_w)\n",
    "\n",
    "    return _BCEWithDynamicPos(pos_weight)\n",
    "\n",
    "\n",
    "def build_adr_regression_loss(cfg_loss: Dict) -> nn.Module:\n",
    "    \"\"\"\n",
    "    ADR regression to TF-IDF using Huber (default) or MSE.\n",
    "    \"\"\"\n",
    "    mode = str(cfg_loss.get(\"type\", \"huber\")).lower()\n",
    "    if mode == \"huber\":\n",
    "        delta = float(cfg_loss.get(\"delta\", 1.0))\n",
    "        class _Huber(nn.Module):\n",
    "            def __init__(self, delta: float):\n",
    "                super().__init__()\n",
    "                self.delta = delta\n",
    "            def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "                # Keep loss in fp32\n",
    "                return F.huber_loss(pred.float(), target.float(), delta=self.delta, reduction=\"mean\")\n",
    "        return _Huber(delta)\n",
    "    elif mode == \"mse\":\n",
    "        class _MSE(nn.Module):\n",
    "            def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "                return F.mse_loss(pred.float(), target.float(), reduction=\"mean\")\n",
    "        return _MSE()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown ADR loss type: {mode}\")\n",
    "\n",
    "\n",
    "# ---------------------- Optimizer & Scheduler ----------------------\n",
    "def build_optimizer_and_scheduler(model: nn.Module, cfg: dict, steps_per_epoch: int) -> Tuple[torch.optim.Optimizer, Optional[LambdaLR]]:\n",
    "    opt_cfg = cfg[\"train\"][\"optimizer\"]\n",
    "    sch_cfg = cfg[\"train\"][\"scheduler\"]\n",
    "\n",
    "    lr = float(opt_cfg.get(\"lr\", 2e-4))\n",
    "    wd = float(opt_cfg.get(\"weight_decay\", 1e-4))\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "\n",
    "    if not bool(sch_cfg.get(\"use_cosine\", True)):\n",
    "        return optimizer, None\n",
    "\n",
    "    warmup_pct = float(sch_cfg.get(\"warmup_pct\", 0.05))\n",
    "    total_epochs = int(cfg[\"train\"][\"epochs\"])\n",
    "    total_steps = max(1, steps_per_epoch * total_epochs)\n",
    "    warmup_steps = int(total_steps * warmup_pct)\n",
    "\n",
    "    def lr_lambda(step: int):\n",
    "        if step < warmup_steps:\n",
    "            return max(1e-8, step / max(1, warmup_steps))  # linear warmup\n",
    "        # cosine decay from 1.0 to 0.0 over remaining steps\n",
    "        progress = (step - warmup_steps) / max(1, total_steps - warmup_steps)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    return optimizer, scheduler\n",
    "\n",
    "\n",
    "# ---------------------- Glue losses together ----------------------\n",
    "lambda_dti = float(cfg[\"train\"][\"loss_weights\"][\"lambda_dti\"])\n",
    "lambda_adr = float(cfg[\"train\"][\"loss_weights\"][\"lambda_adr\"])\n",
    "lambda_con = float(cfg[\"train\"][\"loss_weights\"][\"lambda_con\"])\n",
    "\n",
    "# BCE pos_weight (already computed on train set in Cell 3 -> POS_WEIGHT_TRAIN)\n",
    "criterion_dti = build_dti_bce_loss(POS_WEIGHT_TRAIN)\n",
    "\n",
    "# ADR loss config (allow override in cfg, else default huber)\n",
    "cfg.setdefault(\"losses\", {})\n",
    "cfg[\"losses\"].setdefault(\"adr\", {\"type\": \"huber\", \"delta\": 1.0})\n",
    "criterion_adr = build_adr_regression_loss(cfg[\"losses\"][\"adr\"])\n",
    "\n",
    "# AMP scaler: useful only for fp16; bf16 doesn't need GradScaler\n",
    "use_scaler = AMP_ENABLED and (AMP_PRECISION == \"fp16\")\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_scaler)\n",
    "\n",
    "# ---------------------- One-step dry run (no weight update) ----------------------\n",
    "# We'll assemble a small balanced batch from ds_train and run forward + losses.\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_bs = min(8, int(cfg[\"train\"][\"batch_size\"]))  # small test; must be even\n",
    "if test_bs % 2 == 1:\n",
    "    test_bs += 1\n",
    "\n",
    "sampler = BalancedBatchSampler(ds_train, batch_size=test_bs)\n",
    "# Windows-safe DataLoader: num_workers=0 to avoid multiprocessing pitfalls with CUDA context on Win\n",
    "loader = DataLoader(ds_train, batch_sampler=sampler, num_workers=0, collate_fn=lambda rows: collate_pairs_with_unique_drugs(rows, split=\"train\"))\n",
    "\n",
    "# Build optimizer/scheduler for later (need steps_per_epoch)\n",
    "steps_per_epoch = max(1, len(ds_train) // max(2, int(cfg[\"train\"][\"batch_size\"])))\n",
    "optimizer, scheduler = build_optimizer_and_scheduler(model, cfg, steps_per_epoch=steps_per_epoch)\n",
    "\n",
    "# Contrastive flags\n",
    "use_dp = model.use_dp\n",
    "use_da = model.use_da\n",
    "tau    = model.tau\n",
    "\n",
    "# Get one batch\n",
    "batch = next(iter(loader))\n",
    "# Move label to device for DP contrastive selection and BCE\n",
    "y = batch[\"y\"].to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "# Forward pass (AMP only affects forward)\n",
    "out = model(\n",
    "    x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "    x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "    t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "    pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "    amp_enabled=AMP_ENABLED,\n",
    "    amp_dtype=AMP_DTYPE\n",
    ")\n",
    "\n",
    "logits = out[\"logits\"]                   # [B]\n",
    "y_adr_hat = out[\"y_adr_hat\"]             # [U, K]\n",
    "u = out[\"u\"]; v = out[\"v\"]; w = out[\"w\"] # [B,d], [B,d], [U,d]\n",
    "pos = out[\"u_unique_index\"]              # [U]\n",
    "\n",
    "# Gather TF-IDF targets aligned to y_adr_hat (already aligned in collate via t)\n",
    "t_targets = batch[\"t\"].to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "# Compute each loss part in fp32\n",
    "L_dti = criterion_dti(logits, y)  # BCEWithLogits (pos_weighted)\n",
    "L_adr = criterion_adr(y_adr_hat, t_targets)\n",
    "\n",
    "# Contrastive pieces\n",
    "L_dp = torch.zeros((), device=DEVICE)\n",
    "L_da = torch.zeros((), device=DEVICE)\n",
    "if lambda_con > 0.0:\n",
    "    if use_dp:\n",
    "        L_dp, npos_used = info_nce_from_pairs(u, v, y, tau=tau)\n",
    "    if use_da:\n",
    "        # Build u_unique aligned with t: out[\"u_unique_index\"] maps u rows -> unique drugs\n",
    "        u_unique = u.index_select(0, pos.to(DEVICE))\n",
    "        L_da = info_nce_drug_adr(u_unique, w, tau=tau)\n",
    "\n",
    "L_con = (L_dp + L_da)\n",
    "\n",
    "L_total = (lambda_dti * L_dti) + (lambda_adr * L_adr) + (lambda_con * L_con)\n",
    "\n",
    "# Print diagnostics (dtype/shape/values)\n",
    "print(\"=== Dry Run: Loss breakdown ===\")\n",
    "print(f\"L_dti (BCE pos-w): {L_dti.item():.6f}\")\n",
    "print(f\"L_adr (regress) : {L_adr.item():.6f}\")\n",
    "print(f\"L_dp (InfoNCE)  : {float(L_dp.item()):.6f}\")\n",
    "print(f\"L_da (InfoNCE)  : {float(L_da.item()):.6f}\")\n",
    "print(f\"--> L_total     : {L_total.item():.6f}\")\n",
    "\n",
    "print(\"\\n=== Sanity: tensor shapes/dtypes ===\")\n",
    "print(f\"logits: {tuple(logits.shape)} {logits.dtype}  | y: {tuple(y.shape)} {y.dtype}\")\n",
    "print(f\"y_adr_hat: {tuple(y_adr_hat.shape)} {y_adr_hat.dtype}  | t_targets: {tuple(t_targets.shape)} {t_targets.dtype}\")\n",
    "print(f\"u: {tuple(u.shape)}, v: {tuple(v.shape)}, w: {tuple(w.shape)}  | pos: {tuple(pos.shape)}\")\n",
    "print(f\"AMP enabled: {AMP_ENABLED} / {AMP_PRECISION}\")\n",
    "print(\"Dry run forward+loss ✅  (no weights updated)\")\n",
    "\n",
    "# NOTE: We will construct the full training loop (Cell 6) next,\n",
    "# including AMP autocast for forward, fp32 loss math, GradScaler (fp16 only),\n",
    "# gradient clipping, optimizer.step(), and optional scheduler.step().\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688a61ad",
   "metadata": {},
   "source": [
    "Cell 6 — Full training loop with validation, early stopping, and checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "9d49b7a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training for 40 epochs ===\n",
      "Epoch 001 | 5.0s | L: 0.8418 (dti 0.8418 | adr 0.3026 | con 0.0000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fahmid\\AppData\\Local\\Temp\\ipykernel_31012\\3841570617.py:75: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(y, x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  VAL: PR-AUC 0.6383 | ROC-AUC 0.7725 | F1 0.7218 @thr=0.550 | ADR RMSE 0.7870 MAE 0.7303\n",
      "  ✔ Saved new best checkpoint to F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\best.pt\n",
      "Epoch 002 | 4.9s | L: 0.6705 (dti 0.6705 | adr 0.3022 | con 0.0000)\n",
      "  VAL: PR-AUC 0.6441 | ROC-AUC 0.7758 | F1 0.7076 @thr=0.590 | ADR RMSE 0.7866 MAE 0.7299\n",
      "  ✔ Saved new best checkpoint to F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\best.pt\n",
      "Epoch 003 | 5.0s | L: 0.6629 (dti 0.6629 | adr 0.3022 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5567 | ROC-AUC 0.7409 | F1 0.6949 @thr=0.556 | ADR RMSE 0.7867 MAE 0.7300\n",
      "  (no improve: 1/7)\n",
      "Epoch 004 | 5.1s | L: 0.6601 (dti 0.6601 | adr 0.3021 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5464 | ROC-AUC 0.7465 | F1 0.7197 @thr=0.561 | ADR RMSE 0.7865 MAE 0.7296\n",
      "  (no improve: 2/7)\n",
      "Epoch 005 | 5.0s | L: 0.6429 (dti 0.6429 | adr 0.3020 | con 0.0000)\n",
      "  VAL: PR-AUC 0.6433 | ROC-AUC 0.7686 | F1 0.7143 @thr=0.558 | ADR RMSE 0.7863 MAE 0.7292\n",
      "  (no improve: 3/7)\n",
      "Epoch 006 | 4.9s | L: 0.6390 (dti 0.6390 | adr 0.3024 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5503 | ROC-AUC 0.7362 | F1 0.6843 @thr=0.581 | ADR RMSE 0.7871 MAE 0.7296\n",
      "  (no improve: 4/7)\n",
      "Epoch 007 | 4.9s | L: 0.6235 (dti 0.6235 | adr 0.3027 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5637 | ROC-AUC 0.7484 | F1 0.7035 @thr=0.590 | ADR RMSE 0.7875 MAE 0.7298\n",
      "  (no improve: 5/7)\n",
      "Epoch 008 | 4.9s | L: 0.6243 (dti 0.6243 | adr 0.3030 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5990 | ROC-AUC 0.7621 | F1 0.7187 @thr=0.579 | ADR RMSE 0.7877 MAE 0.7298\n",
      "  (no improve: 6/7)\n",
      "Epoch 009 | 5.1s | L: 0.6114 (dti 0.6114 | adr 0.3030 | con 0.0000)\n",
      "  VAL: PR-AUC 0.5776 | ROC-AUC 0.7612 | F1 0.7117 @thr=0.581 | ADR RMSE 0.7874 MAE 0.7295\n",
      "  (no improve: 7/7)\n",
      "Early stopping at epoch 9 (best epoch 2, best val PR-AUC 0.6441)\n",
      "\n",
      "=== Evaluating on TEST with best val threshold ===\n",
      "TEST: PR-AUC 0.5495 | ROC-AUC 0.7714 | F1 0.5443 @thr=0.590 | ADR RMSE 0.7877 MAE 0.7297\n",
      "Saved ADR prototypes to runs\\dti_adr_v1\\prototypes_C.npy\n",
      "\n",
      "Training loop complete ✅\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6: Full training loop with validation, early stopping, and checkpointing ===\n",
    "\n",
    "import math, time, json\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ----------------- DataLoaders -----------------\n",
    "BATCH_SIZE = int(cfg[\"train\"][\"batch_size\"])\n",
    "\n",
    "# train_loader = DataLoader(\n",
    "#     ds_train,\n",
    "#     batch_sampler=BalancedBatchSampler(ds_train, batch_size=BATCH_SIZE),\n",
    "#     num_workers=0,  # Windows-safe\n",
    "#     collate_fn=lambda rows: collate_pairs_with_unique_drugs(rows, split=\"train\"),\n",
    "# )\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    ds_train,\n",
    "    batch_sampler=BalancedBatchSampler(ds_train, batch_size=BATCH_SIZE, pos_frac=0.65),\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda rows: collate_pairs_with_unique_drugs(rows, split=\"train\"),\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda rows: collate_pairs_with_unique_drugs(rows, split=\"val\"),\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=lambda rows: collate_pairs_with_unique_drugs(rows, split=\"test\"),\n",
    ")\n",
    "\n",
    "steps_per_epoch = max(1, len(train_loader))\n",
    "optimizer, scheduler = build_optimizer_and_scheduler(model, cfg, steps_per_epoch)\n",
    "\n",
    "# ----------------- Metric helpers (pure numpy) -----------------\n",
    "def _binary_classification_curves(y_true: np.ndarray, y_score: np.ndarray):\n",
    "    \"\"\"Returns ROC and PR curves sorted by score descending.\"\"\"\n",
    "    # Sort by score desc\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "\n",
    "    # Cum sums for TP/FP\n",
    "    tp = np.cumsum(y_true == 1)\n",
    "    fp = np.cumsum(y_true == 0)\n",
    "    fn = tp[-1] - tp\n",
    "    tn = fp[-1] - fp\n",
    "\n",
    "    # ROC\n",
    "    tpr = tp / np.maximum(tp[-1], 1)\n",
    "    fpr = fp / np.maximum(fp[-1], 1)\n",
    "\n",
    "    # Precision-Recall\n",
    "    precision = tp / np.maximum(tp + fp, 1)\n",
    "    recall = tp / np.maximum(tp[-1], 1)\n",
    "\n",
    "    return fpr, tpr, precision, recall, y_true, y_score\n",
    "\n",
    "def _auc(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    \"\"\"Trapezoidal integral; assumes x is monotonic increasing.\"\"\"\n",
    "    if len(x) < 2:\n",
    "        return float(\"nan\")\n",
    "    return float(np.trapz(y, x))\n",
    "\n",
    "def roc_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    fpr, tpr, *_ = _binary_classification_curves(y_true, y_score)\n",
    "    # Ensure increasing fpr\n",
    "    order = np.argsort(fpr)\n",
    "    return _auc(fpr[order], tpr[order])\n",
    "\n",
    "def pr_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    *_, precision, recall, _, _ = _binary_classification_curves(y_true, y_score)\n",
    "    # Ensure increasing recall\n",
    "    order = np.argsort(recall)\n",
    "    return _auc(recall[order], precision[order])\n",
    "\n",
    "def best_f1_threshold(y_true: np.ndarray, y_score: np.ndarray) -> Tuple[float, float]:\n",
    "    \"\"\"Returns (best_threshold, best_f1).\"\"\"\n",
    "    # evaluate on unique sorted scores plus 0.5 heuristic\n",
    "    thresholds = np.unique(y_score)\n",
    "    best_t, best_f1 = 0.5, 0.0\n",
    "    for t in thresholds:\n",
    "        y_hat = (y_score >= t).astype(np.int32)\n",
    "        tp = (y_hat & (y_true == 1)).sum()\n",
    "        fp = (y_hat & (y_true == 0)).sum()\n",
    "        fn = ((1 - y_hat) & (y_true == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = float(f1), float(t)\n",
    "    return best_t, best_f1\n",
    "\n",
    "def rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.sqrt(np.mean((a - b) ** 2)))\n",
    "\n",
    "def mae(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return float(np.mean(np.abs(a - b)))\n",
    "\n",
    "def recall_at_k(y_true_row: np.ndarray, y_score_row: np.ndarray, k: int) -> float:\n",
    "    \"\"\"y_true_row, y_score_row are 1D arrays for a single drug; positives are non-zero TF-IDF.\"\"\"\n",
    "    k = min(k, y_score_row.size)\n",
    "    topk = np.argpartition(-y_score_row, k-1)[:k]\n",
    "    hits = (y_true_row[topk] > 0).sum()\n",
    "    total_pos = (y_true_row > 0).sum()\n",
    "    return float(hits / max(total_pos, 1))\n",
    "\n",
    "def ndcg_at_k(y_true_row: np.ndarray, y_score_row: np.ndarray, k: int) -> float:\n",
    "    k = min(k, y_score_row.size)\n",
    "    order = np.argsort(-y_score_row)[:k]\n",
    "    gains = y_true_row[order]  # TF-IDF as relevance proxy\n",
    "    discounts = 1.0 / np.log2(np.arange(2, k+2))\n",
    "    dcg = float(np.sum(gains * discounts))\n",
    "    # Ideal DCG\n",
    "    ideal_order = np.argsort(-y_true_row)[:k]\n",
    "    ideal = float(np.sum(y_true_row[ideal_order] * discounts))\n",
    "    return float(dcg / ideal) if ideal > 0 else 0.0\n",
    "\n",
    "# ----------------- Evaluation -----------------\n",
    "def evaluate(model: torch.nn.Module, loader: DataLoader, split_name: str, val_threshold: Optional[float]=None, ks: List[int]=[5,10]) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    y_all, p_all = [], []\n",
    "\n",
    "    # ADR metrics: aggregate per unique-drug rows over all batches\n",
    "    adr_preds, adr_tgts = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            y = batch[\"y\"].to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "            out = model(\n",
    "                x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "                x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "                t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "                pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "                amp_enabled=AMP_ENABLED,\n",
    "                amp_dtype=AMP_DTYPE\n",
    "            )\n",
    "            logits = out[\"logits\"]\n",
    "            # prob = torch.sigmoid(logits).detach().float().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            # After loading T if present\n",
    "            calib_path = OUT_DIR / \"calibration.json\"\n",
    "            if calib_path.exists():\n",
    "                T_star = json.loads(calib_path.read_text())[\"temperature\"]\n",
    "            else:\n",
    "                T_star = 1.0\n",
    "\n",
    "            prob = torch.sigmoid(out[\"logits\"] / T_star).detach().float().cpu().numpy()\n",
    "            \n",
    "            \n",
    "            \n",
    "            y_np  = y.detach().cpu().numpy().astype(np.int32)\n",
    "\n",
    "            y_all.append(y_np)\n",
    "            p_all.append(prob)\n",
    "\n",
    "            # ADR accumulators\n",
    "            y_adr_hat = out[\"y_adr_hat\"].detach().float().cpu().numpy()\n",
    "            t_targets = batch[\"t\"].detach().float().cpu().numpy()\n",
    "            adr_preds.append(y_adr_hat)\n",
    "            adr_tgts.append(t_targets)\n",
    "\n",
    "    if len(y_all) == 0:\n",
    "        return {\"dti_pr_auc\": float(\"nan\"), \"dti_roc_auc\": float(\"nan\"), \"dti_f1\": float(\"nan\")}\n",
    "\n",
    "    y_all = np.concatenate(y_all, axis=0)\n",
    "    p_all = np.concatenate(p_all, axis=0)\n",
    "\n",
    "    pr = pr_auc(y_all, p_all)\n",
    "    roc = roc_auc(y_all, p_all)\n",
    "\n",
    "    # Threshold for F1 (choose on val; use provided val_threshold for test)\n",
    "    if split_name == \"val\" or val_threshold is None:\n",
    "        thr, f1 = best_f1_threshold(y_all, p_all)\n",
    "    else:\n",
    "        thr = float(val_threshold)\n",
    "        y_hat = (p_all >= thr).astype(np.int32)\n",
    "        tp = (y_hat & (y_all == 1)).sum()\n",
    "        fp = (y_hat & (y_all == 0)).sum()\n",
    "        fn = ((1 - y_hat) & (y_all == 1)).sum()\n",
    "        prec = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        rec  = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "\n",
    "    # ADR metrics across all unique-drug rows concatenated (duplicates across batches are fine—macro average)\n",
    "    if len(adr_preds) > 0:\n",
    "        P = np.concatenate(adr_preds, axis=0)  # [U_total, K]\n",
    "        T = np.concatenate(adr_tgts, axis=0)   # [U_total, K]\n",
    "        adr_rmse = rmse(P, T)\n",
    "        adr_mae_ = mae(P, T)\n",
    "\n",
    "        # Ranking metrics\n",
    "        rec_at = {}\n",
    "        ndcg_at = {}\n",
    "        for k in ks:\n",
    "            # average across drug rows\n",
    "            recs = [recall_at_k(T[i], P[i], k) for i in range(P.shape[0])]\n",
    "            ndcgs = [ndcg_at_k(T[i], P[i], k) for i in range(P.shape[0])]\n",
    "            rec_at[f\"recall@{k}\"] = float(np.mean(recs))\n",
    "            ndcg_at[f\"ndcg@{k}\"] = float(np.mean(ndcgs))\n",
    "    else:\n",
    "        adr_rmse = adr_mae_ = float(\"nan\")\n",
    "        rec_at, ndcg_at = {}, {}\n",
    "\n",
    "    metrics = {\n",
    "        \"dti_pr_auc\": float(pr),\n",
    "        \"dti_roc_auc\": float(roc),\n",
    "        \"dti_f1\": float(f1),\n",
    "        \"dti_thr\": float(thr),\n",
    "        \"adr_rmse\": float(adr_rmse),\n",
    "        \"adr_mae\": float(adr_mae_),\n",
    "        **rec_at,\n",
    "        **ndcg_at,\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "# ----------------- Training Loop -----------------\n",
    "EPOCHS = int(cfg[\"train\"][\"epochs\"])\n",
    "CLIP_NORM = float(cfg[\"train\"][\"clip_grad_norm\"])\n",
    "\n",
    "best_val = -np.inf\n",
    "best_epoch = -1\n",
    "best_thr = 0.5\n",
    "patience = int(cfg.get(\"train\", {}).get(\"early_stop_patience\", 7))\n",
    "no_improve = 0\n",
    "history = []\n",
    "\n",
    "print(f\"\\n=== Training for {EPOCHS} epochs ===\")\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    \n",
    "    if epoch <= 5:\n",
    "        lambda_con_epoch = lambda_con\n",
    "    else:\n",
    "        lambda_con_epoch = lambda_con * 0.25  # try 0.5 or 0.25\n",
    "    # then use lambda_con_epoch below:\n",
    "    \n",
    "    L_total = (lambda_dti * L_dti) + (lambda_adr * L_adr) + (lambda_con_epoch * L_con)\n",
    "    running = {\"L_total\": 0.0, \"L_dti\": 0.0, \"L_adr\": 0.0, \"L_con\": 0.0}\n",
    "    steps = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        steps += 1\n",
    "        y = batch[\"y\"].to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "\n",
    "        # Forward\n",
    "        out = model(\n",
    "            x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "            x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "            t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "            pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "            amp_enabled=AMP_ENABLED,\n",
    "            amp_dtype=AMP_DTYPE\n",
    "        )\n",
    "        logits = out[\"logits\"]\n",
    "        y_adr_hat = out[\"y_adr_hat\"]\n",
    "        u, v, w = out[\"u\"], out[\"v\"], out[\"w\"]\n",
    "        pos = out[\"u_unique_index\"]\n",
    "\n",
    "        # Losses (fp32 math)\n",
    "        L_dti = criterion_dti(logits, y)\n",
    "        L_adr = criterion_adr(y_adr_hat, batch[\"t\"].to(DEVICE, dtype=torch.float32))\n",
    "        L_dp = torch.zeros((), device=DEVICE)\n",
    "        L_da = torch.zeros((), device=DEVICE)\n",
    "        if lambda_con > 0.0:\n",
    "            if model.use_dp:\n",
    "                L_dp, _ = info_nce_from_pairs(u, v, y, tau=model.tau)\n",
    "            if model.use_da:\n",
    "                u_unique = u.index_select(0, pos.to(DEVICE))\n",
    "                L_da = info_nce_drug_adr(u_unique, w, tau=model.tau)\n",
    "        L_con = L_dp + L_da\n",
    "        L_total = (lambda_dti * L_dti) + (lambda_adr * L_adr) + (lambda_con * L_con)\n",
    "\n",
    "        # Backward\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        if scaler.is_enabled():\n",
    "            scaler.scale(L_total).backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            L_total.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_NORM)\n",
    "            optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Logs\n",
    "        running[\"L_total\"] += float(L_total.item())\n",
    "        running[\"L_dti\"]   += float(L_dti.item())\n",
    "        running[\"L_adr\"]   += float(L_adr.item())\n",
    "        running[\"L_con\"]   += float(L_con.item())\n",
    "\n",
    "    # Epoch logs\n",
    "    for k in running:\n",
    "        running[k] /= max(1, steps)\n",
    "    t1 = time.time()\n",
    "    print(f\"Epoch {epoch:03d} | {t1-t0:.1f}s | \"\n",
    "          f\"L: {running['L_total']:.4f} (dti {running['L_dti']:.4f} | adr {running['L_adr']:.4f} | con {running['L_con']:.4f})\")\n",
    "\n",
    "    # ---------- Validation ----------\n",
    "    m_val = evaluate(model, val_loader, split_name=\"val\", val_threshold=None, ks=cfg[\"eval\"][\"metrics\"].get(\"adr_k\", [5,10]))\n",
    "    print(f\"  VAL: PR-AUC {m_val['dti_pr_auc']:.4f} | ROC-AUC {m_val['dti_roc_auc']:.4f} | F1 {m_val['dti_f1']:.4f} @thr={m_val['dti_thr']:.3f} \"\n",
    "          f\"| ADR RMSE {m_val['adr_rmse']:.4f} MAE {m_val['adr_mae']:.4f}\")\n",
    "\n",
    "    # Early stopping on dti_pr_auc\n",
    "    current = m_val[\"dti_pr_auc\"]\n",
    "    if current > best_val:\n",
    "        best_val = current\n",
    "        best_epoch = epoch\n",
    "        best_thr = m_val[\"dti_thr\"]\n",
    "        no_improve = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        ckpt = {\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"optimizer_state\": optimizer.state_dict(),\n",
    "            \"scheduler_state\": scheduler.state_dict() if scheduler is not None else None,\n",
    "            \"metrics_val\": m_val,\n",
    "            \"best_thr\": best_thr,\n",
    "            \"config\": cfg,\n",
    "        }\n",
    "        torch.save(ckpt, OUT_DIR / \"best.pt\")\n",
    "        with open(OUT_DIR / \"best_metrics_val.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(m_val, f, indent=2)\n",
    "        print(f\"  ✔ Saved new best checkpoint to {OUT_DIR / 'best.pt'}\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        print(f\"  (no improve: {no_improve}/{patience})\")\n",
    "\n",
    "    history.append({\"epoch\": epoch, \"train\": running, \"val\": m_val})\n",
    "\n",
    "    if no_improve >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch} (best epoch {best_epoch}, best val PR-AUC {best_val:.4f})\")\n",
    "        break\n",
    "\n",
    "# ----------------- Final evaluation on TEST with best threshold -----------------\n",
    "print(\"\\n=== Evaluating on TEST with best val threshold ===\")\n",
    "m_test = evaluate(model, test_loader, split_name=\"test\", val_threshold=best_thr, ks=cfg[\"eval\"][\"metrics\"].get(\"adr_k\", [5,10]))\n",
    "print(f\"TEST: PR-AUC {m_test['dti_pr_auc']:.4f} | ROC-AUC {m_test['dti_roc_auc']:.4f} | F1 {m_test['dti_f1']:.4f} @thr={best_thr:.3f} \"\n",
    "      f\"| ADR RMSE {m_test['adr_rmse']:.4f} MAE {m_test['adr_mae']:.4f}\")\n",
    "with open(OUT_DIR / \"test_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({**m_test, \"thr\": best_thr}, f, indent=2)\n",
    "\n",
    "# ----------------- Optional: export ADR prototypes -----------------\n",
    "art_cfg = cfg.get(\"artifacts\", {})\n",
    "if bool(art_cfg.get(\"save_prototypes\", True)):\n",
    "    proto_path = Path(art_cfg.get(\"prototypes_path\", str(OUT_DIR / \"prototypes_C.npy\")))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Pass the entire TRAIN TF-IDF through f_a to produce prototypes per ADR via mean of drug embeddings.\n",
    "        # But our design defines prototypes per ADR as mean of f_a(TFIDF_d) over drugs where TF-IDF(d,k) > 0.\n",
    "        # We'll compute W = f_a(T_train) -> [U_train, d], then aggregate by ADR column.\n",
    "        W = model.f_a(T_train.to(DEVICE, dtype=torch.float32))\n",
    "        W = W.detach().cpu().numpy()  # [U, d]\n",
    "        T_np = T_train.cpu().numpy()  # [U, K]\n",
    "\n",
    "        K = T_np.shape[1]\n",
    "        d = W.shape[1]\n",
    "        C = np.zeros((K, d), dtype=np.float32)\n",
    "        for k in range(K):\n",
    "            mask = T_np[:, k] > 0\n",
    "            if mask.any():\n",
    "                C[k] = W[mask].mean(axis=0)\n",
    "            else:\n",
    "                C[k] = 0.0\n",
    "\n",
    "        np.save(proto_path, C)\n",
    "        print(f\"Saved ADR prototypes to {proto_path}\")\n",
    "\n",
    "# ----------------- Save training history -----------------\n",
    "with open(OUT_DIR / \"history.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print(\"\\nTraining loop complete ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cfbaf89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Baseline] DTI Random PR-AUC: 0.262 (pos rate ~0.255)\n",
      "[Baseline] DTI Global prior PR-AUC: 0.368\n",
      "Pair overlap (train↔val): 52/22257 vs 4835 | (train↔test): 12/22257 vs 7595 | (val↔test): 8/4835 vs 7595\n",
      "Drug overlap train↔val: 4/723 vs 155 | Protein overlap train↔val: 1106/2065 vs 1311\n",
      "Drug overlap train↔test:1/723 vs 156 | Protein overlap train↔test:1078/2065 vs 1209\n",
      "Missing embeddings → drugs: 0 rows | proteins: 0 rows\n",
      "Drug emb norm: mean=3.209 std=0.448 | Protein emb norm: mean=53.578 std=1343.141\n",
      "Label-shuffle test: expected ROC ≈ 0.5 after retrain; run only if you plan to retrain.\n",
      "Cell 12 done ✅ — If random/prior PR-AUC is near your model, suspect misalignment or severe class skew.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fahmid\\AppData\\Local\\Temp\\ipykernel_31012\\1472679418.py:25: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(prec[order], rec[order]))\n"
     ]
    }
   ],
   "source": [
    "# === Cell 12: Baselines & Leakage Audit ===\n",
    "import numpy as np, pandas as pd, torch, math\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Random baseline & frequency baseline (DTI)\n",
    "def eval_loader_probs(loader, prob_fn):\n",
    "    y_all, p_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            y = batch[\"y\"].cpu().numpy().astype(np.int32)\n",
    "            p = prob_fn(batch)\n",
    "            y_all.append(y); p_all.append(p)\n",
    "    if not y_all: return np.array([]), np.array([])\n",
    "    return np.concatenate(y_all), np.concatenate(p_all)\n",
    "\n",
    "def pr_auc_simple(y, p):\n",
    "    if y.size == 0: return float(\"nan\")\n",
    "    order = np.argsort(-p)\n",
    "    y = y[order]\n",
    "    tp = np.cumsum(y==1); fp = np.cumsum(y==0)\n",
    "    prec = tp / np.maximum(tp+fp, 1)\n",
    "    rec  = tp / np.maximum(tp[-1], 1)\n",
    "    # trapezoid integral with recall ascending\n",
    "    order = np.argsort(rec)\n",
    "    return float(np.trapz(prec[order], rec[order]))\n",
    "\n",
    "# Random baseline (DTI)\n",
    "rng = np.random.RandomState(1337)\n",
    "y_t, p_rand = eval_loader_probs(test_loader, lambda b: rng.rand(len(b[\"y\"])))\n",
    "pr_rand = pr_auc_simple(y_t, p_rand)\n",
    "print(f\"[Baseline] DTI Random PR-AUC: {pr_rand:.3f} (pos rate ~{(y_t.mean() if y_t.size else float('nan')):.3f})\")\n",
    "\n",
    "# Frequency baseline by protein: P(y=1 | protein) computed on TRAIN\n",
    "prot_counts = Counter()\n",
    "prot_pos = Counter()\n",
    "for r in ds_train.rows:\n",
    "    prot_counts[r.prot_idx] += 1\n",
    "    prot_pos[r.prot_idx]    += int(r.label == 1)\n",
    "def prot_freq_prob(b):\n",
    "    idx = [PROT_ID2IDX[p] if isinstance(p, str) else p for p in []]  # not used; collate gives indices only\n",
    "    # we have prot indices in rows; rebuild from x_p index gathered by collate\n",
    "    # batch gives only tensors; we approximate by re-encoding and nearest rows:\n",
    "    # Instead, compute mean P(y=1) over training; apply global prior as fallback.\n",
    "    prior = (sum(prot_pos.values()) / max(1, sum(prot_counts.values())))\n",
    "    # We cannot access prot_idx directly from batch; use prior baseline:\n",
    "    return np.full(len(b[\"y\"]), prior, dtype=np.float32)\n",
    "y_t, p_prior = eval_loader_probs(test_loader, prot_freq_prob)\n",
    "print(f\"[Baseline] DTI Global prior PR-AUC: {pr_auc_simple(y_t, p_prior):.3f}\")\n",
    "\n",
    "# 2) Leakage/overlap checks\n",
    "def overlap_counts(A: pd.DataFrame, B: pd.DataFrame, key_cols):\n",
    "    a = set(map(tuple, A[key_cols].values))\n",
    "    b = set(map(tuple, B[key_cols].values))\n",
    "    return len(a & b), len(a), len(b)\n",
    "\n",
    "# pairs_train/val/test defined in Cell 3\n",
    "keys = [\"drug_chembl_id\",\"target_uniprot_id\",\"label\"]\n",
    "ov_tv, n_t, n_v = overlap_counts(pairs_train, pairs_val, keys)\n",
    "ov_tt, n_t2, n_te = overlap_counts(pairs_train, pairs_test, keys)\n",
    "ov_vt, n_v2, n_te2 = overlap_counts(pairs_val, pairs_test, keys)\n",
    "print(f\"Pair overlap (train↔val): {ov_tv}/{n_t} vs {n_v} | (train↔test): {ov_tt}/{n_t2} vs {n_te} | (val↔test): {ov_vt}/{n_v2} vs {n_te2}\")\n",
    "\n",
    "# We also check marginal overlap by drug or protein (distribution shift)\n",
    "def marginal_overlap(A, B, col):\n",
    "    a = set(A[col].astype(str))\n",
    "    b = set(B[col].astype(str))\n",
    "    return len(a & b), len(a), len(b)\n",
    "od_tv = marginal_overlap(pairs_train, pairs_val, \"drug_chembl_id\")\n",
    "op_tv = marginal_overlap(pairs_train, pairs_val, \"target_uniprot_id\")\n",
    "od_tt = marginal_overlap(pairs_train, pairs_test, \"drug_chembl_id\")\n",
    "op_tt = marginal_overlap(pairs_train, pairs_test, \"target_uniprot_id\")\n",
    "print(f\"Drug overlap train↔val: {od_tv[0]}/{od_tv[1]} vs {od_tv[2]} | Protein overlap train↔val: {op_tv[0]}/{op_tv[1]} vs {op_tv[2]}\")\n",
    "print(f\"Drug overlap train↔test:{od_tt[0]}/{od_tt[1]} vs {od_tt[2]} | Protein overlap train↔test:{op_tt[0]}/{op_tt[1]} vs {op_tt[2]}\")\n",
    "\n",
    "# 3) Embedding coverage & scales\n",
    "missing_drugs = pairs_df[~pairs_df[\"drug_chembl_id\"].isin(DRUG_ID2IDX)].shape[0]\n",
    "missing_prots = pairs_df[~pairs_df[\"target_uniprot_id\"].isin(PROT_ID2IDX)].shape[0]\n",
    "print(f\"Missing embeddings → drugs: {missing_drugs} rows | proteins: {missing_prots} rows\")\n",
    "\n",
    "# Norm stats\n",
    "d_norm = DRUG_TENSOR.norm(dim=1).numpy()\n",
    "p_norm = PROT_TENSOR.norm(dim=1).numpy()\n",
    "print(f\"Drug emb norm: mean={d_norm.mean():.3f} std={d_norm.std():.3f} | Protein emb norm: mean={p_norm.mean():.3f} std={p_norm.std():.3f}\")\n",
    "\n",
    "# 4) Label-shuffle test (should collapse to ~0.5 ROC / pos-rate PR)\n",
    "def shuffle_test():\n",
    "    import copy\n",
    "    # duplicate dataset with shuffled labels\n",
    "    y = np.array([r.label for r in ds_train.rows])\n",
    "    y_shuf = np.random.RandomState(123).permutation(y)\n",
    "    # quick check: if a fully-trained model still shows high ROC on shuffled labels, there’s leakage\n",
    "    print(\"Label-shuffle test: expected ROC ≈ 0.5 after retrain; run only if you plan to retrain.\")\n",
    "shuffle_test()\n",
    "print(\"Cell 12 done ✅ — If random/prior PR-AUC is near your model, suspect misalignment or severe class skew.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0f4fe51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DTI PREDICTION SCORES:\n",
      "Accuracy: 0.6584\n",
      "F1-Score: 0.5443\n",
      "ROC-AUC:  0.7714\n",
      "\n",
      "ADR PREDICTION SCORES:\n",
      "Accuracy: 0.0194\n",
      "F1-Score: 0.0380\n",
      "ROC-AUC:  0.4883\n",
      "==================================================\n",
      "Test Scores at Best Epoch:\n",
      "  DTI - Acc: 0.6584, F1: 0.5443, AUC: 0.7714\n",
      "  ADR - Acc: 0.0194, F1: 0.0380, AUC: 0.4883\n",
      "\n",
      "Saved report to:\n",
      " - F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\final_report.txt\n",
      " - F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\final_report.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fahmid\\AppData\\Local\\Temp\\ipykernel_31012\\220608369.py:25: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n",
      "  return float(np.trapz(y[order], x[order]))\n"
     ]
    }
   ],
   "source": [
    "# === Cell 11: Final Report (DTI & ADR) — pretty print + save ===\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "# --- small metric helpers (dup from earlier so this cell is standalone) ---\n",
    "def _binary_classification_curves(y_true: np.ndarray, y_score: np.ndarray):\n",
    "    order = np.argsort(-y_score)\n",
    "    y_true = y_true[order]\n",
    "    y_score = y_score[order]\n",
    "    tp = np.cumsum(y_true == 1)\n",
    "    fp = np.cumsum(y_true == 0)\n",
    "    fn = tp[-1] - tp\n",
    "    tn = fp[-1] - fp\n",
    "    tpr = tp / np.maximum(tp[-1], 1)\n",
    "    fpr = fp / np.maximum(fp[-1], 1)\n",
    "    precision = tp / np.maximum(tp + fp, 1)\n",
    "    recall = tp / np.maximum(tp[-1], 1)\n",
    "    return fpr, tpr, precision, recall\n",
    "\n",
    "def _auc(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    if len(x) < 2: return float(\"nan\")\n",
    "    order = np.argsort(x)\n",
    "    return float(np.trapz(y[order], x[order]))\n",
    "\n",
    "def roc_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    fpr, tpr, *_ = _binary_classification_curves(y_true, y_score)\n",
    "    return _auc(fpr, tpr)\n",
    "\n",
    "def best_f1_threshold(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    # sweep on unique scores (cap to 200 evenly-spaced if huge for speed)\n",
    "    uniq = np.unique(y_score)\n",
    "    if uniq.size > 200:\n",
    "        uniq = np.linspace(0.0, 1.0, 200)\n",
    "    best_t, best_f1 = 0.5, 0.0\n",
    "    for t in uniq:\n",
    "        yhat = (y_score >= t).astype(np.int32)\n",
    "        tp = (yhat & (y_true==1)).sum()\n",
    "        fp = (yhat & (y_true==0)).sum()\n",
    "        fn = ((1-yhat) & (y_true==1)).sum()\n",
    "        prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "        rec  = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
    "        f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = float(f1), float(t)\n",
    "    return float(best_t)\n",
    "\n",
    "def acc_f1(y_true_bin: np.ndarray, y_pred_bin: np.ndarray) -> tuple[float,float]:\n",
    "    acc = (y_true_bin == y_pred_bin).mean() if y_true_bin.size else float(\"nan\")\n",
    "    tp = np.logical_and(y_pred_bin==1, y_true_bin==1).sum()\n",
    "    fp = np.logical_and(y_pred_bin==1, y_true_bin==0).sum()\n",
    "    fn = np.logical_and(y_pred_bin==0, y_true_bin==1).sum()\n",
    "    prec = tp / (tp+fp) if (tp+fp)>0 else 0.0\n",
    "    rec  = tp / (tp+fn) if (tp+fn)>0 else 0.0\n",
    "    f1 = 2*prec*rec/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "    return float(acc), float(f1)\n",
    "\n",
    "# --- load best threshold + temperature (if present) ---\n",
    "best_val_path = OUT_DIR / \"best_metrics_val.json\"\n",
    "calib_path = OUT_DIR / \"calibration.json\"\n",
    "dti_thr = 0.5\n",
    "if best_val_path.exists():\n",
    "    try:\n",
    "        dti_thr = json.loads(best_val_path.read_text())[\"dti_thr\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "T_STAR = 1.0\n",
    "if calib_path.exists():\n",
    "    try:\n",
    "        T_STAR = json.loads(calib_path.read_text())[\"temperature\"]\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# --- helper to gather scores from a loader ---\n",
    "@torch.no_grad()\n",
    "def collect_dti_scores(loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    for batch in loader:\n",
    "        y = batch[\"y\"].to(DEVICE, dtype=torch.float32)\n",
    "        out = model(\n",
    "            x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "            x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "            t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "            pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "            amp_enabled=AMP_ENABLED, amp_dtype=AMP_DTYPE\n",
    "        )\n",
    "        prob = torch.sigmoid(out[\"logits\"] / T_STAR).float().cpu().numpy()\n",
    "        ys.append(y.cpu().numpy().astype(np.int32))\n",
    "        ps.append(prob)\n",
    "    if not ys: return np.array([]), np.array([])\n",
    "    return np.concatenate(ys), np.concatenate(ps)\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_adr_predictions(loader):\n",
    "    \"\"\"\n",
    "    Returns flattened arrays over all unique-drug rows:\n",
    "      y_true_bin: (U_total*K,) with 1 if TF-IDF>0 else 0\n",
    "      y_score:    (U_total*K,) predicted scores in [0,1] via Softplus regression normalized to [0,1] per column proxy\n",
    "    We use raw regression outputs; for classification we will select a threshold on VAL.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Ys, Ps = [], []\n",
    "    for batch in loader:\n",
    "        out = model(\n",
    "            x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "            x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "            t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "            pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "            amp_enabled=AMP_ENABLED, amp_dtype=AMP_DTYPE\n",
    "        )\n",
    "        y_hat = out[\"y_adr_hat\"].detach().float().cpu().numpy()  # [U, K], >=0\n",
    "        t_true = batch[\"t\"].detach().float().cpu().numpy()       # [U, K]\n",
    "        # Turn regression into a probability-like score in [0,1]:\n",
    "        # Normalize by (1 + value) which is a simple monotonic squash.\n",
    "        p = (y_hat / (1.0 + y_hat)).astype(np.float32)\n",
    "        yb = (t_true > 0.0).astype(np.int32)\n",
    "        Ps.append(p.reshape(-1))\n",
    "        Ys.append(yb.reshape(-1))\n",
    "    if not Ys: return np.array([]), np.array([])\n",
    "    return np.concatenate(Ys), np.concatenate(Ps)\n",
    "\n",
    "# --- collect scores ---\n",
    "y_val_dti, p_val_dti   = collect_dti_scores(val_loader)\n",
    "y_test_dti, p_test_dti = collect_dti_scores(test_loader)\n",
    "\n",
    "y_val_adr, p_val_adr   = collect_adr_predictions(val_loader)\n",
    "y_test_adr, p_test_adr = collect_adr_predictions(test_loader)\n",
    "\n",
    "# --- choose ADR threshold on VAL to maximize macro F1 ---\n",
    "adr_thr = best_f1_threshold(y_val_adr, p_val_adr) if y_val_adr.size else 0.5\n",
    "\n",
    "# --- compute metrics ---\n",
    "# DTI\n",
    "dti_auc  = roc_auc(y_test_dti, p_test_dti) if y_test_dti.size else float(\"nan\")\n",
    "dti_pred = (p_test_dti >= float(dti_thr)).astype(np.int32) if y_test_dti.size else np.array([])\n",
    "dti_acc, dti_f1 = acc_f1(y_test_dti, dti_pred) if y_test_dti.size else (float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "# ADR (macro over all ADR labels across drugs)\n",
    "adr_auc  = roc_auc(y_test_adr, p_test_adr) if y_test_adr.size else float(\"nan\")\n",
    "adr_pred = (p_test_adr >= float(adr_thr)).astype(np.int32) if y_test_adr.size else np.array([])\n",
    "adr_acc, adr_f1 = acc_f1(y_test_adr, adr_pred) if y_test_adr.size else (float(\"nan\"), float(\"nan\"))\n",
    "\n",
    "# --- pretty print ---\n",
    "def _fmt(x): \n",
    "    return \"nan\" if (x!=x) else f\"{x:.4f}\"\n",
    "\n",
    "print(\"\\nDTI PREDICTION SCORES:\")\n",
    "print(f\"Accuracy: {_fmt(dti_acc)}\")\n",
    "print(f\"F1-Score: {_fmt(dti_f1)}\")\n",
    "print(f\"ROC-AUC:  {_fmt(dti_auc)}\\n\")\n",
    "\n",
    "print(\"ADR PREDICTION SCORES:\")\n",
    "print(f\"Accuracy: {_fmt(adr_acc)}\")\n",
    "print(f\"F1-Score: {_fmt(adr_f1)}\")\n",
    "print(f\"ROC-AUC:  {_fmt(adr_auc)}\")\n",
    "print(\"==================================================\")\n",
    "print(\"Test Scores at Best Epoch:\")\n",
    "print(f\"  DTI - Acc: {_fmt(dti_acc)}, F1: {_fmt(dti_f1)}, AUC: {_fmt(dti_auc)}\")\n",
    "print(f\"  ADR - Acc: {_fmt(adr_acc)}, F1: {_fmt(adr_f1)}, AUC: {_fmt(adr_auc)}\")\n",
    "\n",
    "# --- save to files ---\n",
    "report_txt = OUT_DIR / \"final_report.txt\"\n",
    "report_json = OUT_DIR / \"final_report.json\"\n",
    "with open(report_txt, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\n",
    "        f\"DTI PREDICTION SCORES:\\n\"\n",
    "        f\"Accuracy: {_fmt(dti_acc)}\\n\"\n",
    "        f\"F1-Score: {_fmt(dti_f1)}\\n\"\n",
    "        f\"ROC-AUC:  {_fmt(dti_auc)}\\n\\n\"\n",
    "        f\"ADR PREDICTION SCORES:\\n\"\n",
    "        f\"Accuracy: {_fmt(adr_acc)}\\n\"\n",
    "        f\"F1-Score: {_fmt(adr_f1)}\\n\"\n",
    "        f\"ROC-AUC:  {_fmt(adr_auc)}\\n\"\n",
    "        f\"==================================================\\n\"\n",
    "        f\"Test Scores at Best Epoch:\\n\"\n",
    "        f\"  DTI - Acc: {_fmt(dti_acc)}, F1: {_fmt(dti_f1)}, AUC: {_fmt(dti_auc)}\\n\"\n",
    "        f\"  ADR - Acc: {_fmt(adr_acc)}, F1: {_fmt(adr_f1)}, AUC: {_fmt(adr_auc)}\\n\"\n",
    "    )\n",
    "json.dump({\n",
    "    \"DTI\": {\"accuracy\": dti_acc, \"f1\": dti_f1, \"roc_auc\": dti_auc, \"threshold\": float(dti_thr), \"temperature\": float(T_STAR)},\n",
    "    \"ADR\": {\"accuracy\": adr_acc, \"f1\": adr_f1, \"roc_auc\": adr_auc, \"threshold\": float(adr_thr)},\n",
    "}, open(report_json, \"w\"), indent=2)\n",
    "\n",
    "print(f\"\\nSaved report to:\\n - {report_txt}\\n - {report_json}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2cb918",
   "metadata": {},
   "source": [
    "Cell 6.5 — Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "264f5e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated temperature T* = 3.338\n"
     ]
    }
   ],
   "source": [
    "# === Cell 6.5: Temperature scaling on VAL logits ===\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "model.eval()\n",
    "# Collect raw logits and labels on VAL\n",
    "logits_val, labels_val = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        out = model(\n",
    "            x_d=batch[\"x_d\"].to(DEVICE),\n",
    "            x_p=batch[\"x_p\"].to(DEVICE),\n",
    "            t=batch[\"t\"].to(DEVICE),\n",
    "            pair_to_u=batch[\"pair_to_u\"].to(DEVICE),\n",
    "            amp_enabled=AMP_ENABLED, amp_dtype=AMP_DTYPE\n",
    "        )\n",
    "        logits_val.append(out[\"logits\"].detach().float().cpu())\n",
    "        labels_val.append(batch[\"y\"].detach().float().cpu())\n",
    "logits_val = torch.cat(logits_val)  # [N]\n",
    "labels_val = torch.cat(labels_val)  # [N]\n",
    "\n",
    "# Optimize a scalar temperature T ≥ 0.5 for numerical stability\n",
    "T = torch.nn.Parameter(torch.tensor(1.0, dtype=torch.float32))\n",
    "opt = torch.optim.LBFGS([T], lr=0.1, max_iter=50, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "def _nll():\n",
    "    opt.zero_grad()\n",
    "    p = torch.sigmoid(logits_val / torch.clamp(T, min=0.5))\n",
    "    # Binary NLL\n",
    "    loss = -(labels_val*torch.log(p+1e-9) + (1-labels_val)*torch.log(1-p+1e-9)).mean()\n",
    "    loss.backward()\n",
    "    return loss\n",
    "\n",
    "opt.step(_nll)\n",
    "T_star = float(T.item())\n",
    "print(f\"Calibrated temperature T* = {T_star:.3f}\")\n",
    "\n",
    "# Save for inference/eval\n",
    "with open(OUT_DIR / \"calibration.json\", \"w\") as f:\n",
    "    json.dump({\"temperature\": T_star}, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cf4be53",
   "metadata": {},
   "source": [
    "Cell 7 — Inference utilities — p(bind) + top-k ADRs using prototypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "5b210c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_rxnorm_meddra_v2.parquet\n",
      "Loaded MedDRA name map: final_rxnorm_meddra_v2.parquet  (rows=4817)\n",
      "Inference API (with ADR names) ready. Use:\n",
      "Drug=CHEMBL1009 | Protein=Q9UHI5\n",
      "p_bind = 0.8315  (val-threshold* ≈ 0.752)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "adr_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "adr_name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "9c43677a-4865-44c9-b7a3-442cfd11c4a8",
       "rows": [
        [
         "0",
         "1",
         "10007554",
         "Cardiac failure",
         "0.5119044"
        ],
        [
         "1",
         "2",
         "10002959",
         "Aphthous ulcer",
         "0.5113886"
        ],
        [
         "2",
         "3",
         "10002967",
         "Aplastic anaemia",
         "0.5108823"
        ],
        [
         "3",
         "4",
         "10067125",
         "Liver injury",
         "0.5106938"
        ],
        [
         "4",
         "5",
         "10038923",
         "Retinopathy",
         "0.50908244"
        ],
        [
         "5",
         "6",
         "10054524",
         "Palmar-plantar erythrodysesthesia syndrome",
         "0.5090577"
        ],
        [
         "6",
         "7",
         "10010770",
         "Consciousness disturbed",
         "0.5089958"
        ],
        [
         "7",
         "8",
         "10059206",
         "Nail toxicity",
         "0.5088989"
        ],
        [
         "8",
         "9",
         "10047862",
         "Weakness",
         "0.5085048"
        ],
        [
         "9",
         "10",
         "10012536",
         "Detachment",
         "0.50849277"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>adr_id</th>\n",
       "      <th>adr_name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10007554</td>\n",
       "      <td>Cardiac failure</td>\n",
       "      <td>0.511904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10002959</td>\n",
       "      <td>Aphthous ulcer</td>\n",
       "      <td>0.511389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10002967</td>\n",
       "      <td>Aplastic anaemia</td>\n",
       "      <td>0.510882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10067125</td>\n",
       "      <td>Liver injury</td>\n",
       "      <td>0.510694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10038923</td>\n",
       "      <td>Retinopathy</td>\n",
       "      <td>0.509082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10054524</td>\n",
       "      <td>Palmar-plantar erythrodysesthesia syndrome</td>\n",
       "      <td>0.509058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>10010770</td>\n",
       "      <td>Consciousness disturbed</td>\n",
       "      <td>0.508996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10059206</td>\n",
       "      <td>Nail toxicity</td>\n",
       "      <td>0.508899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10047862</td>\n",
       "      <td>Weakness</td>\n",
       "      <td>0.508505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10012536</td>\n",
       "      <td>Detachment</td>\n",
       "      <td>0.508493</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank    adr_id                                    adr_name     score\n",
       "0     1  10007554                             Cardiac failure  0.511904\n",
       "1     2  10002959                              Aphthous ulcer  0.511389\n",
       "2     3  10002967                            Aplastic anaemia  0.510882\n",
       "3     4  10067125                                Liver injury  0.510694\n",
       "4     5  10038923                                 Retinopathy  0.509082\n",
       "5     6  10054524  Palmar-plantar erythrodysesthesia syndrome  0.509058\n",
       "6     7  10010770                     Consciousness disturbed  0.508996\n",
       "7     8  10059206                               Nail toxicity  0.508899\n",
       "8     9  10047862                                    Weakness  0.508505\n",
       "9    10  10012536                                  Detachment  0.508493"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 7 (updated): Inference + ADR names via MedDRA mapping parquet in Model_v1/ ===\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- Load best checkpoint ---\n",
    "model_ckpt_path = OUT_DIR / \"best.pt\"\n",
    "if not model_ckpt_path.exists():\n",
    "    raise FileNotFoundError(f\"Best checkpoint not found at {model_ckpt_path}\")\n",
    "\n",
    "ckpt = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# --- Helper: find the MedDRA mapping parquet at Model_v1 root ---\n",
    "# It must contain the 3 columns:\n",
    "#   rxnorm_ingredient_id (object), meddra_id (int), meddra_name (object)\n",
    "def find_meddra_map_parquet(nb_root: Path) -> Path | None:\n",
    "    for p in nb_root.glob(\"*.parquet\"):\n",
    "        try:\n",
    "            df = pd.read_parquet(p, columns=[\"rxnorm_ingredient_id\", \"meddra_id\", \"meddra_name\"])\n",
    "            # lightweight schema verification\n",
    "            if {\"rxnorm_ingredient_id\",\"meddra_id\",\"meddra_name\"}.issubset(df.columns):\n",
    "                return p\n",
    "        except Exception:\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "# meddra_map_path = find_meddra_map_parquet(NB_ROOT)\n",
    "meddra_map_path = \"final_rxnorm_meddra_v2.parquet\"\n",
    "\n",
    "print(meddra_map_path)\n",
    "if meddra_map_path is None:\n",
    "    print(\"⚠️ Could not find a MedDRA mapping parquet at Model_v1/*.parquet \"\n",
    "          \"with columns [rxnorm_ingredient_id, meddra_id, meddra_name]. \"\n",
    "          \"ADR names will fall back to idf_table’s first column.\")\n",
    "    meddra_map_df = None\n",
    "else:\n",
    "    meddra_map_df = pd.read_parquet(meddra_map_path, columns=[\"meddra_id\",\"meddra_name\"]).drop_duplicates()\n",
    "    print(f\"Loaded MedDRA name map: {meddra_map_path}  (rows={len(meddra_map_df)})\")\n",
    "\n",
    "# --- Load ADR prototypes (or build) ---\n",
    "proto_path = Path(cfg.get(\"artifacts\", {}).get(\"prototypes_path\", OUT_DIR / \"prototypes_C.npy\"))\n",
    "if proto_path.exists():\n",
    "    C = np.load(proto_path)\n",
    "else:\n",
    "    with torch.no_grad():\n",
    "        W = model.f_a(T_train.to(DEVICE, dtype=torch.float32))\n",
    "        W = W.detach().cpu().numpy()  # [U_train, d]\n",
    "        T_np = T_train.cpu().numpy()  # [U_train, K]\n",
    "        K = T_np.shape[1]; d = W.shape[1]\n",
    "        C = np.zeros((K, d), dtype=np.float32)\n",
    "        for k in range(K):\n",
    "            mask = T_np[:, k] > 0\n",
    "            C[k] = W[mask].mean(axis=0) if mask.any() else 0.0\n",
    "np.testing.assert_equal(C.shape[0], int(cfg[\"data\"][\"K\"]))\n",
    "C_tensor = torch.from_numpy(C).to(DEVICE, dtype=torch.float32)  # [K, d]\n",
    "\n",
    "# --- Build ADR ID & Name lists (from idf_table + optional MedDRA join) ---\n",
    "idf_table_path = (NB_ROOT / cfg[\"data\"][\"adr_root\"] / \"idf_table.parquet\").resolve()\n",
    "idf_table = pd.read_parquet(idf_table_path)\n",
    "\n",
    "# Heuristic: first column is the ADR identifier (often MedDRA ID or term)\n",
    "ADR_ID_COL = idf_table.columns[0]\n",
    "adr_ids_raw = idf_table[ADR_ID_COL]\n",
    "\n",
    "# Try to coerce to int MedDRA IDs (safe if already strings of ints)\n",
    "adr_ids_int = None\n",
    "try:\n",
    "    adr_ids_int = pd.to_numeric(adr_ids_raw, errors=\"raise\").astype(\"int64\")\n",
    "except Exception:\n",
    "    # Not numeric; leave None\n",
    "    pass\n",
    "\n",
    "if meddra_map_df is not None and adr_ids_int is not None:\n",
    "    # Join on meddra_id to get meddra_name\n",
    "    names_df = pd.DataFrame({\"meddra_id\": adr_ids_int})\n",
    "    names_df = names_df.merge(meddra_map_df, on=\"meddra_id\", how=\"left\")\n",
    "    ADR_NAMES = names_df[\"meddra_name\"].fillna(adr_ids_raw.astype(str)).astype(str).tolist()\n",
    "    ADR_IDS_DISPLAY = adr_ids_int.astype(int).tolist()\n",
    "else:\n",
    "    # Fallback to whatever the idf_table’s first column provides\n",
    "    ADR_NAMES = adr_ids_raw.astype(str).tolist()\n",
    "    ADR_IDS_DISPLAY = adr_ids_raw.astype(str).tolist()\n",
    "\n",
    "assert len(ADR_NAMES) == C.shape[0], \"ADR names length != K\"\n",
    "\n",
    "# --- Scoring weights for ADR ranking ---\n",
    "alpha = float(cfg[\"model\"][\"adr_scoring\"][\"alpha\"])\n",
    "beta  = float(cfg[\"model\"][\"adr_scoring\"][\"beta\"])\n",
    "gamma = float(cfg[\"model\"][\"adr_scoring\"][\"gamma\"])\n",
    "\n",
    "@torch.no_grad()\n",
    "def _encode_drug_protein(drug_id: str, prot_id: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if drug_id not in DRUG_ID2IDX:\n",
    "        raise KeyError(f\"Unknown drug_chembl_id: {drug_id}\")\n",
    "    if prot_id not in PROT_ID2IDX:\n",
    "        raise KeyError(f\"Unknown protein id: {prot_id}\")\n",
    "\n",
    "    di = DRUG_ID2IDX[drug_id]\n",
    "    pi = PROT_ID2IDX[prot_id]\n",
    "\n",
    "    x_d = DRUG_TENSOR[di:di+1].to(DEVICE, dtype=torch.float32)\n",
    "    x_p = PROT_TENSOR[pi:pi+1].to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "    u = model.f_d(x_d)  # [1, d]\n",
    "    v = model.f_p(x_p)  # [1, d]\n",
    "    return u, v\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_pair(\n",
    "    drug_chembl_id: str,\n",
    "    protein_id: str,\n",
    "    topk: int = 10,\n",
    "    return_dataframe: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dict with p_bind, threshold_used, topk DataFrame:\n",
    "      columns = [rank, adr_id, adr_name, score]\n",
    "    \"\"\"\n",
    "    u, v = _encode_drug_protein(drug_chembl_id, protein_id)  # [1,d] each\n",
    "\n",
    "    # Binding probability\n",
    "    logits = model.h_dti(u, v)             # [1]\n",
    "    # p_bind = torch.sigmoid(logits).item()\n",
    "    \n",
    "    # load T once near top of Cell 7\n",
    "    calib_path = OUT_DIR / \"calibration.json\"\n",
    "    T_STAR = json.loads(calib_path.read_text())[\"temperature\"] if calib_path.exists() else 1.0\n",
    "\n",
    "    # inside predict_pair (after computing logits)\n",
    "    p_bind = torch.sigmoid(logits / T_STAR).item()\n",
    "\n",
    "\n",
    "    # Pair-conditioned ADR score:\n",
    "    # score_k = sigmoid( α*(u·Cᵀ) + β*(v·Cᵀ) + γ*(u·v) )\n",
    "    u_n = u / (u.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
    "    v_n = v / (v.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
    "    C_n = C_tensor / (C_tensor.norm(p=1, dim=1, keepdim=False).unsqueeze(1) + 1e-8)\n",
    "\n",
    "    s_u = torch.matmul(u_n, C_n.T)  # [1, K]\n",
    "    s_v = torch.matmul(v_n, C_n.T)  # [1, K]\n",
    "    pair_sim = torch.sum(u_n * v_n, dim=1, keepdim=True)  # [1,1]\n",
    "    scores = torch.sigmoid(alpha * s_u + beta * s_v + gamma * pair_sim).squeeze(0)  # [K]\n",
    "\n",
    "    # Top-k ADRs\n",
    "    topk = int(min(topk, scores.numel()))\n",
    "    idx = torch.topk(scores, k=topk, largest=True, sorted=True).indices.detach().cpu().numpy()\n",
    "    vals = scores.detach().cpu().numpy()[idx]\n",
    "\n",
    "    # Build output with names\n",
    "    adr_ids_sel   = [ADR_IDS_DISPLAY[i] for i in idx]\n",
    "    adr_names_sel = [ADR_NAMES[i] for i in idx]\n",
    "\n",
    "    if return_dataframe:\n",
    "        df = pd.DataFrame({\n",
    "            \"rank\": np.arange(1, topk+1, dtype=int),\n",
    "            \"adr_id\": adr_ids_sel,\n",
    "            \"adr_name\": adr_names_sel,\n",
    "            \"score\": vals\n",
    "        })\n",
    "        print(f\"Drug={drug_chembl_id} | Protein={protein_id}\")\n",
    "        print(f\"p_bind = {p_bind:.4f}  (val-threshold* ≈ {ckpt.get('best_thr', np.nan):.3f})\")\n",
    "        display(df)\n",
    "        return {\n",
    "            \"p_bind\": p_bind,\n",
    "            \"threshold_used\": float(ckpt.get(\"best_thr\", np.nan)),\n",
    "            \"topk\": df\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"p_bind\": p_bind,\n",
    "            \"threshold_used\": float(ckpt.get(\"best_thr\", np.nan)),\n",
    "            \"topk_idx\": idx,\n",
    "            \"topk_scores\": vals,\n",
    "            \"topk_adr_ids\": adr_ids_sel,\n",
    "            \"topk_adr_names\": adr_names_sel\n",
    "        }\n",
    "\n",
    "print(\"Inference API (with ADR names) ready. Use:\")\n",
    "_ = predict_pair('CHEMBL1009', 'Q9UHI5', topk=10, return_dataframe=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c2e081db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drug=CHEMBL1009 | Protein=P21918\n",
      "p_bind = 0.6884  (val-threshold* ≈ 0.605)\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "rank",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "adr",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "score",
         "rawType": "float32",
         "type": "float"
        }
       ],
       "ref": "9fac0ebc-8d5e-456b-b63d-33b35071dd15",
       "rows": [
        [
         "0",
         "1",
         "10024492",
         "0.52133936"
        ],
        [
         "1",
         "2",
         "10018473",
         "0.5211203"
        ],
        [
         "2",
         "3",
         "10018232",
         "0.5210758"
        ],
        [
         "3",
         "4",
         "10074859",
         "0.5206071"
        ],
        [
         "4",
         "5",
         "10043088",
         "0.5204162"
        ],
        [
         "5",
         "6",
         "10020916",
         "0.52035475"
        ],
        [
         "6",
         "7",
         "10020915",
         "0.52035475"
        ],
        [
         "7",
         "8",
         "10003458",
         "0.5203006"
        ],
        [
         "8",
         "9",
         "10054209",
         "0.5202008"
        ],
        [
         "9",
         "10",
         "10012703",
         "0.52017635"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 10
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rank</th>\n",
       "      <th>adr</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10024492</td>\n",
       "      <td>0.521339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>10018473</td>\n",
       "      <td>0.521120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>10018232</td>\n",
       "      <td>0.521076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>10074859</td>\n",
       "      <td>0.520607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>10043088</td>\n",
       "      <td>0.520416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>10020916</td>\n",
       "      <td>0.520355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>10020915</td>\n",
       "      <td>0.520355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>10003458</td>\n",
       "      <td>0.520301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>10054209</td>\n",
       "      <td>0.520201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>10012703</td>\n",
       "      <td>0.520176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rank       adr     score\n",
       "0     1  10024492  0.521339\n",
       "1     2  10018473  0.521120\n",
       "2     3  10018232  0.521076\n",
       "3     4  10074859  0.520607\n",
       "4     5  10043088  0.520416\n",
       "5     6  10020916  0.520355\n",
       "6     7  10020915  0.520355\n",
       "7     8  10003458  0.520301\n",
       "8     9  10054209  0.520201\n",
       "9    10  10012703  0.520176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 7: Inference utilities — p(bind) + top-k ADRs using prototypes ===\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "model_ckpt_path = OUT_DIR / \"best.pt\"\n",
    "if not model_ckpt_path.exists():\n",
    "    raise FileNotFoundError(f\"Best checkpoint not found at {model_ckpt_path}\")\n",
    "\n",
    "ckpt = torch.load(model_ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(ckpt[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# Load ADR prototypes (or build from train split if not present)\n",
    "proto_path = Path(cfg.get(\"artifacts\", {}).get(\"prototypes_path\", OUT_DIR / \"prototypes_C.npy\"))\n",
    "if proto_path.exists():\n",
    "    C = np.load(proto_path)\n",
    "else:\n",
    "    # Fallback: compute from TRAIN split quickly (same logic as in Cell 6)\n",
    "    with torch.no_grad():\n",
    "        W = model.f_a(T_train.to(DEVICE, dtype=torch.float32))\n",
    "        W = W.detach().cpu().numpy()  # [U_train, d]\n",
    "        T_np = T_train.cpu().numpy()  # [U_train, K]\n",
    "        K = T_np.shape[1]; d = W.shape[1]\n",
    "        C = np.zeros((K, d), dtype=np.float32)\n",
    "        for k in range(K):\n",
    "            mask = T_np[:, k] > 0\n",
    "            C[k] = W[mask].mean(axis=0) if mask.any() else 0.0\n",
    "np.testing.assert_equal(C.shape[0], int(cfg[\"data\"][\"K\"]))\n",
    "C_tensor = torch.from_numpy(C).to(DEVICE, dtype=torch.float32)  # [K, d]\n",
    "\n",
    "# ADR labels/IDs for display\n",
    "idf_table_path = (NB_ROOT / cfg[\"data\"][\"adr_root\"] / \"idf_table.parquet\").resolve()\n",
    "idf_table = pd.read_parquet(idf_table_path)\n",
    "# Heuristic: use the first column as ADR key/name\n",
    "ADR_NAME_COL = idf_table.columns[0]\n",
    "ADR_NAMES = idf_table[ADR_NAME_COL].astype(str).tolist()\n",
    "assert len(ADR_NAMES) == C.shape[0], \"ADR names length != K\"\n",
    "\n",
    "# Scoring weights for ADR ranking\n",
    "alpha = float(cfg[\"model\"][\"adr_scoring\"][\"alpha\"])\n",
    "beta  = float(cfg[\"model\"][\"adr_scoring\"][\"beta\"])\n",
    "gamma = float(cfg[\"model\"][\"adr_scoring\"][\"gamma\"])\n",
    "\n",
    "@torch.no_grad()\n",
    "def _encode_drug_protein(drug_id: str, prot_id: str) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    if drug_id not in DRUG_ID2IDX:\n",
    "        raise KeyError(f\"Unknown drug_chembl_id: {drug_id}\")\n",
    "    if prot_id not in PROT_ID2IDX:\n",
    "        raise KeyError(f\"Unknown protein id: {prot_id}\")\n",
    "\n",
    "    di = DRUG_ID2IDX[drug_id]\n",
    "    pi = PROT_ID2IDX[prot_id]\n",
    "\n",
    "    x_d = DRUG_TENSOR[di:di+1].to(DEVICE, dtype=torch.float32)\n",
    "    x_p = PROT_TENSOR[pi:pi+1].to(DEVICE, dtype=torch.float32)\n",
    "\n",
    "    # We don’t need TF-IDF here for inference; pass a dummy minimal tensor to satisfy API?\n",
    "    # The forward() requires t and pair_to_u; we’ll bypass heads and call adapters directly for speed.\n",
    "    u = model.f_d(x_d)  # [1, d]\n",
    "    v = model.f_p(x_p)  # [1, d]\n",
    "    return u, v\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_pair(\n",
    "    drug_chembl_id: str,\n",
    "    protein_id: str,\n",
    "    topk: int = 10,\n",
    "    return_dataframe: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      dict with p_bind, threshold_used, topk DataFrame (ADR, score) or numpy arrays.\n",
    "    \"\"\"\n",
    "    u, v = _encode_drug_protein(drug_chembl_id, protein_id)  # [1,d] each\n",
    "    # Binding probability from the selected head\n",
    "    if model.dti_head_type == \"cosine\":\n",
    "        logits = model.h_dti(u, v)           # [1]\n",
    "    else:\n",
    "        logits = model.h_dti(u, v)           # [1]\n",
    "    p_bind = torch.sigmoid(logits).item()\n",
    "\n",
    "    # Pair-conditioned ADR score:\n",
    "    # score_k = sigmoid( α*(u·Cᵀ) + β*(v·Cᵀ) + γ*(u·v) )  (broadcast γ term)\n",
    "    u_n = u / (u.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
    "    v_n = v / (v.norm(p=2, dim=1, keepdim=True) + 1e-8)\n",
    "    C_n = C_tensor / (C_tensor.norm(p=1, dim=1, keepdim=False).unsqueeze(1) + 1e-8)  # light norm for stability\n",
    "\n",
    "    s_u = torch.matmul(u_n, C_n.T)  # [1, K]\n",
    "    s_v = torch.matmul(v_n, C_n.T)  # [1, K]\n",
    "    pair_sim = torch.sum(u_n * v_n, dim=1, keepdim=True)  # [1,1]\n",
    "    scores = torch.sigmoid(alpha * s_u + beta * s_v + gamma * pair_sim).squeeze(0)  # [K]\n",
    "\n",
    "    # Top-k ADRs\n",
    "    topk = int(min(topk, scores.numel()))\n",
    "    idx = torch.topk(scores, k=topk, largest=True, sorted=True).indices.detach().cpu().numpy()\n",
    "    vals = scores.detach().cpu().numpy()[idx]\n",
    "\n",
    "    if return_dataframe:\n",
    "        df = pd.DataFrame({\n",
    "            \"rank\": np.arange(1, topk+1, dtype=int),\n",
    "            \"adr\":  [ADR_NAMES[i] for i in idx],\n",
    "            \"score\": vals\n",
    "        })\n",
    "        # pretty print\n",
    "        print(f\"Drug={drug_chembl_id} | Protein={protein_id}\")\n",
    "        print(f\"p_bind = {p_bind:.4f}  (val-threshold* ≈ {ckpt.get('best_thr', np.nan):.3f})\")\n",
    "        display(df)\n",
    "        return {\n",
    "            \"p_bind\": p_bind,\n",
    "            \"threshold_used\": float(ckpt.get(\"best_thr\", np.nan)),\n",
    "            \"topk\": df\n",
    "        }\n",
    "    else:\n",
    "        return {\n",
    "            \"p_bind\": p_bind,\n",
    "            \"threshold_used\": float(ckpt.get(\"best_thr\", np.nan)),\n",
    "            \"topk_idx\": idx,\n",
    "            \"topk_scores\": vals\n",
    "        }\n",
    "\n",
    "# --------- Quick demo (edit IDs as needed) ----------\n",
    "# Example: pick any valid IDs from DRUG_ID_LIST / PROT_ID_LIST\n",
    "# _ = predict_pair(DRUG_ID_LIST[0], PROT_ID_LIST[0], topk=10, return_dataframe=True)\n",
    "_ = predict_pair(\"CHEMBL1009\", \"P21918\", topk=10, return_dataframe=True)\n",
    "\n",
    "# print(\"Inference API ready. Use: predict_pair('<drug_chembl_id>', '<protein_id>', topk=10)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19731275",
   "metadata": {},
   "source": [
    "Cell 8 — Export compact deployment bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "b868130e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deployment bundle ready at: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\deploy_bundle\n",
      "Contents:\n",
      "- model_state.pt\n",
      "- config.yaml\n",
      "- best_metrics_val.json\n",
      "- prototypes_C.npy\n",
      "- adr_labels.csv\n",
      "- predict_pair_minimal.py\n"
     ]
    }
   ],
   "source": [
    "# === Cell 8: Export compact deployment bundle ===\n",
    "from pathlib import Path\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "bundle_dir = OUT_DIR / \"deploy_bundle\"\n",
    "bundle_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- 1. Save model weights (state_dict only) ----\n",
    "weights_path = bundle_dir / \"model_state.pt\"\n",
    "torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "# ---- 2. Copy config & best metrics ----\n",
    "shutil.copy2(CONFIG_PATH, bundle_dir / \"config.yaml\")\n",
    "best_metrics_src = OUT_DIR / \"best_metrics_val.json\"\n",
    "if best_metrics_src.exists():\n",
    "    shutil.copy2(best_metrics_src, bundle_dir / \"best_metrics_val.json\")\n",
    "\n",
    "# ---- 3. Save ADR prototypes ----\n",
    "proto_path = Path(cfg.get(\"artifacts\", {}).get(\"prototypes_path\", OUT_DIR / \"prototypes_C.npy\"))\n",
    "if proto_path.exists():\n",
    "    shutil.copy2(proto_path, bundle_dir / \"prototypes_C.npy\")\n",
    "else:\n",
    "    # quick rebuild from train split if missing\n",
    "    with torch.no_grad():\n",
    "        W = model.f_a(T_train.to(DEVICE, dtype=torch.float32))\n",
    "        W = W.detach().cpu().numpy()\n",
    "        T_np = T_train.cpu().numpy()\n",
    "        K = T_np.shape[1]; d = W.shape[1]\n",
    "        C = np.zeros((K, d), dtype=np.float32)\n",
    "        for k in range(K):\n",
    "            mask = T_np[:, k] > 0\n",
    "            C[k] = W[mask].mean(axis=0) if mask.any() else 0.0\n",
    "        np.save(bundle_dir / \"prototypes_C.npy\", C)\n",
    "\n",
    "# ---- 4. Save ADR label names ----\n",
    "idf_table_path = (NB_ROOT / cfg[\"data\"][\"adr_root\"] / \"idf_table.parquet\").resolve()\n",
    "idf_table = pd.read_parquet(idf_table_path)\n",
    "adr_name_col = idf_table.columns[0]\n",
    "idf_table[[adr_name_col]].to_csv(bundle_dir / \"adr_labels.csv\", index=False)\n",
    "\n",
    "# ---- 5. Save a minimal inference script ----\n",
    "inference_py = bundle_dir / \"predict_pair_minimal.py\"\n",
    "inference_py.write_text(\n",
    "f\"\"\"\\\n",
    "import torch, numpy as np, pandas as pd, json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_bundle(bundle_dir: str):\n",
    "    p = Path(bundle_dir)\n",
    "    cfg = yaml.safe_load(open(p/'config.yaml'))\n",
    "    model_state = torch.load(p/'model_state.pt', map_location='cpu')\n",
    "    C = np.load(p/'prototypes_C.npy')\n",
    "    adr_labels = pd.read_csv(p/'adr_labels.csv')[{repr(adr_name_col)}].tolist()\n",
    "    return cfg, model_state, C, adr_labels\n",
    "\n",
    "# Usage example:\n",
    "# cfg, state, C, labels = load_bundle('runs/dti_adr_v1/deploy_bundle')\n",
    "# print('Loaded bundle with', len(labels), 'ADRs')\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(f\"Deployment bundle ready at: {bundle_dir}\")\n",
    "print(f\"Contents:\\n- model_state.pt\\n- config.yaml\\n- best_metrics_val.json\\n- prototypes_C.npy\\n- adr_labels.csv\\n- predict_pair_minimal.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff17fd92",
   "metadata": {},
   "source": [
    "Cell 9 — Plot & save metrics for this run (loss curves, PR/ROC, ADR, top-k) + run_summary.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "effff42a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plots to: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\n",
      " - plot_loss_curves.png\n",
      " - plot_val_dti_metrics.png\n",
      " - plot_val_adr_errors.png\n",
      " - plot_pr_curves.png\n",
      " - plot_roc_curves.png\n",
      " - plot_VAL_recall_at_k.png\n",
      " - plot_VAL_ndcg_at_k.png\n",
      " - plot_TEST_recall_at_k.png\n",
      " - plot_TEST_ndcg_at_k.png\n",
      "Updated: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1\\run_summary.csv\n"
     ]
    }
   ],
   "source": [
    "# === Cell 9: Visualize & Save Metrics for THIS run ===\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------- Load history / best / test ----------\n",
    "hist_path = OUT_DIR / \"history.json\"\n",
    "best_val_path = OUT_DIR / \"best_metrics_val.json\"\n",
    "test_path = OUT_DIR / \"test_metrics.json\"\n",
    "\n",
    "if not hist_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {hist_path}. Train first (Cell 6).\")\n",
    "\n",
    "history = json.loads(Path(hist_path).read_text(encoding=\"utf-8\"))\n",
    "best_val = json.loads(Path(best_val_path).read_text(encoding=\"utf-8\")) if best_val_path.exists() else {}\n",
    "m_test   = json.loads(Path(test_path).read_text(encoding=\"utf-8\")) if test_path.exists() else {}\n",
    "\n",
    "# Extract per-epoch series\n",
    "epochs = [h[\"epoch\"] for h in history]\n",
    "L_total = [h[\"train\"][\"L_total\"] for h in history]\n",
    "L_dti   = [h[\"train\"][\"L_dti\"] for h in history]\n",
    "L_adr   = [h[\"train\"][\"L_adr\"] for h in history]\n",
    "L_con   = [h[\"train\"][\"L_con\"] for h in history]\n",
    "\n",
    "val_pr  = [h[\"val\"].get(\"dti_pr_auc\", np.nan) for h in history]\n",
    "val_roc = [h[\"val\"].get(\"dti_roc_auc\", np.nan) for h in history]\n",
    "val_f1  = [h[\"val\"].get(\"dti_f1\", np.nan) for h in history]\n",
    "val_thr = [h[\"val\"].get(\"dti_thr\", np.nan) for h in history]\n",
    "val_rmse= [h[\"val\"].get(\"adr_rmse\", np.nan) for h in history]\n",
    "val_mae = [h[\"val\"].get(\"adr_mae\", np.nan) for h in history]\n",
    "\n",
    "# ---------- Helper: recompute PR & ROC curves for nice plots ----------\n",
    "def collect_scores(loader):\n",
    "    model.eval()\n",
    "    y_all, p_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            y = batch[\"y\"].to(DEVICE, dtype=torch.float32, non_blocking=True)\n",
    "            out = model(\n",
    "                x_d=batch[\"x_d\"].to(DEVICE, non_blocking=True),\n",
    "                x_p=batch[\"x_p\"].to(DEVICE, non_blocking=True),\n",
    "                t=batch[\"t\"].to(DEVICE, non_blocking=True),\n",
    "                pair_to_u=batch[\"pair_to_u\"].to(DEVICE, non_blocking=True),\n",
    "                amp_enabled=AMP_ENABLED,\n",
    "                amp_dtype=AMP_DTYPE\n",
    "            )\n",
    "            prob = torch.sigmoid(out[\"logits\"]).detach().float().cpu().numpy()\n",
    "            y_all.append(y.detach().cpu().numpy().astype(np.int32))\n",
    "            p_all.append(prob)\n",
    "    if len(y_all) == 0:\n",
    "        return np.array([]), np.array([])\n",
    "    return np.concatenate(y_all), np.concatenate(p_all)\n",
    "\n",
    "def pr_curve_points(y, p):\n",
    "    # sort by p desc\n",
    "    order = np.argsort(-p)\n",
    "    y = y[order]; p = p[order]\n",
    "    tp = np.cumsum(y == 1)\n",
    "    fp = np.cumsum(y == 0)\n",
    "    precision = tp / np.maximum(tp + fp, 1)\n",
    "    recall    = tp / np.maximum(tp[-1] if tp.size else 1, 1)\n",
    "    return recall, precision\n",
    "\n",
    "def roc_curve_points(y, p):\n",
    "    # sort by p desc\n",
    "    order = np.argsort(-p)\n",
    "    y = y[order]; p = p[order]\n",
    "    tp = np.cumsum(y == 1)\n",
    "    fp = np.cumsum(y == 0)\n",
    "    fn = (y == 1).sum() - tp\n",
    "    tn = (y == 0).sum() - fp\n",
    "    tpr = tp / np.maximum((tp+fn), 1)\n",
    "    fpr = fp / np.maximum((fp+tn), 1)\n",
    "    return fpr, tpr\n",
    "\n",
    "# Collect scores for val & test to draw smooth curves\n",
    "y_val, p_val = collect_scores(val_loader)\n",
    "y_tst, p_tst = collect_scores(test_loader)\n",
    "\n",
    "# ---------- Plot 1: Training loss curves ----------\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(epochs, L_total, label=\"Total\")\n",
    "plt.plot(epochs, L_dti,   label=\"DTI\")\n",
    "plt.plot(epochs, L_adr,   label=\"ADR\")\n",
    "plt.plot(epochs, L_con,   label=\"Contrastive\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Training Loss Curves\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_loss_curves.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- Plot 2: Validation DTI metrics by epoch ----------\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(epochs, val_pr,  label=\"PR-AUC\")\n",
    "plt.plot(epochs, val_roc, label=\"ROC-AUC\")\n",
    "plt.plot(epochs, val_f1,  label=\"F1\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Validation DTI Metrics\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_val_dti_metrics.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- Plot 3: Validation ADR errors (RMSE/MAE) ----------\n",
    "plt.figure(figsize=(7,4.5))\n",
    "plt.plot(epochs, val_rmse, label=\"ADR RMSE\")\n",
    "plt.plot(epochs, val_mae,  label=\"ADR MAE\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Error\"); plt.title(\"Validation ADR Errors\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(OUT_DIR / \"plot_val_adr_errors.png\", dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# ---------- Plot 4: PR & ROC curves (val & test) ----------\n",
    "if y_val.size > 0 and y_tst.size > 0:\n",
    "    rv, pv = pr_curve_points(y_val, p_val)\n",
    "    rt, pt = pr_curve_points(y_tst, p_tst)\n",
    "    plt.figure(figsize=(6.2,4.5))\n",
    "    plt.plot(rv, pv, label=f\"VAL (PR-AUC={best_val.get('dti_pr_auc', np.nan):.3f})\")\n",
    "    plt.plot(rt, pt, label=f\"TEST (PR-AUC={m_test.get('dti_pr_auc', np.nan):.3f})\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\"); plt.title(\"Precision–Recall\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"plot_pr_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "    fv, tv = roc_curve_points(y_val, p_val)\n",
    "    ft, tt = roc_curve_points(y_tst, p_tst)\n",
    "    plt.figure(figsize=(6.2,4.5))\n",
    "    plt.plot(fv, tv, label=f\"VAL (ROC-AUC={best_val.get('dti_roc_auc', np.nan):.3f})\")\n",
    "    plt.plot(ft, tt, label=f\"TEST (ROC-AUC={m_test.get('dti_roc_auc', np.nan):.3f})\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\"); plt.title(\"ROC\")\n",
    "    plt.legend(); plt.tight_layout()\n",
    "    plt.savefig(OUT_DIR / \"plot_roc_curves.png\", dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# ---------- Plot 5: Top-k ranking (if recorded in best/test) ----------\n",
    "def _maybe_plot_topk(prefix: str, metrics: dict):\n",
    "    ks = [k for k in metrics.keys() if k.startswith(\"recall@\") or k.startswith(\"ndcg@\")]\n",
    "    if not ks:\n",
    "        return\n",
    "    # group into recall and ndcg\n",
    "    rec = sorted([(int(k.split(\"@\")[1]), metrics[k]) for k in ks if k.startswith(\"recall@\")])\n",
    "    ndc = sorted([(int(k.split(\"@\")[1]), metrics[k]) for k in ks if k.startswith(\"ndcg@\")])\n",
    "    if rec:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar([f\"@{k}\" for k,_ in rec], [v for _,v in rec])\n",
    "        plt.ylim(0,1)\n",
    "        plt.title(f\"{prefix} ADR Recall@k\"); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / f\"plot_{prefix.lower()}_recall_at_k.png\", dpi=150)\n",
    "        plt.close()\n",
    "    if ndc:\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.bar([f\"@{k}\" for k,_ in ndc], [v for _,v in ndc])\n",
    "        plt.ylim(0,1)\n",
    "        plt.title(f\"{prefix} ADR NDCG@k\"); plt.tight_layout()\n",
    "        plt.savefig(OUT_DIR / f\"plot_{prefix.lower()}_ndcg_at_k.png\", dpi=150)\n",
    "        plt.close()\n",
    "\n",
    "if best_val: _maybe_plot_topk(\"VAL\", best_val)\n",
    "if m_test:   _maybe_plot_topk(\"TEST\", m_test)\n",
    "\n",
    "print(\"Saved plots to:\", OUT_DIR)\n",
    "for p in [\"plot_loss_curves.png\",\"plot_val_dti_metrics.png\",\"plot_val_adr_errors.png\",\"plot_pr_curves.png\",\"plot_roc_curves.png\",\"plot_VAL_recall_at_k.png\",\"plot_VAL_ndcg_at_k.png\",\"plot_TEST_recall_at_k.png\",\"plot_TEST_ndcg_at_k.png\"]:\n",
    "    q = OUT_DIR / p\n",
    "    if q.exists():\n",
    "        print(\" -\", q.name)\n",
    "\n",
    "# ---------- Append run_summary.csv ----------\n",
    "summary_row = {\n",
    "    \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"run_name\": cfg[\"run\"][\"name\"],\n",
    "    \"drug_encoder\": cfg[\"model\"][\"drug_encoder\"],\n",
    "    \"protein_encoder\": cfg[\"model\"][\"protein_encoder\"],\n",
    "    \"shared_dim\": cfg[\"model\"][\"shared_dim\"],\n",
    "    \"dti_head\": cfg[\"model\"][\"dti_head\"],\n",
    "    \"val_pr_auc\": best_val.get(\"dti_pr_auc\", np.nan),\n",
    "    \"val_roc_auc\": best_val.get(\"dti_roc_auc\", np.nan),\n",
    "    \"val_f1\": best_val.get(\"dti_f1\", np.nan),\n",
    "    \"val_thr\": best_val.get(\"dti_thr\", np.nan),\n",
    "    \"val_adr_rmse\": best_val.get(\"adr_rmse\", np.nan),\n",
    "    \"val_adr_mae\": best_val.get(\"adr_mae\", np.nan),\n",
    "    \"test_pr_auc\": m_test.get(\"dti_pr_auc\", np.nan),\n",
    "    \"test_roc_auc\": m_test.get(\"dti_roc_auc\", np.nan),\n",
    "    \"test_f1\": m_test.get(\"dti_f1\", np.nan),\n",
    "    \"test_adr_rmse\": m_test.get(\"adr_rmse\", np.nan),\n",
    "    \"test_adr_mae\": m_test.get(\"adr_mae\", np.nan),\n",
    "}\n",
    "\n",
    "summary_csv = OUT_DIR / \"run_summary.csv\"\n",
    "df_row = pd.DataFrame([summary_row])\n",
    "if summary_csv.exists():\n",
    "    df_old = pd.read_csv(summary_csv)\n",
    "    df_out = pd.concat([df_old, df_row], ignore_index=True)\n",
    "else:\n",
    "    df_out = df_row\n",
    "df_out.to_csv(summary_csv, index=False)\n",
    "print(\"Updated:\", summary_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b599c3",
   "metadata": {},
   "source": [
    "Cell 10 — Compare multiple runs (build a cross-run table + small plots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "05f3e862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "run_path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "drug_encoder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "protein_encoder",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dti_head",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "shared_dim",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "val_pr_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_roc_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_thr",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_adr_rmse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "val_adr_mae",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_pr_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_roc_auc",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_f1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_adr_rmse",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "test_adr_mae",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "ae82c418-efe6-4890-9263-e7fca01981a9",
       "rows": [
        [
         "0",
         "F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\\dti_adr_v1",
         "chemberta",
         "esm",
         "cosine",
         "512",
         "0.7879831547735247",
         "0.8662981736395708",
         "0.7527430466955858",
         "0.5948307514190674",
         "0.08005495369434357",
         "0.06617925316095352",
         "0.6807435993004514",
         "0.8307227522732694",
         "0.4421845574387947",
         "0.0357389934360981",
         "0.026058100163936615"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_path</th>\n",
       "      <th>drug_encoder</th>\n",
       "      <th>protein_encoder</th>\n",
       "      <th>dti_head</th>\n",
       "      <th>shared_dim</th>\n",
       "      <th>val_pr_auc</th>\n",
       "      <th>val_roc_auc</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>val_thr</th>\n",
       "      <th>val_adr_rmse</th>\n",
       "      <th>val_adr_mae</th>\n",
       "      <th>test_pr_auc</th>\n",
       "      <th>test_roc_auc</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>test_adr_rmse</th>\n",
       "      <th>test_adr_mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F:\\Thesis Korbi na\\dti-prediction-with-adr\\Mod...</td>\n",
       "      <td>chemberta</td>\n",
       "      <td>esm</td>\n",
       "      <td>cosine</td>\n",
       "      <td>512</td>\n",
       "      <td>0.787983</td>\n",
       "      <td>0.866298</td>\n",
       "      <td>0.752743</td>\n",
       "      <td>0.594831</td>\n",
       "      <td>0.080055</td>\n",
       "      <td>0.066179</td>\n",
       "      <td>0.680744</td>\n",
       "      <td>0.830723</td>\n",
       "      <td>0.442185</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.026058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            run_path drug_encoder  \\\n",
       "0  F:\\Thesis Korbi na\\dti-prediction-with-adr\\Mod...    chemberta   \n",
       "\n",
       "  protein_encoder dti_head  shared_dim  val_pr_auc  val_roc_auc    val_f1  \\\n",
       "0             esm   cosine         512    0.787983     0.866298  0.752743   \n",
       "\n",
       "    val_thr  val_adr_rmse  val_adr_mae  test_pr_auc  test_roc_auc   test_f1  \\\n",
       "0  0.594831      0.080055     0.066179     0.680744      0.830723  0.442185   \n",
       "\n",
       "   test_adr_rmse  test_adr_mae  \n",
       "0       0.035739      0.026058  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved comparison charts to: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\runs\n"
     ]
    }
   ],
   "source": [
    "# === Cell 10: Multi-run aggregator for comparison across encoder combos ===\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RUNS_DIR = (NB_ROOT / \"runs\").resolve()\n",
    "rows = []\n",
    "for run_dir in RUNS_DIR.glob(\"*\"):\n",
    "    if not run_dir.is_dir():\n",
    "        continue\n",
    "    hist = run_dir / \"history.json\"\n",
    "    best = run_dir / \"best_metrics_val.json\"\n",
    "    test = run_dir / \"test_metrics.json\"\n",
    "    cfgp = run_dir.parent / \"config.yaml\"  # often copied in deploy_bundle; we also have OUT_DIR/config.yaml\n",
    "    # Accept missing cfg; pull encoders from checkpoint config if present\n",
    "    meta = {\"run_path\": str(run_dir)}\n",
    "    try:\n",
    "        b = json.loads(best.read_text(encoding=\"utf-8\")) if best.exists() else {}\n",
    "        t = json.loads(test.read_text(encoding=\"utf-8\")) if test.exists() else {}\n",
    "        # Try to read resolved_config.json for encoders\n",
    "        rc = run_dir / \"resolved_config.json\"\n",
    "        enc = {}\n",
    "        if rc.exists():\n",
    "            rcj = json.loads(rc.read_text(encoding=\"utf-8\"))\n",
    "            enc[\"drug_encoder\"] = rcj.get(\"model\", {}).get(\"drug_encoder\")\n",
    "            enc[\"protein_encoder\"] = rcj.get(\"model\", {}).get(\"protein_encoder\")\n",
    "            enc[\"dti_head\"] = rcj.get(\"model\", {}).get(\"dti_head\")\n",
    "            enc[\"shared_dim\"] = rcj.get(\"model\", {}).get(\"shared_dim\")\n",
    "        meta.update(enc)\n",
    "        meta.update({\n",
    "            \"val_pr_auc\": b.get(\"dti_pr_auc\", float(\"nan\")),\n",
    "            \"val_roc_auc\": b.get(\"dti_roc_auc\", float(\"nan\")),\n",
    "            \"val_f1\": b.get(\"dti_f1\", float(\"nan\")),\n",
    "            \"val_thr\": b.get(\"dti_thr\", float(\"nan\")),\n",
    "            \"val_adr_rmse\": b.get(\"adr_rmse\", float(\"nan\")),\n",
    "            \"val_adr_mae\": b.get(\"adr_mae\", float(\"nan\")),\n",
    "            \"test_pr_auc\": t.get(\"dti_pr_auc\", float(\"nan\")),\n",
    "            \"test_roc_auc\": t.get(\"dti_roc_auc\", float(\"nan\")),\n",
    "            \"test_f1\": t.get(\"dti_f1\", float(\"nan\")),\n",
    "            \"test_adr_rmse\": t.get(\"adr_rmse\", float(\"nan\")),\n",
    "            \"test_adr_mae\": t.get(\"adr_mae\", float(\"nan\")),\n",
    "        })\n",
    "        rows.append(meta)\n",
    "    except Exception:\n",
    "        continue\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"No completed runs found in ./runs/*\")\n",
    "\n",
    "df_runs = pd.DataFrame(rows)\n",
    "display(df_runs.sort_values([\"val_pr_auc\",\"test_pr_auc\"], ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Simple comparison plots (val PR-AUC & test PR-AUC by encoder combo)\n",
    "def _label_combo(r):\n",
    "    return f\"{r.get('drug_encoder','?')}/{r.get('protein_encoder','?')}:{r.get('dti_head','?')}\"\n",
    "\n",
    "df_runs[\"combo\"] = df_runs.apply(_label_combo, axis=1)\n",
    "\n",
    "plt.figure(figsize=(8,4.5))\n",
    "plt.bar(df_runs[\"combo\"], df_runs[\"val_pr_auc\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"PR-AUC (val)\"); plt.title(\"Validation PR-AUC by Encoder Combo\")\n",
    "plt.tight_layout(); plt.savefig(RUNS_DIR / \"compare_val_pr_auc.png\", dpi=150); plt.close()\n",
    "\n",
    "plt.figure(figsize=(8,4.5))\n",
    "plt.bar(df_runs[\"combo\"], df_runs[\"test_pr_auc\"])\n",
    "plt.xticks(rotation=30, ha=\"right\")\n",
    "plt.ylabel(\"PR-AUC (test)\"); plt.title(\"Test PR-AUC by Encoder Combo\")\n",
    "plt.tight_layout(); plt.savefig(RUNS_DIR / \"compare_test_pr_auc.png\", dpi=150); plt.close()\n",
    "\n",
    "print(\"Saved comparison charts to:\", RUNS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
