{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f5ea8fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Modular RxNorm ↔ PubChem/ChEMBL/InChIKey pipeline.\n",
    "\n",
    "Run stages independently:\n",
    "  A) Historystatus & lineage   → debug_historystatus.tsv, lineage.parquet\n",
    "  B) Resolution (current RXCUI → CID/InChIKey) → rxnorm_resolved.parquet\n",
    "  C) Join (single full parquet) → mapping_combined.parquet\n",
    "\n",
    "Toggle STAGE_* flags and options in the CONTROL BLOCK at the end.\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Tuple, Set, Iterable\n",
    "from pathlib import Path\n",
    "import time\n",
    "import threading\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5d01edfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= CONFIG (paths common to all runs) =========================\n",
    "DATASET_A_DIR = Path(r\"../Data/Helper_Datasets/merged_id.parquet\")  # single parquet file or a dir (defensive fallback)\n",
    "A_GLOB = \"*.parquet\"\n",
    "\n",
    "ONSIDES_CSV_DIR = Path(r\"../Data/Main_Datasets/onsides-v3.1.0\")\n",
    "ONSIDES_INGREDIENT_CSV = ONSIDES_CSV_DIR / \"vocab_rxnorm_ingredient.csv\"              # has rxnorm_id or rxcui\n",
    "ONSIDES_PRODUCT_TO_RXNORM_CSV = ONSIDES_CSV_DIR / \"product_to_rxnorm.csv\"             # has rxnorm_product_id\n",
    "# (Optional helper) if you ever want to pivot product↔ingredient; not required for product lineage itself:\n",
    "ONSIDES_RXNORM_ING_TO_PRODUCT_CSV = ONSIDES_CSV_DIR / \"vocab_rxnorm_ingredient_to_product.csv\"\n",
    "\n",
    "OUT_DIR = Path(\"mapping_out/\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canonical cache/artifacts (reused across runs)\n",
    "HISTORY_TSV_PATH = OUT_DIR / \"debug_historystatus.tsv\"     # full historystatus (audit) with source_type\n",
    "LINEAGE_PATH     = OUT_DIR / \"lineage.parquet\"             # columns: original_rxcui, rxcui, source_type\n",
    "CURRENT_SET_JSON = OUT_DIR / \"current_rxcui_set.json\"      # {\"ingredient\":[...], \"product\":[...], \"all\":[...]}\n",
    "RESOLVED_PATH    = OUT_DIR / \"rxnorm_resolved.parquet\"     # current rxcui → pubchem_cid/inchikey cache (shared)\n",
    "COMBINED_OUT     = OUT_DIR / \"mapping_combined.parquet\"\n",
    "RES_DEBUG_TSV    = OUT_DIR / \"debug_resolution.tsv\"\n",
    "\n",
    "# Column names in your A shards / full file\n",
    "COL_CHEMBL   = \"chembl_id\"\n",
    "COL_PUBCHEM  = \"pubchem_id\"   # normalized to pubchem_cid\n",
    "COL_INCHIKEY = \"inchikey\"\n",
    "\n",
    "# Rate limiting\n",
    "REQUESTS_PER_SECOND = 4.0\n",
    "HTTP_TIMEOUT_SEC = 20\n",
    "\n",
    "# Threading\n",
    "NUM_WORKERS_RESOLVE = 8\n",
    "\n",
    "# ---- Endpoints ----\n",
    "RXNAV   = \"https://rxnav.nlm.nih.gov/REST\"\n",
    "PUBCHEM = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\"\n",
    "HEADERS = {\"User-Agent\": \"rxnorm-pubchem-mapper-batch/6.0\", \"Accept\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a17561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Utilities =========================\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate_per_sec: float, capacity: Optional[float] = None):\n",
    "        self.rate = float(rate_per_sec)\n",
    "        self.capacity = capacity if capacity is not None else max(1.0, self.rate)\n",
    "        self.tokens = self.capacity\n",
    "        self.timestamp = time.monotonic()\n",
    "        self.lock = threading.Lock()\n",
    "    def consume(self, tokens: float = 1.0):\n",
    "        while True:\n",
    "            with self.lock:\n",
    "                now = time.monotonic()\n",
    "                elapsed = now - self.timestamp\n",
    "                self.tokens = min(self.capacity, self.tokens + elapsed * self.rate)\n",
    "                self.timestamp = now\n",
    "                if self.tokens >= tokens:\n",
    "                    self.tokens -= tokens\n",
    "                    return\n",
    "            time.sleep(max(0.001, tokens / self.rate / 2))\n",
    "\n",
    "bucket = TokenBucket(REQUESTS_PER_SECOND)\n",
    "session = requests.Session()\n",
    "\n",
    "def safe_get(url: str, params: Dict[str, Any] = None, max_retries: int = 3) -> Optional[Dict[str, Any]]:\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            bucket.consume(1.0)\n",
    "            r = session.get(url, params=params, headers=HEADERS, timeout=HTTP_TIMEOUT_SEC)\n",
    "            if r.status_code == 200:\n",
    "                return r.json()\n",
    "            if r.status_code in (400, 404):\n",
    "                return None\n",
    "        except requests.RequestException:\n",
    "            pass\n",
    "        time.sleep(min(10.0, (2 ** attempt)))\n",
    "    return None\n",
    "\n",
    "# Sanitizers\n",
    "_DB_RE = re.compile(r\"(DB\\d{5})\", re.IGNORECASE)\n",
    "_UNII_RE = re.compile(r\"[A-Z0-9]{4,12}\")\n",
    "def clean_drugbank_id(raw: str) -> Optional[str]:\n",
    "    if not raw: return None\n",
    "    s = raw.strip().upper().replace(\"DRUGBANK:\", \" \").replace(\"DBANK:\", \" \").replace(\"DB:\", \" \")\n",
    "    m = _DB_RE.search(s)\n",
    "    return m.group(1).upper() if m else None\n",
    "def clean_unii(raw: str) -> Optional[str]:\n",
    "    if not raw: return None\n",
    "    s = raw.strip().upper().replace(\"UNII:\", \" \").replace(\"UNII \", \" \")\n",
    "    m = _UNII_RE.search(s)\n",
    "    return m.group(0) if m else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6858356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= RxNav helpers =========================\n",
    "def rxnav_get_historystatus(rxcui: str) -> Dict[str, Any]:\n",
    "    url = f\"{RXNAV}/rxcui/{rxcui}/historystatus.json\"\n",
    "    return safe_get(url) or {}\n",
    "\n",
    "def _parse_historystatus_payload(data: dict) -> tuple[str, list[str]]:\n",
    "    if not data: return (\"Unknown\", [])\n",
    "    hs = data.get(\"historystatus\")\n",
    "    if isinstance(hs, dict):\n",
    "        status = hs.get(\"conceptStatus\", \"Unknown\")\n",
    "        remapped = hs.get(\"remappedConcept\", [])\n",
    "        if isinstance(remapped, dict): remapped = [remapped]\n",
    "        targets = []\n",
    "        for x in remapped:\n",
    "            rcui = x.get(\"rxcui\") or x.get(\"remappedRxCui\")\n",
    "            if rcui: targets.append(str(rcui))\n",
    "        return (status, sorted(set(targets)))\n",
    "    h2 = data.get(\"rxcuiStatusHistory\")\n",
    "    if isinstance(h2, dict):\n",
    "        status = (h2.get(\"metaData\") or {}).get(\"status\", \"Unknown\")\n",
    "        remapped = (h2.get(\"derivedConcepts\") or {}).get(\"remappedConcept\", []) or []\n",
    "        if isinstance(remapped, dict): remapped = [remapped]\n",
    "        targets = []\n",
    "        for x in remapped:\n",
    "            rcui = x.get(\"remappedRxCui\") or x.get(\"rxcui\")\n",
    "            if rcui: targets.append(str(rcui))\n",
    "        return (status, sorted(set(targets)))\n",
    "    return (\"Unknown\", [])\n",
    "\n",
    "def normalize_rxcui(rxcui: str) -> list[str]:\n",
    "    payload = rxnav_get_historystatus(str(rxcui))\n",
    "    status, targets = _parse_historystatus_payload(payload)\n",
    "    if status and status.upper() == \"ACTIVE\":   return [str(rxcui)]\n",
    "    if status and status.upper() == \"REMAPPED\": return targets\n",
    "    return []\n",
    "\n",
    "def rxnav_properties_min(rxcui: str) -> Tuple[Optional[str], Optional[str]]:\n",
    "    url = f\"{RXNAV}/rxcui/{rxcui}/properties.json\"\n",
    "    data = safe_get(url) or {}\n",
    "    props = data.get(\"properties\", {})\n",
    "    return (props.get(\"name\"), props.get(\"tty\"))\n",
    "\n",
    "def rxnav_all_properties(rxcui: str) -> Dict[str, Any]:\n",
    "    url = f\"{RXNAV}/rxcui/{rxcui}/allProperties.json\"\n",
    "    return safe_get(url, params={\"prop\": \"names codes\"}) or {}\n",
    "\n",
    "def extract_codes_from_all_props(all_props: Dict[str, Any]) -> Dict[str, List[str]]:\n",
    "    out: Dict[str, List[str]] = {}\n",
    "    group = all_props.get(\"propConceptGroup\", {})\n",
    "    pcs = group.get(\"propConcept\")\n",
    "    if not pcs: return out\n",
    "    if isinstance(pcs, dict): pcs = [pcs]\n",
    "    for pc in pcs:\n",
    "        cat = (pc.get(\"propCategory\") or \"\").upper()\n",
    "        name = (pc.get(\"propName\") or \"\").upper()\n",
    "        val = pc.get(\"propValue\")\n",
    "        if not val: continue\n",
    "        key = None\n",
    "        if cat == \"CODES\":\n",
    "            if name == \"DRUGBANK\":   key = \"drugbank_id\"\n",
    "            elif name == \"UNII_CODE\": key = \"unii\"\n",
    "        elif cat == \"NAMES\":\n",
    "            if name in (\"RXNORM NAME\", \"RXNAV_STR\"): key = \"rxnorm_name\"\n",
    "        if key:\n",
    "            out.setdefault(key, []).append(val)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e19e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= PubChem helpers =========================\n",
    "def pubchem_sids_from_drugbank(dbid: str) -> List[int]:\n",
    "    url = f\"{PUBCHEM}/substance/sourceid/DrugBank/{dbid}/sids/JSON\"\n",
    "    data = safe_get(url) or {}\n",
    "    return data.get(\"IdentifierList\", {}).get(\"SID\", []) or []\n",
    "def pubchem_sids_from_unii(unii: str) -> List[int]:\n",
    "    url = f\"{PUBCHEM}/substance/xref/RegistryID/{unii}/sids/JSON\"\n",
    "    data = safe_get(url) or {}\n",
    "    return data.get(\"IdentifierList\", {}).get(\"SID\", []) or []\n",
    "def pubchem_cids_from_sid(sid: int) -> List[int]:\n",
    "    url = f\"{PUBCHEM}/substance/sid/{sid}/cids/JSON\"\n",
    "    data = safe_get(url) or {}\n",
    "    info = data.get(\"InformationList\", {}).get(\"Information\", [])\n",
    "    if not info: return []\n",
    "    return info[0].get(\"CID\", []) or []\n",
    "def pubchem_cids_from_name_exact(name: str) -> List[int]:\n",
    "    url = f\"{PUBCHEM}/compound/name/{requests.utils.quote(name)}/cids/JSON\"\n",
    "    data = safe_get(url, params={\"name_type\": \"exact\"}) or {}\n",
    "    return data.get(\"IdentifierList\", {}).get(\"CID\", []) or []\n",
    "def pubchem_inchikey_from_cid(cid: int) -> Optional[str]:\n",
    "    url = f\"{PUBCHEM}/compound/cid/{cid}/property/InChIKey/JSON\"\n",
    "    data = safe_get(url) or {}\n",
    "    props = data.get(\"PropertyTable\", {}).get(\"Properties\", [])\n",
    "    if not props: return None\n",
    "    return props[0].get(\"InChIKey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b79b7809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Stage A: Historystatus + Lineage (ING + PROD) =========================\n",
    "def load_onsides_ingredient_rxcui(ingredient_csv: Path) -> pd.DataFrame:\n",
    "    if not ingredient_csv.exists():\n",
    "        raise FileNotFoundError(f\"{ingredient_csv} not found\")\n",
    "    df = pd.read_csv(ingredient_csv)\n",
    "    rx_col = \"rxnorm_id\" if \"rxnorm_id\" in df.columns else \"rxcui\" if \"rxcui\" in df.columns else None\n",
    "    if rx_col is None:\n",
    "        raise ValueError(\"vocab_rxnorm_ingredient.csv must contain 'rxnorm_id' or 'rxcui'\")\n",
    "    out = df[[rx_col]].rename(columns={rx_col: \"original_rxcui\"}).dropna().drop_duplicates()\n",
    "    out[\"original_rxcui\"] = out[\"original_rxcui\"].astype(str)\n",
    "    out[\"source_type\"] = \"ingredient\"\n",
    "    return out\n",
    "\n",
    "def load_onsides_product_rxcui(product_to_rxnorm_csv: Path) -> pd.DataFrame:\n",
    "    if not product_to_rxnorm_csv.exists():\n",
    "        raise FileNotFoundError(f\"{product_to_rxnorm_csv} not found\")\n",
    "    dfp = pd.read_csv(product_to_rxnorm_csv)\n",
    "    if \"rxnorm_product_id\" not in dfp.columns:\n",
    "        raise ValueError(\"product_to_rxnorm.csv must contain 'rxnorm_product_id'\")\n",
    "    out = dfp[[\"rxnorm_product_id\"]].rename(columns={\"rxnorm_product_id\": \"original_rxcui\"}).dropna().drop_duplicates()\n",
    "    out[\"original_rxcui\"] = out[\"original_rxcui\"].astype(str)\n",
    "    out[\"source_type\"] = \"product\"\n",
    "    return out\n",
    "\n",
    "def stage_A_build_historystatus_and_lineage(\n",
    "    include_types: Iterable[str] = (\"ingredient\",\"product\"),\n",
    "    overwrite_tsv: bool = False,\n",
    "    overwrite_lineage: bool = False\n",
    ") -> Tuple[pd.DataFrame, Dict[str, List[str]]]:\n",
    "    print(\"[A] Loading RxCUIs from OnSIDES ...\")\n",
    "    parts = []\n",
    "    if \"ingredient\" in include_types:\n",
    "        parts.append(load_onsides_ingredient_rxcui(ONSIDES_INGREDIENT_CSV))\n",
    "    if \"product\" in include_types:\n",
    "        parts.append(load_onsides_product_rxcui(ONSIDES_PRODUCT_TO_RXNORM_CSV))\n",
    "    if not parts:\n",
    "        raise ValueError(\"include_types must contain 'ingredient' and/or 'product'\")\n",
    "    df_orig = pd.concat(parts, ignore_index=True).drop_duplicates()\n",
    "    print(f\"    found {len(df_orig)} original RxCUIs (ingredients: {sum(df_orig['source_type']=='ingredient')}, products: {sum(df_orig['source_type']=='product')})\")\n",
    "\n",
    "    # Historystatus + lineage (one pass) with source_type\n",
    "    print(\"[A] Normalizing via historystatus (one pass) ...\")\n",
    "    lineage_rows = []\n",
    "    if overwrite_tsv or not HISTORY_TSV_PATH.exists():\n",
    "        with HISTORY_TSV_PATH.open(\"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            w = csv.DictWriter(f, fieldnames=[\"original_rxcui\", \"status\", \"targets\", \"source_type\"], delimiter=\"\\t\")\n",
    "            w.writeheader()\n",
    "            for i, row in enumerate(tqdm(df_orig.itertuples(index=False), total=len(df_orig), desc=\"Historystatus (RxCUI remaps)\", unit=\"rxcui\")):\n",
    "                rx = row.original_rxcui\n",
    "                st = row.source_type\n",
    "                targets = normalize_rxcui(rx)\n",
    "                status = \"Active\" if targets == [rx] else (\"Remapped\" if targets else \"Obsolete/None\")\n",
    "                w.writerow({\"original_rxcui\": rx, \"status\": status, \"targets\": \";\".join(targets), \"source_type\": st})\n",
    "                if targets:\n",
    "                    for t in targets:\n",
    "                        lineage_rows.append({\"original_rxcui\": rx, \"rxcui\": t, \"source_type\": st})\n",
    "                else:\n",
    "                    lineage_rows.append({\"original_rxcui\": rx, \"rxcui\": None, \"source_type\": st})\n",
    "                if (i + 1) % 500 == 0: f.flush()\n",
    "    else:\n",
    "        print(\"    using existing debug_historystatus.tsv (set overwrite_tsv=True to re-create)\")\n",
    "        # still rebuild lineage for downstream using live API (keeps fresh)\n",
    "        for row in tqdm(df_orig.itertuples(index=False), total=len(df_orig), desc=\"Historystatus (RxCUI remaps)\", unit=\"rxcui\"):\n",
    "            rx = row.original_rxcui\n",
    "            st = row.source_type\n",
    "            targets = normalize_rxcui(rx)\n",
    "            if targets:\n",
    "                for t in targets:\n",
    "                    lineage_rows.append({\"original_rxcui\": rx, \"rxcui\": t, \"source_type\": st})\n",
    "            else:\n",
    "                lineage_rows.append({\"original_rxcui\": rx, \"rxcui\": None, \"source_type\": st})\n",
    "\n",
    "    df_lineage = pd.DataFrame(lineage_rows).drop_duplicates()\n",
    "    if overwrite_lineage or not LINEAGE_PATH.exists():\n",
    "        df_lineage.to_parquet(LINEAGE_PATH, index=False)\n",
    "    else:\n",
    "        try:\n",
    "            old = pd.read_parquet(LINEAGE_PATH)\n",
    "            df_lineage = pd.concat([old, df_lineage], ignore_index=True).drop_duplicates()\n",
    "            df_lineage.to_parquet(LINEAGE_PATH, index=False)\n",
    "        except Exception:\n",
    "            df_lineage.to_parquet(LINEAGE_PATH, index=False)\n",
    "\n",
    "    # Build current sets per type + combined\n",
    "    current_ing = sorted(df_lineage.query(\"source_type=='ingredient' and rxcui.notna()\")[\"rxcui\"].astype(str).unique().tolist())\n",
    "    current_prod = sorted(df_lineage.query(\"source_type=='product' and rxcui.notna()\")[\"rxcui\"].astype(str).unique().tolist())\n",
    "    current_all  = sorted(set(current_ing) | set(current_prod))\n",
    "    current_sets = {\"ingredient\": current_ing, \"product\": current_prod, \"all\": current_all}\n",
    "\n",
    "    with CURRENT_SET_JSON.open(\"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(current_sets, f)\n",
    "\n",
    "    n_orphans = (df_lineage[\"rxcui\"].isna()).sum()\n",
    "    print(f\"[A] current RxCUIs → ingredient:{len(current_ing)} | product:{len(current_prod)} | all:{len(current_all)}  (orphans: {n_orphans})\")\n",
    "    return df_lineage, current_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92413177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Stage B: Resolve current RXCUIs (shared cache) =========================\n",
    "def resolve_single_rxcui_current(rxcui: str, debug_rows: list) -> List[Dict[str, Any]]:\n",
    "    rows: List[Dict[str, Any]] = []\n",
    "    rxn_name, tty = rxnav_properties_min(rxcui)\n",
    "    all_props = rxnav_all_properties(rxcui)\n",
    "    codes = extract_codes_from_all_props(all_props)\n",
    "    db_ids = [clean_drugbank_id(x) for x in codes.get(\"drugbank_id\", []) if clean_drugbank_id(x)]\n",
    "    unii_ids = [clean_unii(x) for x in codes.get(\"unii\", []) if clean_unii(x)]\n",
    "\n",
    "    resolved_cids: Set[int] = set()\n",
    "    prov: List[str] = []\n",
    "    dbg = {\"rxcui\": rxcui, \"rxnorm_name\": rxn_name, \"tty\": tty, \"db_ids\": \";\".join(db_ids), \"unii_ids\": \";\".join(unii_ids), \"step\": \"\", \"result_cids\": \"\"}\n",
    "\n",
    "    # DrugBank → Substance → SID → CID\n",
    "    for dbid in db_ids:\n",
    "        sids = pubchem_sids_from_drugbank(dbid)\n",
    "        for sid in sids:\n",
    "            for cid in pubchem_cids_from_sid(sid):\n",
    "                resolved_cids.add(int(cid))\n",
    "        if sids: prov.append(f\"DrugBank:{dbid}\")\n",
    "    if resolved_cids:\n",
    "        dbg[\"step\"] = \"DrugBank\"; dbg[\"result_cids\"] = \",\".join(map(str, sorted(resolved_cids)))\n",
    "\n",
    "    # UNII → Substance → SID → CID\n",
    "    if not resolved_cids:\n",
    "        for unii in unii_ids:\n",
    "            sids = pubchem_sids_from_unii(unii)\n",
    "            for sid in sids:\n",
    "                for cid in pubchem_cids_from_sid(sid):\n",
    "                    resolved_cids.add(int(cid))\n",
    "            if sids: prov.append(f\"UNII:{unii}\")\n",
    "        if resolved_cids:\n",
    "            dbg[\"step\"] = \"UNII\"; dbg[\"result_cids\"] = \",\".join(map(str, sorted(resolved_cids)))\n",
    "\n",
    "    # RxNorm preferred name exact (heuristic but conservative)\n",
    "    heuristic = False\n",
    "    if not resolved_cids and rxn_name:\n",
    "        for cid in pubchem_cids_from_name_exact(rxn_name):\n",
    "            resolved_cids.add(int(cid))\n",
    "        if resolved_cids:\n",
    "            heuristic = True\n",
    "            prov.append(f\"RxNormName:{rxn_name}\")\n",
    "            dbg[\"step\"] = \"NAME\"; dbg[\"result_cids\"] = \",\".join(map(str, sorted(resolved_cids)))\n",
    "\n",
    "    # Emit rows\n",
    "    if resolved_cids:\n",
    "        for cid in sorted(resolved_cids):\n",
    "            ik = pubchem_inchikey_from_cid(cid)\n",
    "            rows.append({\"rxcui\": rxcui, \"rxnorm_name\": rxn_name, \"tty\": tty, \"pubchem_cid\": cid,\n",
    "                         \"inchikey\": (ik or None), \"drugbank_id\": \";\".join(db_ids) or None, \"unii\": \";\".join(unii_ids) or None,\n",
    "                         \"confidence\": \"XREF\" if not heuristic else \"HEURISTIC_NAME\", \"provenance\": \",\".join(prov) or None})\n",
    "    else:\n",
    "        rows.append({\"rxcui\": rxcui, \"rxnorm_name\": rxn_name, \"tty\": tty, \"pubchem_cid\": None, \"inchikey\": None,\n",
    "                     \"drugbank_id\": \";\".join(db_ids) or None, \"unii\": \";\".join(unii_ids) or None,\n",
    "                     \"confidence\": \"UNRESOLVED\", \"provenance\": None})\n",
    "    debug_rows.append(dbg)\n",
    "    return rows\n",
    "\n",
    "def stage_B_resolve_current(current_set: List[str], append_cache: bool = True) -> pd.DataFrame:\n",
    "    print(\"[B] Resolving current RxCUIs → CID/InChIKey ...\")\n",
    "\n",
    "    cached = pd.DataFrame()\n",
    "    if RESOLVED_PATH.exists():\n",
    "        cached = pd.read_parquet(RESOLVED_PATH)\n",
    "        cached[\"rxcui\"] = cached[\"rxcui\"].astype(str)\n",
    "\n",
    "    to_do = set(current_set)\n",
    "    if not cached.empty:\n",
    "        done = set(cached[\"rxcui\"].unique())\n",
    "        to_do = sorted(list(to_do - done))\n",
    "    else:\n",
    "        to_do = sorted(list(to_do))\n",
    "\n",
    "    print(f\"    total current RXCUIs: {len(current_set)} | cached: {0 if cached.empty else len(cached['rxcui'].unique())} | to resolve: {len(to_do)}\")\n",
    "\n",
    "    out_rows: List[Dict[str, Any]] = []\n",
    "    debug_rows: List[Dict[str, Any]] = []\n",
    "    if to_do:\n",
    "        with ThreadPoolExecutor(max_workers=NUM_WORKERS_RESOLVE) as ex:\n",
    "            futures = {ex.submit(resolve_single_rxcui_current, r, debug_rows): r for r in to_do}\n",
    "            for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Resolving (parallel)\"):\n",
    "                out_rows.extend(fut.result())\n",
    "\n",
    "    dbg_df = pd.DataFrame(debug_rows)\n",
    "    if not dbg_df.empty:\n",
    "        dbg_df.to_csv(RES_DEBUG_TSV, sep=\"\\t\", index=False)\n",
    "\n",
    "    new_df = pd.DataFrame(out_rows)\n",
    "    if not new_df.empty:\n",
    "        if \"inchikey\" in new_df.columns:\n",
    "            new_df[\"inchikey\"] = new_df[\"inchikey\"].astype(str).str.upper()\n",
    "        final = pd.concat([cached, new_df], ignore_index=True).drop_duplicates() if append_cache else new_df\n",
    "    else:\n",
    "        final = cached\n",
    "\n",
    "    final.to_parquet(RESOLVED_PATH, index=False)\n",
    "    print(f\"[B] cache updated at {RESOLVED_PATH} | total rows: {len(final)}\")\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f9577dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= Stage C: Join single file =========================\n",
    "def normalize_a_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    rename = {}\n",
    "    if COL_PUBCHEM in df.columns and \"pubchem_cid\" not in df.columns:\n",
    "        rename[COL_PUBCHEM] = \"pubchem_cid\"\n",
    "    if COL_INCHIKEY in df.columns and \"inchikey\" not in df.columns:\n",
    "        rename[COL_INCHIKEY] = \"inchikey\"\n",
    "    if rename:\n",
    "        df = df.rename(columns=rename)\n",
    "\n",
    "    if \"pubchem_cid\" in df.columns:\n",
    "        df[\"pubchem_cid\"] = pd.to_numeric(df[\"pubchem_cid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "    if \"inchikey\" in df.columns:\n",
    "        df[\"inchikey\"] = df[\"inchikey\"].astype(str).str.upper()\n",
    "    return df\n",
    "\n",
    "def join_full_with_rx(df_a: pd.DataFrame, df_rx: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Join A (your whole parquet) with resolved RxNorm mapping in two passes:\n",
    "      1) exact on InChIKey\n",
    "      2) then on remaining rows by PubChem CID\n",
    "    \"\"\"\n",
    "    if df_a.empty:\n",
    "        return pd.DataFrame(columns=[\n",
    "            \"original_rxcui\",\"rxcui\",\"source_type\",\"pubchem_cid\",\"inchikey\",\n",
    "            COL_CHEMBL,\"rxnorm_name\",\"tty\",\"drugbank_id\",\"unii\",\"confidence\",\"provenance\"\n",
    "        ])\n",
    "\n",
    "    # Pass 1: inchikey\n",
    "    out_ik = (\n",
    "        df_a.merge(df_rx, on=\"inchikey\", how=\"inner\", suffixes=(\"_a\", \"_rx\"))\n",
    "        if (\"inchikey\" in df_a.columns and \"inchikey\" in df_rx.columns)\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "\n",
    "    # Pass 2: remaining by pubchem_cid\n",
    "    out = out_ik\n",
    "    if \"pubchem_cid\" in df_a.columns and \"pubchem_cid\" in df_rx.columns:\n",
    "        matched_idx = out_ik.index if not out_ik.empty else []\n",
    "        remaining = df_a[~df_a.index.isin(matched_idx)] if len(df_a) else df_a\n",
    "\n",
    "        left = remaining.copy()\n",
    "        right = df_rx.copy()\n",
    "        left[\"pubchem_cid\"]  = pd.to_numeric(left[\"pubchem_cid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "        right[\"pubchem_cid\"] = pd.to_numeric(right[\"pubchem_cid\"], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        out_cid = left.merge(right, on=\"pubchem_cid\", how=\"inner\", suffixes=(\"_a\", \"_rx\"))\n",
    "        out = pd.concat([out_ik, out_cid], ignore_index=True) if not out_ik.empty else out_cid\n",
    "\n",
    "    # Keep lineage/type and important columns\n",
    "    keep = [\n",
    "        \"original_rxcui\",\"rxcui\",\"source_type\",\"pubchem_cid\",\"inchikey\",COL_CHEMBL,\n",
    "        \"rxnorm_name\",\"tty\",\"drugbank_id\",\"unii\",\"confidence\",\"provenance\"\n",
    "    ]\n",
    "    for c in keep:\n",
    "        if c not in out.columns and f\"{c}_rx\" in out.columns:\n",
    "            out[c] = out[f\"{c}_rx\"]\n",
    "        elif c not in out.columns and f\"{c}_a\" in out.columns:\n",
    "            out[c] = out[f\"{c}_a\"]\n",
    "        elif c not in out.columns:\n",
    "            out[c] = None\n",
    "\n",
    "    out = out[keep].drop_duplicates()\n",
    "    return out\n",
    "\n",
    "def stage_C_join_single(join_types: Iterable[str] = (\"ingredient\",\"product\")) -> Path:\n",
    "    print(\"[C] Joining full dataset (single parquet) ...\")\n",
    "    # Preconditions\n",
    "    if not LINEAGE_PATH.exists():\n",
    "        raise FileNotFoundError(\"lineage.parquet not found. Run Stage A first.\")\n",
    "    if not RESOLVED_PATH.exists():\n",
    "        raise FileNotFoundError(\"rxnorm_resolved.parquet not found. Run Stage B first.\")\n",
    "\n",
    "    # Load lineage filtered by type and merge with resolved cache\n",
    "    df_lineage_all = pd.read_parquet(LINEAGE_PATH)\n",
    "    df_lineage = df_lineage_all[df_lineage_all[\"source_type\"].isin(set(join_types))].copy()\n",
    "\n",
    "    df_rx_base = pd.read_parquet(RESOLVED_PATH)\n",
    "    df_rx = df_lineage.merge(df_rx_base, on=\"rxcui\", how=\"left\")\n",
    "\n",
    "    # Read the whole A file at once (defensive if a directory is passed)\n",
    "    a_path = Path(DATASET_A_DIR)\n",
    "    if not a_path.exists():\n",
    "        raise FileNotFoundError(f\"{a_path} not found\")\n",
    "    if a_path.is_dir():\n",
    "        files = sorted(a_path.glob(A_GLOB))\n",
    "        if not files:\n",
    "            raise FileNotFoundError(f\"No parquet files found under {a_path} matching {A_GLOB}\")\n",
    "        df_a = pd.concat([pd.read_parquet(p) for p in files], ignore_index=True)\n",
    "    else:\n",
    "        df_a = pd.read_parquet(a_path)\n",
    "\n",
    "    df_a = normalize_a_cols(df_a)\n",
    "\n",
    "    # If neither inchikey nor pubchem_cid exists, emit empty combined mapping\n",
    "    if \"inchikey\" not in df_a.columns and \"pubchem_cid\" not in df_a.columns:\n",
    "        print(\"    No 'inchikey' or 'pubchem_cid' columns in A; writing empty combined mapping.\")\n",
    "        pd.DataFrame(columns=[\n",
    "            \"original_rxcui\",\"rxcui\",\"source_type\",\"pubchem_cid\",\"inchikey\",COL_CHEMBL,\n",
    "            \"rxnorm_name\",\"tty\",\"drugbank_id\",\"unii\",\"confidence\",\"provenance\"\n",
    "        ]).to_parquet(COMBINED_OUT, index=False)\n",
    "        print(f\"[C] combined mapping written to {COMBINED_OUT}\")\n",
    "        return COMBINED_OUT\n",
    "\n",
    "    # Join and write\n",
    "    out = join_full_with_rx(df_a, df_rx)\n",
    "    out.to_parquet(COMBINED_OUT, index=False)\n",
    "    print(f\"[C] combined mapping written to {COMBINED_OUT} | rows: {len(out)}\")\n",
    "    return COMBINED_OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfda8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========================= CONTROL BLOCK =========================\n",
    "if __name__ == \"__main__\":\n",
    "    # Toggle stages independently\n",
    "    STAGE_A = True   # build historystatus + lineage\n",
    "    STAGE_B = True   # resolve current RXCUIs (shared cache)\n",
    "    STAGE_C = True   # join full dataset (single parquet)\n",
    "\n",
    "    # Stage A options\n",
    "    INCLUDE_TYPES = (\"ingredient\",\"product\")  # choose any of: \"ingredient\", \"product\"\n",
    "    OVERWRITE_HISTORY_TSV = False\n",
    "    OVERWRITE_LINEAGE     = False\n",
    "\n",
    "    # Stage B options\n",
    "    # (no change—uses CURRENT_SET_JSON[\"all\"] to resolve once for both types)\n",
    "\n",
    "    # Stage C (join) selection — single file mode\n",
    "    JOIN_TYPES   = (\"ingredient\",\"product\")   # which lineage types to join into the mapping\n",
    "\n",
    "    # ===== Run =====\n",
    "    lineage_df, current_sets = (None, None)\n",
    "    if STAGE_A:\n",
    "        lineage_df, current_sets = stage_A_build_historystatus_and_lineage(\n",
    "            include_types=INCLUDE_TYPES,\n",
    "            overwrite_tsv=OVERWRITE_HISTORY_TSV,\n",
    "            overwrite_lineage=OVERWRITE_LINEAGE\n",
    "        )\n",
    "    else:\n",
    "        if not LINEAGE_PATH.exists() or not CURRENT_SET_JSON.exists():\n",
    "            raise FileNotFoundError(\"Artifacts missing. Run STAGE_A once to create lineage & current set.\")\n",
    "        lineage_df = pd.read_parquet(LINEAGE_PATH)\n",
    "        with CURRENT_SET_JSON.open(\"r\", encoding=\"utf-8\") as f:\n",
    "            current_sets = json.load(f)\n",
    "\n",
    "    if STAGE_B:\n",
    "        # Resolve across all current RXCUIs from both types (deduped) so cache is shared\n",
    "        stage_B_resolve_current(current_sets[\"all\"], append_cache=True)\n",
    "\n",
    "    if STAGE_C:\n",
    "        stage_C_join_single(join_types=JOIN_TYPES)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
