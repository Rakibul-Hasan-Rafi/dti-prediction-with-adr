{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d8d612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (34741, 7)\n",
      "Columns: ['drug_chembl_id', 'target_uniprot_id', 'label', 'smiles', 'sequence', 'molfile_3d', 'rxcui']\n",
      "\n",
      "First few rows:\n",
      "  drug_chembl_id target_uniprot_id  label  \\\n",
      "0     CHEMBL1000            O15245      0   \n",
      "1     CHEMBL1000            P08183      1   \n",
      "2     CHEMBL1000            P35367      1   \n",
      "3     CHEMBL1000            Q02763      0   \n",
      "4     CHEMBL1000            Q12809      0   \n",
      "\n",
      "                                        smiles  \\\n",
      "0  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "1  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "2  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "3  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "4  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "\n",
      "                                            sequence  \\\n",
      "0  MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...   \n",
      "1  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...   \n",
      "2  MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...   \n",
      "3  MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...   \n",
      "4  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...   \n",
      "\n",
      "                                          molfile_3d  rxcui  \n",
      "0  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "1  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "2  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "3  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "4  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "\n",
      "SMILES column found! Number of SMILES: 34741\n",
      "Sample SMILES:\n",
      "['O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1', 'CC(C)(C)NC[C@H](O)c1ccc(O)c(CO)c1', 'CC(C)(C)NC[C@H](O)c1ccc(O)c(CO)c1', 'CC(C)(C)NC[C@H](O)c1ccc(O)c(CO)c1']\n"
     ]
    }
   ],
   "source": [
    "# Load and examine the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the parquet file\n",
    "data_path = \"scope_onside_common_v3.parquet\"\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check if 'smiles' column exists\n",
    "if 'smiles' in df.columns:\n",
    "    print(f\"\\nSMILES column found! Number of SMILES: {len(df['smiles'])}\")\n",
    "    print(\"Sample SMILES:\")\n",
    "    print(df['smiles'].head(10).tolist())\n",
    "else:\n",
    "    print(\"\\nAvailable columns:\", df.columns.tolist())\n",
    "    print(\"Please check which column contains the SMILES data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaa6a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "SMILES2Vec-style encoder: character-level tokenizer + embedding + CNN + BiGRU\n",
    "Outputs fixed-size vectors for each SMILES string.\n",
    "\n",
    "Usage (CLI):\n",
    "    python smiles2vec_encoder.py \\\n",
    "        --input data.csv --smiles-col smiles \\\n",
    "        --output embeddings.parquet \\\n",
    "        --batch-size 256 --device cuda \\\n",
    "        --embedding-dim 32 --cnn-ch 64 --rnn-hidden 128 --proj-dim 256\n",
    "\n",
    "Module usage:\n",
    "    from smiles2vec_encoder import Smiles2Vec, encode_smiles\n",
    "    model = Smiles2Vec(vocab=DEFAULT_VOCAB).to(\"cuda\")\n",
    "    model.eval()\n",
    "    vecs, ids = encode_smiles([\"CCO\", \"c1ccccc1\"], model, device=\"cuda\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd84dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import argparse\n",
    "import math\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Iterable, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b4d850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1 Tokenizer / Vocabulary\n",
    "# -----------------------------\n",
    "# Covers common SMILES charset; extend if your data needs more symbols\n",
    "DEFAULT_VOCAB = list(\n",
    "    \"#%()+-./0123456789=@ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\\\]abcdefghijklmnopqrstuvwxyz|*\"\n",
    ")\n",
    "# Special tokens\n",
    "PAD = \"<pad>\"\n",
    "UNK = \"<unk>\"\n",
    "SOS = \"<s>\"\n",
    "EOS = \"</s>\"\n",
    "\n",
    "def build_vocab(custom: Optional[List[str]] = None) -> Tuple[Dict[str,int], List[str]]:\n",
    "    base = list(dict.fromkeys((custom or []) + DEFAULT_VOCAB))  # stable, unique\n",
    "    tokens = [PAD, UNK, SOS, EOS] + base\n",
    "    stoi = {t:i for i,t in enumerate(tokens)}\n",
    "    return stoi, tokens\n",
    "\n",
    "@dataclass\n",
    "class SmilesTokenizer:\n",
    "    stoi: Dict[str, int]\n",
    "    itos: List[str]\n",
    "    pad_idx: int\n",
    "    unk_idx: int\n",
    "    sos_idx: int\n",
    "    eos_idx: int\n",
    "    max_len: int\n",
    "\n",
    "    @classmethod\n",
    "    def make(cls, max_len: int = 256, custom_vocab: Optional[List[str]] = None):\n",
    "        stoi, itos = build_vocab(custom_vocab)\n",
    "        return cls(\n",
    "            stoi=stoi,\n",
    "            itos=itos,\n",
    "            pad_idx=stoi[PAD],\n",
    "            unk_idx=stoi[UNK],\n",
    "            sos_idx=stoi[SOS],\n",
    "            eos_idx=stoi[EOS],\n",
    "            max_len=max_len,\n",
    "        )\n",
    "\n",
    "    def encode(self, s: str) -> List[int]:\n",
    "        # Truncate to leave room for SOS/EOS\n",
    "        inner_max = self.max_len - 2\n",
    "        s = s if len(s) <= inner_max else s[:inner_max]\n",
    "        ids = [self.sos_idx]\n",
    "        for ch in s:\n",
    "            ids.append(self.stoi.get(ch, self.unk_idx))\n",
    "        ids.append(self.eos_idx)\n",
    "        # Pad\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.pad_idx] * (self.max_len - len(ids))\n",
    "        return ids\n",
    "\n",
    "    def batch_encode(self, smiles: List[str]) -> torch.LongTensor:\n",
    "        arr = [self.encode(s) for s in smiles]\n",
    "        return torch.tensor(arr, dtype=torch.long)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2 Dataset / Collate\n",
    "# -----------------------------\n",
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, smiles: List[str], ids: Optional[List[str]] = None):\n",
    "        self.smiles = smiles\n",
    "        self.ids = ids if ids is not None else [str(i) for i in range(len(smiles))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\"id\": self.ids[idx], \"smiles\": self.smiles[idx]}\n",
    "\n",
    "def make_collate(tokenizer: SmilesTokenizer):\n",
    "    def collate(batch):\n",
    "        ids = [b[\"id\"] for b in batch]\n",
    "        s_list = [b[\"smiles\"] for b in batch]\n",
    "        x = tokenizer.batch_encode(s_list)  # (B, L)\n",
    "        return ids, x\n",
    "    return collate\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 3 SMILES2Vec Model\n",
    "# -----------------------------\n",
    "class Smiles2Vec(nn.Module):\n",
    "    \"\"\"\n",
    "    Char Embedding -> 1D CNN (narrow) + GeLU -> BiGRU -> Global MaxPool -> Projection\n",
    "    Returns a fixed-size vector per SMILES.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab: List[str],\n",
    "        embedding_dim: int = 32,\n",
    "        cnn_channels: int = 64,\n",
    "        cnn_kernel: int = 5,\n",
    "        rnn_hidden: int = 128,\n",
    "        rnn_layers: int = 1,\n",
    "        proj_dim: int = 256,\n",
    "        pad_idx: int = 0,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.pad_idx = pad_idx\n",
    "        self.embedding = nn.Embedding(len(vocab), embedding_dim, padding_idx=pad_idx)\n",
    "        self.conv = nn.Conv1d(embedding_dim, cnn_channels, kernel_size=cnn_kernel, padding=cnn_kernel//2)\n",
    "        self.act = nn.GELU()\n",
    "        self.bigru = nn.GRU(\n",
    "            input_size=cnn_channels,\n",
    "            hidden_size=rnn_hidden,\n",
    "            num_layers=rnn_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True,\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Linear(2*rnn_hidden, 2*rnn_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(2*rnn_hidden, proj_dim),\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(proj_dim)\n",
    "\n",
    "        # Kaiming/Xavier sensible defaults\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "        nn.init.kaiming_uniform_(self.conv.weight, nonlinearity=\"linear\")\n",
    "        if self.conv.bias is not None:\n",
    "            nn.init.zeros_(self.conv.bias)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def infer(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        self.eval()\n",
    "        return self.forward(x)\n",
    "\n",
    "    def forward(self, x: torch.LongTensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, L) int64\n",
    "        returns: (B, D) float32\n",
    "        \"\"\"\n",
    "        # Mask for pooling\n",
    "        mask = (x != self.pad_idx)  # (B, L)\n",
    "\n",
    "        emb = self.embedding(x)              # (B, L, E)\n",
    "        conv_in = emb.transpose(1, 2)        # (B, E, L)\n",
    "        h = self.conv(conv_in)               # (B, C, L)\n",
    "        h = self.act(h).transpose(1, 2)      # (B, L, C)\n",
    "        h, _ = self.bigru(h)                 # (B, L, 2H)\n",
    "\n",
    "        # masked global max pool over L\n",
    "        h_masked = h.masked_fill(~mask.unsqueeze(-1), float(\"-inf\"))\n",
    "        pooled, _ = torch.max(h_masked, dim=1)  # (B, 2H)\n",
    "\n",
    "        z = self.proj(self.dropout(pooled))  # (B, D)\n",
    "        z = self.norm(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Encoding Helpers\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def encode_loader(\n",
    "    loader: DataLoader,\n",
    "    model: Smiles2Vec,\n",
    "    device: str = \"cpu\",\n",
    "    normalize: bool = False,\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    model.eval()\n",
    "    all_vecs = []\n",
    "    all_ids = []\n",
    "    for ids, x in loader:\n",
    "        x = x.to(device)\n",
    "        vecs = model.infer(x)  # (B, D)\n",
    "        if normalize:\n",
    "            vecs = torch.nn.functional.normalize(vecs, p=2, dim=1)\n",
    "        all_vecs.append(vecs.cpu().numpy())\n",
    "        all_ids.extend(ids)\n",
    "    return np.concatenate(all_vecs, axis=0), all_ids\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_smiles(\n",
    "    smiles: List[str],\n",
    "    model: Smiles2Vec,\n",
    "    batch_size: int = 256,\n",
    "    max_len: int = 256,\n",
    "    device: str = \"cpu\",\n",
    "    normalize: bool = False,\n",
    "    ids: Optional[List[str]] = None,\n",
    ") -> Tuple[np.ndarray, List[str]]:\n",
    "    tokenizer = SmilesTokenizer.make(max_len=max_len)\n",
    "    ds = SmilesDataset(smiles, ids=ids)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                    collate_fn=make_collate(tokenizer), num_workers=0, pin_memory=(device.startswith(\"cuda\")))\n",
    "    return encode_loader(dl, model.to(device), device=device, normalize=normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dca98c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 81\n",
      "Model parameters: 293,984\n",
      "Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Smiles2Vec model and tokenizer\n",
    "import torch\n",
    "\n",
    "# Set device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create tokenizer with default vocabulary\n",
    "tokenizer = SmilesTokenizer.make(max_len=256)\n",
    "print(f\"Vocabulary size: {len(tokenizer.itos)}\")\n",
    "\n",
    "# Initialize the Smiles2Vec model with default parameters\n",
    "model = Smiles2Vec(\n",
    "    vocab=tokenizer.itos,\n",
    "    embedding_dim=32,\n",
    "    cnn_channels=64,\n",
    "    cnn_kernel=5,\n",
    "    rnn_hidden=128,\n",
    "    rnn_layers=1,\n",
    "    proj_dim=256,\n",
    "    pad_idx=tokenizer.pad_idx,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "# Move model to device and set to evaluation mode\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"Model initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58a027",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f413e696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting unique SMILES strings...\n",
      "Total unique SMILES: 1028\n",
      "SMILES to encode: 1028\n",
      "Sample SMILES:\n",
      "  0: O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1...\n",
      "  1: CC(C)(C)NC[C@H](O)c1ccc(O)c(CO)c1...\n",
      "  2: CN(C)CCOC(C)(c1ccccc1)c1ccccn1...\n",
      "  3: CCC(=O)N(c1ccccc1)C1(C(=O)OC)CCN(CCC(=O)OC)CC1...\n",
      "  4: N[C@@H](Cc1ccc(O)c(O)c1)C(=O)O...\n"
     ]
    }
   ],
   "source": [
    "# Extract unique SMILES strings and their IDs\n",
    "print(\"Extracting unique SMILES strings...\")\n",
    "\n",
    "# Get unique SMILES and their corresponding drug_chembl_ids\n",
    "unique_smiles_df = df[['drug_chembl_id', 'smiles']].drop_duplicates(subset=['smiles'])\n",
    "print(f\"Total unique SMILES: {len(unique_smiles_df)}\")\n",
    "\n",
    "# Extract lists for encoding\n",
    "smiles_list = unique_smiles_df['smiles'].tolist()\n",
    "drug_ids = unique_smiles_df['drug_chembl_id'].tolist()\n",
    "\n",
    "print(f\"SMILES to encode: {len(smiles_list)}\")\n",
    "print(\"Sample SMILES:\")\n",
    "for i, smi in enumerate(smiles_list[:5]):\n",
    "    print(f\"  {i}: {smi[:60]}...\")  # Show first 60 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9434ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting encoding at 19:00:19\n",
      "Encoding 1028 unique SMILES strings...\n",
      "Encoding completed at 19:00:25\n",
      "Embeddings shape: (1028, 256)\n",
      "Sample embedding (first 10 dimensions): [-1.5950437  -1.81818     1.5632088   0.17663527 -1.269098    0.07423878\n",
      "  0.09562396  0.3082791   0.06107569 -0.6813537 ]\n",
      "Embedding vector length: 256\n",
      "Encoding completed at 19:00:25\n",
      "Embeddings shape: (1028, 256)\n",
      "Sample embedding (first 10 dimensions): [-1.5950437  -1.81818     1.5632088   0.17663527 -1.269098    0.07423878\n",
      "  0.09562396  0.3082791   0.06107569 -0.6813537 ]\n",
      "Embedding vector length: 256\n"
     ]
    }
   ],
   "source": [
    "# Encode SMILES using Smiles2Vec\n",
    "from datetime import datetime\n",
    "import torch\n",
    "\n",
    "print(f\"Starting encoding at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Encoding {len(smiles_list)} unique SMILES strings...\")\n",
    "\n",
    "# Use the encode_smiles function with batch processing\n",
    "batch_size = 256  # Adjust based on your GPU memory\n",
    "normalize = False  # Set to True if you want L2-normalized vectors\n",
    "\n",
    "# Encode all SMILES\n",
    "embeddings, encoded_ids = encode_smiles(\n",
    "    smiles=smiles_list,\n",
    "    model=model,\n",
    "    batch_size=batch_size,\n",
    "    max_len=256,\n",
    "    device=device,\n",
    "    normalize=normalize,\n",
    "    ids=drug_ids\n",
    ")\n",
    "\n",
    "print(f\"Encoding completed at {datetime.now().strftime('%H:%M:%S')}\")\n",
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Sample embedding (first 10 dimensions): {embeddings[0][:10]}\")\n",
    "print(f\"Embedding vector length: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "975595dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to: smiles_embeddings_smiles2vec.parquet\n",
      "Total embeddings saved: 1028\n",
      "Embedding dimension: 256\n",
      "\n",
      "Saved DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1028 entries, 0 to 1027\n",
      "Data columns (total 4 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   drug_chembl_id  1028 non-null   object\n",
      " 1   smiles          1028 non-null   object\n",
      " 2   embedding       1028 non-null   object\n",
      " 3   embedding_dim   1028 non-null   int64 \n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 32.3+ KB\n",
      "None\n",
      "\n",
      "Sample saved data:\n",
      "  drug_chembl_id                                       smiles  \\\n",
      "0     CHEMBL1000  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "1     CHEMBL1002            CC(C)(C)NC[C@H](O)c1ccc(O)c(CO)c1   \n",
      "2     CHEMBL1004               CN(C)CCOC(C)(c1ccccc1)c1ccccn1   \n",
      "\n",
      "                                           embedding  embedding_dim  \n",
      "0  [-1.063607931137085, -0.598328173160553, 0.926...            256  \n",
      "1  [-1.0096789598464966, -0.6098981499671936, 0.9...            256  \n",
      "2  [-1.050285816192627, -0.5798623561859131, 0.99...            256  \n"
     ]
    }
   ],
   "source": [
    "# Save embeddings to parquet file\n",
    "output_path = \"smiles_embeddings_smiles2vec.parquet\"\n",
    "\n",
    "# Create DataFrame with embeddings\n",
    "embeddings_df = pd.DataFrame({\n",
    "    'drug_chembl_id': encoded_ids,\n",
    "    'smiles': smiles_list,\n",
    "    'embedding': [emb.tolist() for emb in embeddings],  # Convert numpy arrays to lists\n",
    "    'embedding_dim': [embeddings.shape[1]] * len(encoded_ids)\n",
    "})\n",
    "\n",
    "# Save to parquet\n",
    "embeddings_df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"Embeddings saved to: {output_path}\")\n",
    "print(f\"Total embeddings saved: {len(embeddings_df)}\")\n",
    "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n",
    "print(\"\\nSaved DataFrame info:\")\n",
    "print(embeddings_df.info())\n",
    "print(\"\\nSample saved data:\")\n",
    "print(embeddings_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f88f27f",
   "metadata": {},
   "source": [
    "# Embedding Quality Assessment\n",
    "\n",
    "Let's validate the quality and correctness of our generated embeddings through several tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99416b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Basic Embedding Analysis ===\n",
      "Embedding shape: (1028, 256)\n",
      "Embedding dtype: float32\n",
      "NaN values: 0\n",
      "Infinite values: 0\n",
      "\n",
      "Embedding statistics:\n",
      "Mean: -0.0000\n",
      "Std: 0.9969\n",
      "Min: -2.3843\n",
      "Max: 2.7254\n",
      "\n",
      "Vector norms:\n",
      "Mean norm: 15.9496\n",
      "Std norm: 0.0005\n",
      "Min norm: 15.9464\n",
      "Max norm: 15.9509\n",
      "Are embeddings unit normalized? False\n"
     ]
    }
   ],
   "source": [
    "# 1. Basic Embedding Statistics and Properties\n",
    "print(\"=== Basic Embedding Analysis ===\")\n",
    "print(f\"Embedding shape: {embeddings.shape}\")\n",
    "print(f\"Embedding dtype: {embeddings.dtype}\")\n",
    "\n",
    "# Check for NaN or infinite values\n",
    "nan_count = np.isnan(embeddings).sum()\n",
    "inf_count = np.isinf(embeddings).sum()\n",
    "print(f\"NaN values: {nan_count}\")\n",
    "print(f\"Infinite values: {inf_count}\")\n",
    "\n",
    "# Statistical properties\n",
    "print(f\"\\nEmbedding statistics:\")\n",
    "print(f\"Mean: {embeddings.mean():.4f}\")\n",
    "print(f\"Std: {embeddings.std():.4f}\")\n",
    "print(f\"Min: {embeddings.min():.4f}\")\n",
    "print(f\"Max: {embeddings.max():.4f}\")\n",
    "\n",
    "# Check if embeddings are normalized\n",
    "norms = np.linalg.norm(embeddings, axis=1)\n",
    "print(f\"\\nVector norms:\")\n",
    "print(f\"Mean norm: {norms.mean():.4f}\")\n",
    "print(f\"Std norm: {norms.std():.4f}\")\n",
    "print(f\"Min norm: {norms.min():.4f}\")\n",
    "print(f\"Max norm: {norms.max():.4f}\")\n",
    "\n",
    "print(f\"Are embeddings unit normalized? {np.allclose(norms, 1.0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47a2d8f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Embedding Consistency Test ===\n",
      "Test SMILES: ['CCO', 'CCO', 'c1ccccc1', 'c1ccccc1', 'CC(=O)O']\n",
      "Embedding shapes: (5, 256)\n",
      "Cosine similarity between identical SMILES 'CCO': 1.000000\n",
      "Cosine similarity between identical SMILES 'c1ccccc1': 1.000000\n",
      "Are identical SMILES producing identical embeddings? True\n"
     ]
    }
   ],
   "source": [
    "# 2. Test Embedding Consistency (Same SMILES should produce same embeddings)\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"=== Embedding Consistency Test ===\")\n",
    "\n",
    "# Test with a few sample SMILES\n",
    "test_smiles = [\"CCO\", \"CCO\", \"c1ccccc1\", \"c1ccccc1\", \"CC(=O)O\"]\n",
    "test_embeddings, _ = encode_smiles(\n",
    "    smiles=test_smiles,\n",
    "    model=model,\n",
    "    batch_size=32,\n",
    "    max_len=256,\n",
    "    device=device,\n",
    "    normalize=False,\n",
    "    ids=None\n",
    ")\n",
    "\n",
    "print(\"Test SMILES:\", test_smiles)\n",
    "print(\"Embedding shapes:\", test_embeddings.shape)\n",
    "\n",
    "# Check if identical SMILES produce identical embeddings\n",
    "similarity_01 = cosine_similarity([test_embeddings[0]], [test_embeddings[1]])[0][0]\n",
    "similarity_23 = cosine_similarity([test_embeddings[2]], [test_embeddings[3]])[0][0]\n",
    "\n",
    "print(f\"Cosine similarity between identical SMILES 'CCO': {similarity_01:.6f}\")\n",
    "print(f\"Cosine similarity between identical SMILES 'c1ccccc1': {similarity_23:.6f}\")\n",
    "\n",
    "# These should be 1.0 (or very close) for identical SMILES\n",
    "print(f\"Are identical SMILES producing identical embeddings? {similarity_01 > 0.999 and similarity_23 > 0.999}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6ea3f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Chemical Similarity Test ===\n",
      "Testing similar molecule pairs:\n",
      "  CCO vs CCC: 0.9994\n",
      "  c1ccccc1 vs c1ccccc1C: 0.9997\n",
      "  CC(=O)O vs CCC(=O)O: 0.9998\n",
      "\n",
      "Testing dissimilar molecule pairs:\n",
      "  CCO vs c1ccccc1: 0.9967\n",
      "  CC(=O)O vs CCCCCCCCCCCCCCCC: 0.9973\n",
      "  CC(=O)O vs CCC(=O)O: 0.9998\n",
      "\n",
      "Testing dissimilar molecule pairs:\n",
      "  CCO vs c1ccccc1: 0.9967\n",
      "  CC(=O)O vs CCCCCCCCCCCCCCCC: 0.9973\n",
      "  CN(C)C vs c1ccccc1: 0.9971\n",
      "\n",
      "Expected: Similar molecules should have higher similarity scores than dissimilar ones.\n",
      "  CN(C)C vs c1ccccc1: 0.9971\n",
      "\n",
      "Expected: Similar molecules should have higher similarity scores than dissimilar ones.\n"
     ]
    }
   ],
   "source": [
    "# 3. Test Chemical Similarity (Similar molecules should have similar embeddings)\n",
    "print(\"=== Chemical Similarity Test ===\")\n",
    "\n",
    "# Define chemically similar and dissimilar pairs\n",
    "similar_pairs = [\n",
    "    (\"CCO\", \"CCC\"),  # Ethanol vs Propane (short alkanes)\n",
    "    (\"c1ccccc1\", \"c1ccccc1C\"),  # Benzene vs Toluene (aromatic)\n",
    "    (\"CC(=O)O\", \"CCC(=O)O\"),  # Acetic acid vs Propionic acid\n",
    "]\n",
    "\n",
    "dissimilar_pairs = [\n",
    "    (\"CCO\", \"c1ccccc1\"),  # Alcohol vs Aromatic\n",
    "    (\"CC(=O)O\", \"CCCCCCCCCCCCCCCC\"),  # Acid vs Long alkane\n",
    "    (\"CN(C)C\", \"c1ccccc1\"),  # Amine vs Aromatic\n",
    "]\n",
    "\n",
    "print(\"Testing similar molecule pairs:\")\n",
    "for smi1, smi2 in similar_pairs:\n",
    "    emb1, _ = encode_smiles([smi1], model, device=device)\n",
    "    emb2, _ = encode_smiles([smi2], model, device=device)\n",
    "    sim = cosine_similarity([emb1[0]], [emb2[0]])[0][0]\n",
    "    print(f\"  {smi1} vs {smi2}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nTesting dissimilar molecule pairs:\")\n",
    "for smi1, smi2 in dissimilar_pairs:\n",
    "    emb1, _ = encode_smiles([smi1], model, device=device)\n",
    "    emb2, _ = encode_smiles([smi2], model, device=device)\n",
    "    sim = cosine_similarity([emb1[0]], [emb2[0]])[0][0]\n",
    "    print(f\"  {smi1} vs {smi2}: {sim:.4f}\")\n",
    "\n",
    "print(\"\\nExpected: Similar molecules should have higher similarity scores than dissimilar ones.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c4a0f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Dimensionality and Uniqueness Analysis ===\n",
      "Total embeddings: 1028\n",
      "Unique embeddings: 1028\n",
      "Are all embeddings unique? True\n",
      "\n",
      "Embedding variance per dimension:\n",
      "Mean variance: 0.0016\n",
      "Min variance: 0.0006\n",
      "Max variance: 0.0060\n",
      "Dimensions with near-zero variance: 0\n",
      "\n",
      "Pairwise distance statistics (sample of 100):\n",
      "Mean distance: 0.9001\n",
      "Std distance: 0.3499\n",
      "Min distance: 0.2306\n",
      "Max distance: 3.0900\n"
     ]
    }
   ],
   "source": [
    "# 4. Dimensionality and Uniqueness Analysis\n",
    "print(\"=== Dimensionality and Uniqueness Analysis ===\")\n",
    "\n",
    "# Check for duplicate embeddings (which shouldn't exist for unique SMILES)\n",
    "unique_embeddings = np.unique(embeddings, axis=0)\n",
    "print(f\"Total embeddings: {len(embeddings)}\")\n",
    "print(f\"Unique embeddings: {len(unique_embeddings)}\")\n",
    "print(f\"Are all embeddings unique? {len(embeddings) == len(unique_embeddings)}\")\n",
    "\n",
    "# Analyze embedding distribution\n",
    "print(f\"\\nEmbedding variance per dimension:\")\n",
    "dim_variances = np.var(embeddings, axis=0)\n",
    "print(f\"Mean variance: {dim_variances.mean():.4f}\")\n",
    "print(f\"Min variance: {dim_variances.min():.4f}\")\n",
    "print(f\"Max variance: {dim_variances.max():.4f}\")\n",
    "\n",
    "# Check if any dimensions are constant (bad sign)\n",
    "constant_dims = np.sum(dim_variances < 1e-6)\n",
    "print(f\"Dimensions with near-zero variance: {constant_dims}\")\n",
    "\n",
    "# Calculate pairwise distances to check for clustering\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "sample_size = min(100, len(embeddings))  # Sample for efficiency\n",
    "sample_embeddings = embeddings[:sample_size]\n",
    "distances = euclidean_distances(sample_embeddings)\n",
    "\n",
    "# Get upper triangle (excluding diagonal)\n",
    "upper_triangle = distances[np.triu_indices_from(distances, k=1)]\n",
    "print(f\"\\nPairwise distance statistics (sample of {sample_size}):\")\n",
    "print(f\"Mean distance: {upper_triangle.mean():.4f}\")\n",
    "print(f\"Std distance: {upper_triangle.std():.4f}\")\n",
    "print(f\"Min distance: {upper_triangle.min():.4f}\")\n",
    "print(f\"Max distance: {upper_triangle.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2987bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Architecture Validation ===\n",
      "Current model architecture:\n",
      "- Vocabulary size: 81\n",
      "- Max sequence length: 256\n",
      "- Embedding dimension: 32\n",
      "- CNN channels: 64\n",
      "- CNN kernel size: 5\n",
      "- RNN hidden size: 128\n",
      "- RNN layers: 1\n",
      "- Bidirectional RNN: True\n",
      "- Final projection dimension: 256\n",
      "\n",
      "Model parameter count: 293,984\n",
      "Model in training mode: False\n",
      "\n",
      "==================================================\n",
      "SUMMARY AND RECOMMENDATIONS:\n",
      "==================================================\n",
      "\n",
      "ðŸ” ABOUT YOUR CURRENT MODEL:\n",
      "- This is NOT the original Smiles2Vec from the research paper\n",
      "- This is a custom character-level CNN+RNN encoder\n",
      "- The original Smiles2Vec uses substructure tokenization and skip-gram training\n",
      "\n",
      "âœ… TO VERIFY EMBEDDING QUALITY:\n",
      "1. Run the validation cells above\n",
      "2. Check that identical SMILES produce identical embeddings\n",
      "3. Verify that similar molecules have higher similarity scores\n",
      "4. Ensure no NaN or infinite values exist\n",
      "5. Confirm embeddings have reasonable variance across dimensions\n",
      "\n",
      "ðŸ’¡ FOR TRUE SMILES2VEC:\n",
      "- Consider using pre-trained models from papers like:\n",
      "  â€¢ 'Molecular representation learning with language models and domain-relevant auxiliary tasks'\n",
      "  â€¢ 'SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction'\n",
      "- Or use established libraries like RDKit descriptors or Morgan fingerprints\n",
      "\n",
      "ðŸŽ¯ YOUR CURRENT EMBEDDINGS ARE VALID IF:\n",
      "- They pass the consistency tests above\n",
      "- They show reasonable chemical similarity patterns\n",
      "- They work well in downstream DTI prediction tasks\n"
     ]
    }
   ],
   "source": [
    "# 5. Model Architecture Validation\n",
    "print(\"=== Model Architecture Validation ===\")\n",
    "\n",
    "print(\"Current model architecture:\")\n",
    "print(f\"- Vocabulary size: {len(tokenizer.itos)}\")\n",
    "print(f\"- Max sequence length: {tokenizer.max_len}\")\n",
    "print(f\"- Embedding dimension: {model.embedding.embedding_dim}\")\n",
    "print(f\"- CNN channels: {model.conv.out_channels}\")\n",
    "print(f\"- CNN kernel size: {model.conv.kernel_size[0]}\")\n",
    "print(f\"- RNN hidden size: {model.bigru.hidden_size}\")\n",
    "print(f\"- RNN layers: {model.bigru.num_layers}\")\n",
    "print(f\"- Bidirectional RNN: {model.bigru.bidirectional}\")\n",
    "print(f\"- Final projection dimension: {model.proj[-1].out_features}\")\n",
    "\n",
    "print(f\"\\nModel parameter count: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Check model mode\n",
    "print(f\"Model in training mode: {model.training}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SUMMARY AND RECOMMENDATIONS:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\nðŸ” ABOUT YOUR CURRENT MODEL:\")\n",
    "print(\"- This is NOT the original Smiles2Vec from the research paper\")\n",
    "print(\"- This is a custom character-level CNN+RNN encoder\")\n",
    "print(\"- The original Smiles2Vec uses substructure tokenization and skip-gram training\")\n",
    "\n",
    "print(\"\\nâœ… TO VERIFY EMBEDDING QUALITY:\")\n",
    "print(\"1. Run the validation cells above\")\n",
    "print(\"2. Check that identical SMILES produce identical embeddings\")\n",
    "print(\"3. Verify that similar molecules have higher similarity scores\")\n",
    "print(\"4. Ensure no NaN or infinite values exist\")\n",
    "print(\"5. Confirm embeddings have reasonable variance across dimensions\")\n",
    "\n",
    "print(\"\\nðŸ’¡ FOR TRUE SMILES2VEC:\")\n",
    "print(\"- Consider using pre-trained models from papers like:\")\n",
    "print(\"  â€¢ 'Molecular representation learning with language models and domain-relevant auxiliary tasks'\")\n",
    "print(\"  â€¢ 'SMILES-BERT: Large Scale Unsupervised Pre-Training for Molecular Property Prediction'\")\n",
    "print(\"- Or use established libraries like RDKit descriptors or Morgan fingerprints\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ YOUR CURRENT EMBEDDINGS ARE VALID IF:\")\n",
    "print(\"- They pass the consistency tests above\")\n",
    "print(\"- They show reasonable chemical similarity patterns\")\n",
    "print(\"- They work well in downstream DTI prediction tasks\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
