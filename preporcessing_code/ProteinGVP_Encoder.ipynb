{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25899d9b",
   "metadata": {},
   "source": [
    "Part 1 (set up the environment, load config, define AA tables, geometry utilities (RBFs, torsions), and a PDB parser that returns clean residue rows with N/CA/C coordinates and pLDDT.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7525de44",
   "metadata": {},
   "source": [
    "Cell 1 — Imports & environment echo (Windows + CUDA 12.8 stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "17a39aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python     : 3.12.11\n",
      "OS         : nt - win32\n",
      "PyTorch    : 2.7.0+cu128\n",
      "CUDA avail : True\n",
      "Device     : NVIDIA GeForce RTX 5070 Ti\n",
      "TF32       : True\n",
      "AMP dtype  : torch.float16\n",
      "cudnn.bench: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports & environment echo\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "import os, sys, json, math, hashlib, time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "# Biopython PDB parser for robust, dependency-light parsing\n",
    "from Bio.PDB import PDBParser, is_aa\n",
    "\n",
    "# Env echo (matches your provided environment)\n",
    "print(\"Python     :\", sys.version.split()[0])\n",
    "print(\"OS         :\", os.name, \"-\", sys.platform)\n",
    "print(\"PyTorch    :\", torch.__version__)\n",
    "print(\"CUDA avail :\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device     :\", torch.cuda.get_device_name(0))\n",
    "    print(\"TF32       :\", torch.backends.cuda.matmul.allow_tf32)\n",
    "    print(\"AMP dtype  :\", torch.get_autocast_dtype('cuda'))\n",
    "    print(\"cudnn.bench:\", torch.backends.cudnn.benchmark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0e535",
   "metadata": {},
   "source": [
    "Cell 2 — Load YAML config + make dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b0515d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main parquet : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\scope_onside_common_v3.parquet\n",
      "PDB dir      : F:\\Thesis Korbi na\\dti-prediction-with-adr\\AlphaFoldData\n",
      "Graph cache  : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\graph_cache_gvp_v1\n",
      "Meta dir     : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\gvp_meta\n",
      "Embeddings   : F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\protein_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Load YAML config + make dirs\n",
    "import yaml\n",
    "\n",
    "CFG_PATH = Path(\"./gvp_config.yaml\")\n",
    "assert CFG_PATH.exists(), f\"Config file not found: {CFG_PATH}\"\n",
    "\n",
    "with open(CFG_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    CFG = yaml.safe_load(f)\n",
    "\n",
    "paths = CFG[\"paths\"]\n",
    "io_cfg = CFG[\"io\"]\n",
    "enc = CFG[\"encoder\"]\n",
    "mdl = CFG[\"model\"]\n",
    "pool = CFG[\"pooling\"]\n",
    "perf = CFG[\"performance\"]\n",
    "repro = CFG[\"repro\"]\n",
    "\n",
    "DATA_ROOT = Path(paths[\"data_root\"]).resolve()\n",
    "PDB_DIR = Path(paths[\"pdb_dir\"]).resolve()\n",
    "MAIN_PARQUET = Path(paths[\"main_parquet\"]).resolve()\n",
    "OUT_PARQUET = Path(paths[\"protein_embeddings_out\"]).resolve()\n",
    "GRAPH_DIR = Path(paths[\"graph_cache_dir\"]).resolve()\n",
    "META_DIR = Path(paths[\"meta_dir\"]).resolve()\n",
    "\n",
    "for d in [GRAPH_DIR, META_DIR, OUT_PARQUET.parent]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Main parquet :\", MAIN_PARQUET)\n",
    "print(\"PDB dir      :\", PDB_DIR)\n",
    "print(\"Graph cache  :\", GRAPH_DIR)\n",
    "print(\"Meta dir     :\", META_DIR)\n",
    "print(\"Embeddings   :\", OUT_PARQUET)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0c711",
   "metadata": {},
   "source": [
    "Cell 3 — Repro settings (TF32, AMP, torch.compile flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e4af6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP_DTYPE   : torch.float16\n",
      "torch.compile: True\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Repro & perf toggles\n",
    "seed = int(repro.get(\"seed\", 1337))\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = bool(perf.get(\"tf32\", True))\n",
    "torch.backends.cudnn.benchmark = bool(perf.get(\"cudnn_benchmark\", True))\n",
    "AMP_DTYPE = torch.float16 if perf.get(\"amp_dtype\", \"float16\") == \"float16\" else torch.bfloat16\n",
    "USE_COMPILE = bool(perf.get(\"torch_compile\", True))\n",
    "print(\"AMP_DTYPE   :\", AMP_DTYPE)\n",
    "print(\"torch.compile:\", USE_COMPILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cc1a49",
   "metadata": {},
   "source": [
    "Cell 4 — Amino acid tables & simple chemistry features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b774e2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Amino acid indices and basic chemistry scalars\n",
    "AA3_TO_AA1 = {\n",
    "    'ALA':'A','ARG':'R','ASN':'N','ASP':'D','CYS':'C','GLN':'Q','GLU':'E','GLY':'G',\n",
    "    'HIS':'H','ILE':'I','LEU':'L','LYS':'K','MET':'M','PHE':'F','PRO':'P','SER':'S',\n",
    "    'THR':'T','TRP':'W','TYR':'Y','VAL':'V'\n",
    "}\n",
    "AA1_LIST = list(\"ARNDCEQGHILKMFPSTWYV\")\n",
    "AA1_TO_IDX = {aa:i for i,aa in enumerate(AA1_LIST)}\n",
    "\n",
    "# Kyte-Doolittle hydrophobicity (scaled later)\n",
    "HYDRO = {\n",
    " 'A':1.8,'R':-4.5,'N':-3.5,'D':-3.5,'C':2.5,'Q':-3.5,'E':-3.5,'G':-0.4,'H':-3.2,'I':4.5,\n",
    " 'L':3.8,'K':-3.9,'M':1.9,'F':2.8,'P':-1.6,'S':-0.8,'T':-0.7,'W':-0.9,'Y':-1.3,'V':4.2\n",
    "}\n",
    "# Simplified charge at pH 7: +1 (K,R,H≈+0.1), -1 (D,E), else 0\n",
    "CHARGE = {aa:(1 if aa in ['K','R'] else (0.1 if aa=='H' else (-1 if aa in ['D','E'] else 0))) for aa in AA1_LIST}\n",
    "# Polarity flag: polar vs nonpolar (0/1)\n",
    "POLAR = {aa:(1 if aa in ['R','N','D','Q','E','H','K','S','T','Y','C','W'] else 0) for aa in AA1_LIST}\n",
    "\n",
    "def aa1_index(aa1: str) -> int:\n",
    "    return AA1_TO_IDX.get(aa1, 20)  # 20 -> unknown\n",
    "\n",
    "def chem_triplet(aa1: str) -> Tuple[float, float, float]:\n",
    "    return float(HYDRO.get(aa1, 0.0)), float(CHARGE.get(aa1, 0.0)), float(POLAR.get(aa1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a09e41",
   "metadata": {},
   "source": [
    "Cell 5 — Geometry helpers: normalize, RBF, angles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56b17160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Geometry helpers (normalize, RBF expansion, dihedrals)\n",
    "def unit_vec(v: np.ndarray, eps: float = 1e-8) -> np.ndarray:\n",
    "    n = np.linalg.norm(v) + eps\n",
    "    return v / n\n",
    "\n",
    "def rbf_expand(d: np.ndarray, num_bins: int, dmin: float, dmax: float) -> np.ndarray:\n",
    "    \"\"\"Gaussian RBF expansion over [dmin, dmax].\"\"\"\n",
    "    centers = np.linspace(dmin, dmax, num_bins)\n",
    "    gamma = 1.0 / ((centers[1] - centers[0] + 1e-8) ** 2)\n",
    "    return np.exp(-gamma * (d[..., None] - centers[None, ...])**2)\n",
    "\n",
    "def dihedral(p0, p1, p2, p3) -> float:\n",
    "    \"\"\"Dihedral angle (radians) for four points.\"\"\"\n",
    "    b0 = p1 - p0\n",
    "    b1 = p2 - p1\n",
    "    b2 = p3 - p2\n",
    "    # Normalize b1 for stability\n",
    "    b1n = b1 / (np.linalg.norm(b1) + 1e-8)\n",
    "    v = b0 - (b0 @ b1n) * b1n\n",
    "    w = b2 - (b2 @ b1n) * b1n\n",
    "    x = np.dot(v, w)\n",
    "    y = np.dot(np.cross(b1n, v), w)\n",
    "    return np.arctan2(y, x)\n",
    "\n",
    "def sincos(x: float) -> Tuple[float, float]:\n",
    "    return float(np.sin(x)), float(np.cos(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b624127",
   "metadata": {},
   "source": [
    "Cell 6 — Build per-residue local frames (x,y,z) from N-CA-C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "daaa4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Local backbone frame from (N, CA, C)\n",
    "def local_frame(N: np.ndarray, CA: np.ndarray, C: np.ndarray) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns an orthonormal (x,y,z) frame:\n",
    "      z = unit(CA - N)\n",
    "      x = unit( (C - CA) - proj_{z}(C - CA) )\n",
    "      y = z × x\n",
    "    \"\"\"\n",
    "    z = unit_vec(CA - N)\n",
    "    c = C - CA\n",
    "    c_orth = c - (c @ z) * z\n",
    "    x = unit_vec(c_orth)\n",
    "    y = unit_vec(np.cross(z, x))\n",
    "    return x, y, z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f43f19d",
   "metadata": {},
   "source": [
    "Cell 7 — PDB parsing (AlphaFold: pLDDT in B-factor), residue rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "101a20fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Parse AlphaFold PDB -> per-residue table with N/CA/C, pLDDT, AA, index\n",
    "def parse_af2_pdb(pdb_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a DataFrame with columns:\n",
    "      ['res_index', 'aa1', 'N', 'CA', 'C', 'pLDDT', 'valid']\n",
    "    Where N/CA/C are (3,) np.float32 arrays in Å. pLDDT from B-factor (CA atom).\n",
    "    \"\"\"\n",
    "    parser = PDBParser(QUIET=True)\n",
    "    structure = parser.get_structure(\"P\", str(pdb_path))\n",
    "    model = next(structure.get_models())  # AF2 usually one model\n",
    "\n",
    "    rows = []\n",
    "    res_i = 0\n",
    "    for chain in model:\n",
    "        for residue in chain:\n",
    "            if not is_aa(residue, standard=True):\n",
    "                continue\n",
    "            try:\n",
    "                N = residue[\"N\"].get_coord().astype(np.float32)\n",
    "                CA = residue[\"CA\"].get_coord().astype(np.float32)\n",
    "                C = residue[\"C\"].get_coord().astype(np.float32)\n",
    "            except KeyError:\n",
    "                # missing backbone atom(s)\n",
    "                rows.append({\n",
    "                    \"res_index\": res_i,\n",
    "                    \"aa1\": AA3_TO_AA1.get(residue.get_resname().upper(), \"X\"),\n",
    "                    \"N\": None, \"CA\": None, \"C\": None,\n",
    "                    \"pLDDT\": np.nan,\n",
    "                    \"valid\": False\n",
    "                })\n",
    "                res_i += 1\n",
    "                continue\n",
    "\n",
    "            # AF2 stores pLDDT in B-factor; use CA B-factor as residue pLDDT\n",
    "            plddt = float(residue[\"CA\"].get_bfactor())\n",
    "            aa1 = AA3_TO_AA1.get(residue.get_resname().upper(), \"X\")\n",
    "\n",
    "            rows.append({\n",
    "                \"res_index\": res_i,\n",
    "                \"aa1\": aa1,\n",
    "                \"N\": N, \"CA\": CA, \"C\": C,\n",
    "                \"pLDDT\": plddt,\n",
    "                \"valid\": True\n",
    "            })\n",
    "            res_i += 1\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9f5cac",
   "metadata": {},
   "source": [
    "Cell 8 — Torsions (ϕ, ψ, ω) and backbone vectors per residue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "af02d4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Compute torsions and backbone vector features\n",
    "def compute_backbone_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Adds:\n",
    "      - phi_sin, phi_cos, psi_sin, psi_cos, omega_sin, omega_cos\n",
    "      - v_N_CA (3,), v_CA_C (3,)\n",
    "      - local frame axes: frame_x, frame_y, frame_z (each (3,))\n",
    "    Missing neighbors -> NaNs for torsions; vectors default to unit/NaN-safe.\n",
    "    \"\"\"\n",
    "    n = len(df)\n",
    "    phi_sin = np.full(n, np.nan, np.float32); phi_cos = np.full(n, np.nan, np.float32)\n",
    "    psi_sin = np.full(n, np.nan, np.float32); psi_cos = np.full(n, np.nan, np.float32)\n",
    "    omg_sin = np.full(n, np.nan, np.float32); omg_cos = np.full(n, np.nan, np.float32)\n",
    "\n",
    "    v_N_CA = np.zeros((n,3), np.float32)\n",
    "    v_CA_C = np.zeros((n,3), np.float32)\n",
    "    fx = np.zeros((n,3), np.float32)\n",
    "    fy = np.zeros((n,3), np.float32)\n",
    "    fz = np.zeros((n,3), np.float32)\n",
    "\n",
    "    for i in range(n):\n",
    "        if not df.at[i, \"valid\"]:\n",
    "            continue\n",
    "        N_i, CA_i, C_i = df.at[i, \"N\"], df.at[i, \"CA\"], df.at[i, \"C\"]\n",
    "        v_N_CA[i] = unit_vec(CA_i - N_i)\n",
    "        v_CA_C[i] = unit_vec(C_i - CA_i)\n",
    "        x, y, z = local_frame(N_i, CA_i, C_i)\n",
    "        fx[i], fy[i], fz[i] = x, y, z\n",
    "\n",
    "        # torsions need neighbors\n",
    "        # phi(i) = C(i-1), N(i), CA(i), C(i)\n",
    "        # psi(i) = N(i), CA(i), C(i), N(i+1)\n",
    "        # omega(i) = CA(i-1), C(i-1), N(i), CA(i)\n",
    "        try:\n",
    "            if i > 0 and df.at[i-1, \"valid\"]:\n",
    "                Cm1 = df.at[i-1, \"C\"]\n",
    "                Np = df.at[i, \"N\"]; CAp = df.at[i, \"CA\"]; Cp = df.at[i, \"C\"]\n",
    "                Nm1 = df.at[i-1, \"N\"]; CAm1 = df.at[i-1, \"CA\"]; Cm1 = df.at[i-1, \"C\"]\n",
    "\n",
    "                # phi\n",
    "                phi = dihedral(df.at[i-1,\"C\"], Np, CAp, Cp)\n",
    "                s, c = sincos(phi); phi_sin[i], phi_cos[i] = s, c\n",
    "\n",
    "                # omega\n",
    "                omega = dihedral(CAm1, Cm1, Np, CAp)\n",
    "                s, c = sincos(omega); omg_sin[i], omg_cos[i] = s, c\n",
    "            if i+1 < n and df.at[i+1, \"valid\"]:\n",
    "                Nn = df.at[i+1, \"N\"]\n",
    "                psi = dihedral(df.at[i,\"N\"], df.at[i,\"CA\"], df.at[i,\"C\"], Nn)\n",
    "                s, c = sincos(psi); psi_sin[i], psi_cos[i] = s, c\n",
    "        except Exception:\n",
    "            # leave NaNs if any geometry missing\n",
    "            pass\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"phi_sin\"], out[\"phi_cos\"] = phi_sin, phi_cos\n",
    "    out[\"psi_sin\"], out[\"psi_cos\"] = psi_sin, psi_cos\n",
    "    out[\"omega_sin\"], out[\"omega_cos\"] = omg_sin, omg_cos\n",
    "    out[\"v_N_CA\"] = list(v_N_CA)\n",
    "    out[\"v_CA_C\"] = list(v_CA_C)\n",
    "    out[\"frame_x\"] = list(fx)\n",
    "    out[\"frame_y\"] = list(fy)\n",
    "    out[\"frame_z\"] = list(fz)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e2756f",
   "metadata": {},
   "source": [
    "Cell 9 — Positional encoding & chemistry scalars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8c6b1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Positional enc (sin/cos) and chemistry triplet\n",
    "def positional_enc(idx: np.ndarray, L: int, num_freqs: int = 8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sin/cos positional enc over residue indices normalized to [0,1].\n",
    "    Returns [len(idx), 2*num_freqs].\n",
    "    \"\"\"\n",
    "    x = idx.astype(np.float32) / max(L - 1, 1)\n",
    "    freqs = np.pi * (2 ** np.arange(num_freqs, dtype=np.float32))\n",
    "    sins = np.sin(x[:, None] * freqs[None, :])\n",
    "    coss = np.cos(x[:, None] * freqs[None, :])\n",
    "    return np.concatenate([sins, coss], axis=-1).astype(np.float32)\n",
    "\n",
    "def add_positional_and_chemistry(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    L = len(df)\n",
    "    idx = df[\"res_index\"].values.astype(np.int32)\n",
    "    pos = positional_enc(idx, L, num_freqs=8)  # shape [L, 16]\n",
    "    # Chemistry\n",
    "    hydro = np.zeros(L, np.float32)\n",
    "    charge = np.zeros(L, np.float32)\n",
    "    polar  = np.zeros(L, np.float32)\n",
    "    aa_idx = np.zeros(L, np.int64)\n",
    "    for i, aa in enumerate(df[\"aa1\"].values):\n",
    "        hydro[i], charge[i], polar[i] = chem_triplet(aa)\n",
    "        aa_idx[i] = aa1_index(aa)\n",
    "\n",
    "    out = df.copy()\n",
    "    out[\"pos_enc\"] = list(pos)\n",
    "    out[\"hydro\"] = hydro\n",
    "    out[\"charge\"] = charge\n",
    "    out[\"polar\"] = polar\n",
    "    out[\"aa_index\"] = aa_idx\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd72e3e",
   "metadata": {},
   "source": [
    "Cell 10 — Quick test on your sample PDB (A0PJK1.pdb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ed7721a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   res_index aa1  pLDDT   phi_sin   phi_cos   psi_sin   psi_cos\n",
      "0          0   M  33.94       NaN       NaN -0.887723  0.460378\n",
      "1          1   A  30.64 -0.758021 -0.652230 -0.822307  0.569044\n",
      "2          2   A  33.57  0.370100  0.928992 -0.717845  0.696203\n",
      "Valid residues: 596 / 596\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Dry run on a single AlphaFold PDB (e.g., A0PJK1.pdb)\n",
    "test_pdb = PDB_DIR / \"A0PJK1.pdb\"\n",
    "if test_pdb.exists():\n",
    "    df_res = parse_af2_pdb(test_pdb)\n",
    "    df_res = compute_backbone_features(df_res)\n",
    "    df_res = add_positional_and_chemistry(df_res)\n",
    "\n",
    "    print(df_res.head(3)[[\"res_index\",\"aa1\",\"pLDDT\",\"phi_sin\",\"phi_cos\",\"psi_sin\",\"psi_cos\"]])\n",
    "    print(\"Valid residues:\", int(df_res[\"valid\"].sum()), \"/\", len(df_res))\n",
    "else:\n",
    "    print(\"Sample PDB not found at:\", test_pdb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0f29c",
   "metadata": {},
   "source": [
    "Part 2 — Graph Construction:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c7163",
   "metadata": {},
   "source": [
    "Cell 11 — Seq-sep buckets, contact flag, md5 helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8b53afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Sequence separation buckets, contact flag, MD5 helper\n",
    "\n",
    "def seq_sep_bucket(delta: int) -> int:\n",
    "    \"\"\"Bucketize |i-j| into {0,1,2..4,5..8,9..16,>16} -> indices 0..5.\"\"\"\n",
    "    if delta == 0:     return 0\n",
    "    if delta == 1:     return 1\n",
    "    if 2 <= delta <= 4:  return 2\n",
    "    if 5 <= delta <= 8:  return 3\n",
    "    if 9 <= delta <= 16: return 4\n",
    "    return 5\n",
    "\n",
    "def contact_flag(dist_A: float, thresh_A: float = 8.0) -> int:\n",
    "    return 1 if dist_A < thresh_A else 0\n",
    "\n",
    "def file_md5(path: Path, chunk: int = 1 << 20) -> str:\n",
    "    m = hashlib.md5()\n",
    "    with open(path, \"rb\") as f:\n",
    "        while True:\n",
    "            b = f.read(chunk)\n",
    "            if not b: break\n",
    "            m.update(b)\n",
    "    return m.hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd3fe88",
   "metadata": {},
   "source": [
    "Cell 12 — k-NN ∪ radius ∪ sequence edges (Cα graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "979406fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Edge builder (k-NN ∪ radius ∪ sequence edges)\n",
    "\n",
    "def build_edges_from_CA(\n",
    "    CA: np.ndarray,\n",
    "    knn_k: int,\n",
    "    radius_A: float,\n",
    "    add_seq_edges: bool = True,\n",
    "    bidirectional: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    CA: [L,3] coordinates (float32)\n",
    "    Returns edge_index [2, E] with unique pairs (i->j).\n",
    "    \"\"\"\n",
    "    L = CA.shape[0]\n",
    "    # Pairwise distances (broadcast, no scipy)\n",
    "    # D[i,j] = ||CA[i] - CA[j]||\n",
    "    diff = CA[:, None, :] - CA[None, :, :]\n",
    "    D = np.linalg.norm(diff, axis=-1)  # [L,L]\n",
    "    np.fill_diagonal(D, np.inf)\n",
    "\n",
    "    edges = set()\n",
    "\n",
    "    # k-NN edges\n",
    "    if knn_k is not None and knn_k > 0:\n",
    "        idx_knn = np.argpartition(D, kth=knn_k, axis=1)[:, :knn_k]  # [L,k]\n",
    "        for i in range(L):\n",
    "            for j in idx_knn[i]:\n",
    "                edges.add((i, int(j)))\n",
    "\n",
    "    # radius edges\n",
    "    if radius_A is not None and radius_A > 0:\n",
    "        within = np.where(D <= radius_A)\n",
    "        for i, j in zip(within[0], within[1]):\n",
    "            edges.add((int(i), int(j)))\n",
    "\n",
    "    # sequence edges\n",
    "    if add_seq_edges:\n",
    "        for i in range(L - 1):\n",
    "            edges.add((i, i + 1))\n",
    "            edges.add((i + 1, i))\n",
    "\n",
    "    # bidirectional\n",
    "    if bidirectional:\n",
    "        extra = []\n",
    "        for (i, j) in edges:\n",
    "            extra.append((j, i))\n",
    "        for e in extra:\n",
    "            edges.add(e)\n",
    "\n",
    "    # remove self loops\n",
    "    edges = [(i, j) for (i, j) in edges if i != j]\n",
    "\n",
    "    # to array\n",
    "    if len(edges) == 0:\n",
    "        return np.empty((2, 0), dtype=np.int64)\n",
    "    edge_index = np.array(edges, dtype=np.int64).T  # [2, E]\n",
    "    # de-duplicate\n",
    "    edge_index = np.unique(edge_index, axis=1)\n",
    "    return edge_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f5076e",
   "metadata": {},
   "source": [
    "Cell 13 — Edge features (RBF distance, direction vector, seq-sep, contact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e87d9913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Compute edge features\n",
    "\n",
    "def compute_edge_features(\n",
    "    CA: np.ndarray,\n",
    "    edge_index: np.ndarray,\n",
    "    rbf_bins: int,\n",
    "    rbf_dmin: float,\n",
    "    rbf_dmax: float,\n",
    "    use_seq_sep_buckets: bool = True,\n",
    "    add_contact: bool = True\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      edge_scalar: [E, S_e]\n",
    "      edge_vector: [E, V_e, 3]\n",
    "      Also returns auxiliaries: raw distance [E,], seq_sep_idx [E,] if requested.\n",
    "    \"\"\"\n",
    "    src, dst = edge_index\n",
    "    E = edge_index.shape[1]\n",
    "\n",
    "    # distances + direction vectors\n",
    "    vec_ij = CA[dst] - CA[src]                  # [E,3]\n",
    "    dist = np.linalg.norm(vec_ij, axis=1)      # [E]\n",
    "    u_ij = (vec_ij / (dist[:, None] + 1e-8)).astype(np.float32)  # unit vectors\n",
    "\n",
    "    # RBF expansion\n",
    "    rbf = rbf_expand(dist.astype(np.float32), rbf_bins, rbf_dmin, rbf_dmax)  # [E, rbf_bins]\n",
    "\n",
    "    # sequence separation buckets\n",
    "    if use_seq_sep_buckets:\n",
    "        sep = np.abs(dst - src)\n",
    "        sep_idx = np.array([seq_sep_bucket(int(d)) for d in sep], dtype=np.int64)\n",
    "        # one-hot (6 buckets)\n",
    "        sep_oh = np.zeros((E, 6), dtype=np.float32)\n",
    "        sep_oh[np.arange(E), sep_idx] = 1.0\n",
    "    else:\n",
    "        sep_idx = None\n",
    "        sep_oh = np.zeros((E, 0), dtype=np.float32)\n",
    "\n",
    "    # contact flag\n",
    "    if add_contact:\n",
    "        contact = np.array([contact_flag(float(d)) for d in dist], dtype=np.int64)\n",
    "        contact = contact.reshape(-1, 1).astype(np.float32)\n",
    "    else:\n",
    "        contact = np.zeros((E, 0), dtype=np.float32)\n",
    "\n",
    "    # Pack edge scalars/vectors\n",
    "    edge_scalar = np.concatenate([rbf.astype(np.float32), sep_oh, contact], axis=1).astype(np.float32)  # [E, S_e]\n",
    "    edge_vector = u_ij[:, None, :].astype(np.float32)  # [E, 1, 3]  (V_e=1 so far)\n",
    "\n",
    "    return {\n",
    "        \"edge_scalar\": edge_scalar,\n",
    "        \"edge_vector\": edge_vector,\n",
    "        \"dist\": dist.astype(np.float32),\n",
    "        \"seq_sep_idx\": sep_idx\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41812d",
   "metadata": {},
   "source": [
    "Cell 14 — Node feature packing (scalar/vector blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "968267d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: Assemble node scalar/vector feature blocks (Tier B)\n",
    "\n",
    "def stack_node_features(\n",
    "    df: pd.DataFrame,\n",
    "    use_local_frame: bool = True,\n",
    "    use_backbone_dirs: bool = True,\n",
    "    use_torsions: bool = True,\n",
    "    use_positional_enc: bool = True,\n",
    "    use_pLDDT: bool = True,\n",
    "    use_basic_chemistry: bool = True\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      node_scalar: [L, S_n] float32\n",
    "      node_vector: [L, V_n, 3] float32\n",
    "      aa_index:   [L] int64\n",
    "      plddt:      [L] float32\n",
    "      valid_mask: [L] bool\n",
    "    \"\"\"\n",
    "    L = len(df)\n",
    "    scalars = []\n",
    "\n",
    "    # pLDDT\n",
    "    if use_pLDDT:\n",
    "        plddt = df[\"pLDDT\"].fillna(0.0).values.astype(np.float32)\n",
    "        scalars.append(plddt[:, None])\n",
    "    else:\n",
    "        plddt = np.zeros(L, np.float32)\n",
    "\n",
    "    # torsions (sin/cos)\n",
    "    if use_torsions:\n",
    "        for nm in [\"phi_sin\",\"phi_cos\",\"psi_sin\",\"psi_cos\",\"omega_sin\",\"omega_cos\"]:\n",
    "            v = df[nm].fillna(0.0).values.astype(np.float32)\n",
    "            scalars.append(v[:, None])\n",
    "\n",
    "    # positional encoding\n",
    "    if use_positional_enc:\n",
    "        pos = np.stack(df[\"pos_enc\"].values, axis=0).astype(np.float32)  # [L,16]\n",
    "        scalars.append(pos)\n",
    "\n",
    "    # chemistry triplet\n",
    "    if use_basic_chemistry:\n",
    "        hydro = df[\"hydro\"].values.astype(np.float32)\n",
    "        charge = df[\"charge\"].values.astype(np.float32)\n",
    "        polar  = df[\"polar\"].values.astype(np.float32)\n",
    "        scalars.append(np.stack([hydro, charge, polar], axis=1))  # [L,3]\n",
    "\n",
    "    # concat scalar block\n",
    "    node_scalar = np.concatenate(scalars, axis=1).astype(np.float32) if len(scalars) else np.zeros((L,0), np.float32)\n",
    "\n",
    "    # vector block\n",
    "    vectors = []\n",
    "    if use_local_frame:\n",
    "        fx = np.stack(df[\"frame_x\"].values, axis=0).astype(np.float32)  # [L,3]\n",
    "        fy = np.stack(df[\"frame_y\"].values, axis=0).astype(np.float32)\n",
    "        fz = np.stack(df[\"frame_z\"].values, axis=0).astype(np.float32)\n",
    "        vectors += [fx, fy, fz]\n",
    "    if use_backbone_dirs:\n",
    "        v1 = np.stack(df[\"v_N_CA\"].values, axis=0).astype(np.float32)\n",
    "        v2 = np.stack(df[\"v_CA_C\"].values, axis=0).astype(np.float32)\n",
    "        vectors += [v1, v2]\n",
    "    if len(vectors):\n",
    "        node_vector = np.stack(vectors, axis=1).astype(np.float32)  # [L, V_n, 3]\n",
    "    else:\n",
    "        node_vector = np.zeros((L, 0, 3), dtype=np.float32)\n",
    "\n",
    "    aa_index = df[\"aa_index\"].values.astype(np.int64)\n",
    "    valid_mask = df[\"valid\"].values.astype(bool)\n",
    "\n",
    "    return {\n",
    "        \"node_scalar\": node_scalar,\n",
    "        \"node_vector\": node_vector,\n",
    "        \"aa_index\": aa_index,\n",
    "        \"plddt\": plddt,\n",
    "        \"valid_mask\": valid_mask\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75375472",
   "metadata": {},
   "source": [
    "Cell 15 — Graph pack & NPZ cache writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2956cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15: Pack graph + metadata and save to NPZ\n",
    "\n",
    "def pack_and_save_graph_npz(\n",
    "    uniprot_id: str,\n",
    "    df: pd.DataFrame,\n",
    "    edge_index: np.ndarray,\n",
    "    edge_feats: Dict[str, np.ndarray],\n",
    "    node_feats: Dict[str, np.ndarray],\n",
    "    pdb_path: Path,\n",
    "    cfg: dict,\n",
    "    out_dir: Path\n",
    ") -> Path:\n",
    "    \"\"\"\n",
    "    Writes {uniprot_id}.graph.npz with:\n",
    "      node_scalar [L,S_n], node_vector [L,V_n,3], aa_index [L], valid_mask [L], plddt [L]\n",
    "      edge_index [2,E], edge_scalar [E,S_e], edge_vector [E,V_e,3]\n",
    "      CA coords [L,3] (for debugging) and metadata json\n",
    "    \"\"\"\n",
    "    L = len(df)\n",
    "    CA = np.stack(df[\"CA\"].values, axis=0).astype(np.float32)\n",
    "\n",
    "    meta = {\n",
    "        \"uniprot_id\": uniprot_id,\n",
    "        \"L\": int(L),\n",
    "        \"E\": int(edge_index.shape[1]),\n",
    "        \"node_scalar_shape\": list(node_feats[\"node_scalar\"].shape),\n",
    "        \"node_vector_shape\": list(node_feats[\"node_vector\"].shape),\n",
    "        \"edge_scalar_shape\": list(edge_feats[\"edge_scalar\"].shape),\n",
    "        \"edge_vector_shape\": list(edge_feats[\"edge_vector\"].shape),\n",
    "        \"knn_k\": int(cfg[\"encoder\"][\"knn_k\"]),\n",
    "        \"radius_A\": float(cfg[\"encoder\"][\"radius_cutoff_A\"]),\n",
    "        \"add_sequence_edges\": bool(cfg[\"encoder\"][\"add_sequence_edges\"]),\n",
    "        \"bidirectional_edges\": bool(cfg[\"encoder\"][\"bidirectional_edges\"]),\n",
    "        \"rbf_bins\": int(cfg[\"encoder\"][\"rbf_bins\"]),\n",
    "        \"rbf_range\": [float(cfg[\"encoder\"][\"rbf_dmin\"]), float(cfg[\"encoder\"][\"rbf_dmax\"])],\n",
    "        \"encoder_version\": cfg[\"repro\"][\"encoder_version\"],\n",
    "        \"cache_version\": int(cfg[\"repro\"][\"cache_version\"]),\n",
    "        \"pdb_md5\": file_md5(pdb_path),\n",
    "        \"pdb_path\": str(pdb_path.resolve()),\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%dT%H:%M:%S%z\", time.localtime()),\n",
    "    }\n",
    "\n",
    "    out_path = out_dir / f\"{uniprot_id}.graph.npz\"\n",
    "    np.savez_compressed(\n",
    "        out_path,\n",
    "        node_scalar=node_feats[\"node_scalar\"],\n",
    "        node_vector=node_feats[\"node_vector\"],\n",
    "        aa_index=node_feats[\"aa_index\"],\n",
    "        valid_mask=node_feats[\"valid_mask\"],\n",
    "        plddt=node_feats[\"plddt\"],\n",
    "        edge_index=edge_index.astype(np.int64),\n",
    "        edge_scalar=edge_feats[\"edge_scalar\"],\n",
    "        edge_vector=edge_feats[\"edge_vector\"],\n",
    "        CA=CA,\n",
    "        meta=json.dumps(meta).encode(\"utf-8\")\n",
    "    )\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d3175",
   "metadata": {},
   "source": [
    "Cell 16 — One-protein graph builder (end-to-end) + dry run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20823520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\graph_cache_gvp_v1\\A0PJK1.graph.npz\n",
      "L: 596 E: 26116\n",
      "node_scalar: (596, 26) node_vector: (596, 5, 3)\n",
      "edge_scalar: (26116, 39) edge_vector: (26116, 1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Build & cache graph for a UniProt ID (end-to-end)\n",
    "\n",
    "def build_and_cache_graph_for_uniprot(uniprot_id: str) -> Dict[str, object]:\n",
    "    pdb_path = PDB_DIR / f\"{uniprot_id}.pdb\"\n",
    "    assert pdb_path.exists(), f\"PDB not found for {uniprot_id} at {pdb_path}\"\n",
    "\n",
    "    # 1) Parse & per-residue features\n",
    "    df = parse_af2_pdb(pdb_path)\n",
    "    df = compute_backbone_features(df)\n",
    "    df = add_positional_and_chemistry(df)\n",
    "\n",
    "    # Ensure all residues valid for Tier B (your dataset claims no missing)\n",
    "    assert df[\"valid\"].all(), \"Encountered invalid residues; your data promised no missing.\"\n",
    "\n",
    "    # 2) Build edges on Cα\n",
    "    CA = np.stack(df[\"CA\"].values, axis=0).astype(np.float32)\n",
    "    edge_index = build_edges_from_CA(\n",
    "        CA=CA,\n",
    "        knn_k=int(enc[\"knn_k\"]),\n",
    "        radius_A=float(enc[\"radius_cutoff_A\"]),\n",
    "        add_seq_edges=bool(enc[\"add_sequence_edges\"]),\n",
    "        bidirectional=bool(enc[\"bidirectional_edges\"])\n",
    "    )\n",
    "\n",
    "    # 3) Edge features\n",
    "    edge_feats = compute_edge_features(\n",
    "        CA=CA,\n",
    "        edge_index=edge_index,\n",
    "        rbf_bins=int(enc[\"rbf_bins\"]),\n",
    "        rbf_dmin=float(enc[\"rbf_dmin\"]),\n",
    "        rbf_dmax=float(enc[\"rbf_dmax\"]),\n",
    "        use_seq_sep_buckets=bool(enc[\"use_seq_sep_buckets\"]),\n",
    "        add_contact=bool(enc[\"add_contact_flag\"])\n",
    "    )\n",
    "\n",
    "    # 4) Node features\n",
    "    node_feats = stack_node_features(\n",
    "        df=df,\n",
    "        use_local_frame=bool(enc[\"use_local_frame\"]),\n",
    "        use_backbone_dirs=bool(enc[\"use_backbone_dirs\"]),\n",
    "        use_torsions=bool(enc[\"use_torsions\"]),\n",
    "        use_positional_enc=bool(enc[\"use_positional_enc\"]),\n",
    "        use_pLDDT=bool(enc[\"use_pLDDT\"]),\n",
    "        use_basic_chemistry=bool(enc[\"use_basic_chemistry\"])\n",
    "    )\n",
    "\n",
    "    # 5) Save NPZ\n",
    "    out_path = pack_and_save_graph_npz(\n",
    "        uniprot_id=uniprot_id,\n",
    "        df=df,\n",
    "        edge_index=edge_index,\n",
    "        edge_feats=edge_feats,\n",
    "        node_feats=node_feats,\n",
    "        pdb_path=pdb_path,\n",
    "        cfg=CFG,\n",
    "        out_dir=GRAPH_DIR\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"graph_path\": out_path,\n",
    "        \"L\": len(df),\n",
    "        \"E\": int(edge_index.shape[1]),\n",
    "        \"node_scalar_shape\": node_feats[\"node_scalar\"].shape,\n",
    "        \"node_vector_shape\": node_feats[\"node_vector\"].shape,\n",
    "        \"edge_scalar_shape\": edge_feats[\"edge_scalar\"].shape,\n",
    "        \"edge_vector_shape\": edge_feats[\"edge_vector\"].shape,\n",
    "    }\n",
    "\n",
    "# --- Dry run on A0PJK1 ---\n",
    "dry = build_and_cache_graph_for_uniprot(\"A0PJK1\")\n",
    "print(\"Saved:\", dry[\"graph_path\"])\n",
    "print(\"L:\", dry[\"L\"], \"E:\", dry[\"E\"])\n",
    "print(\"node_scalar:\", dry[\"node_scalar_shape\"], \"node_vector:\", dry[\"node_vector_shape\"])\n",
    "print(\"edge_scalar:\", dry[\"edge_scalar_shape\"], \"edge_vector:\", dry[\"edge_vector_shape\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ee110",
   "metadata": {},
   "source": [
    "Cell 17 — Scalar/Vector norms, LayerNorm, GVP block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "64a4ae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: SV utils, LayerNorm, and GVP block\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LayerNormSV(nn.Module):\n",
    "    \"\"\"LayerNorm on scalar stream only; vector stream gets per-vector L2 re-scale.\"\"\"\n",
    "    def __init__(self, s_dim: int, v_dim: int, eps: float = 1e-5):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(s_dim, eps=eps)\n",
    "        self.v_dim = v_dim\n",
    "\n",
    "    def forward(self, s: torch.Tensor, v: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # s: [N, S], v: [N, V, 3]\n",
    "        s = self.ln(s)\n",
    "        if v.shape[1] > 0:\n",
    "            # normalize vector magnitudes softly (keep direction)\n",
    "            vn = torch.linalg.norm(v, dim=-1, keepdim=True).clamp_min(1e-6)\n",
    "            v = v / vn\n",
    "        return s, v\n",
    "\n",
    "class GVP(nn.Module):\n",
    "    \"\"\"\n",
    "    Geometric Vector Perceptron: maps (s_in, v_in) -> (s_out, v_out)\n",
    "    Uses gating from scalars to modulate vectors. All linear maps are equivariant\n",
    "    (vectors are mixed only with vectors).\n",
    "    \"\"\"\n",
    "    def __init__(self, s_in: int, v_in: int, s_out: int, v_out: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.s_in, self.v_in, self.s_out, self.v_out = s_in, v_in, s_out, v_out\n",
    "\n",
    "        # Scalar pathway\n",
    "        self.ws = nn.Linear(s_in + (v_in if v_in > 0 else 0), s_out)\n",
    "\n",
    "        # Vector pathway: mix vectors with vectors only\n",
    "        self.wv = nn.Linear(v_in, v_out) if v_in > 0 and v_out > 0 else None\n",
    "\n",
    "        # Gates to modulate vector magnitudes using scalars\n",
    "        self.v_gate = nn.Linear(s_out, v_out) if v_out > 0 else None\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, s: torch.Tensor, v: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        # s: [N, S_in], v: [N, V_in, 3]\n",
    "        if self.v_in > 0:\n",
    "            v_norm = torch.linalg.norm(v, dim=-1)  # [N, V_in]\n",
    "            s_cat = torch.cat([s, v_norm], dim=-1)  # concat norms into scalar stream\n",
    "        else:\n",
    "            s_cat = s\n",
    "\n",
    "        s_out = self.ws(s_cat)\n",
    "        s_out = F.silu(s_out)\n",
    "        s_out = self.drop(s_out)\n",
    "\n",
    "        if self.wv is not None:\n",
    "            # Mix vector channels linearly, preserving 3D\n",
    "            # Rearrange to [N, 3, V_in] -> [N, 3, V_out] -> back to [N, V_out, 3]\n",
    "            W = self.wv.weight   # [V_out, V_in]\n",
    "            b = self.wv.bias     # [V_out] or None\n",
    "            v_out = torch.einsum('bvc,ov->boc', v, W)  # (N,V_in,3) x (V_out,V_in) -> (N,V_out,3)\n",
    "            if b is not None:\n",
    "                v_out = v_out + b.view(1, -1, 1)       # broadcast over xyz\n",
    "\n",
    "            # Gate vectors with scalars\n",
    "            g = torch.sigmoid(self.v_gate(s_out)).unsqueeze(-1)  # [N, V_out, 1]\n",
    "            v_out = v_out * g\n",
    "            v_out = self.drop(v_out)\n",
    "        else:\n",
    "            v_out = v.new_zeros((v.shape[0], self.v_out, 3))\n",
    "\n",
    "        return s_out, v_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f8b8d1",
   "metadata": {},
   "source": [
    "Cell 18 — Message passing layer (GVPConv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2f2114ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18: GVPConv layer with edge conditioning\n",
    "\n",
    "class GVPConv(nn.Module):\n",
    "    \"\"\"\n",
    "    One GVP message-passing layer:\n",
    "      - Edge encoder turns (edge_scalar, edge_vector) into edge messages.\n",
    "      - Messages combine src node -> dst using edge conditioning.\n",
    "      - Node update via a GVP.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        s_node: int, v_node: int,\n",
    "        s_edge: int, v_edge: int,\n",
    "        s_hidden: int, v_hidden: int,\n",
    "        dropout: float = 0.1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        # Edge encoder: (s_e, v_e) -> (s_h, v_h)\n",
    "        self.edge_gvp = GVP(s_edge, v_edge, s_hidden, v_hidden, dropout=dropout)\n",
    "        self.edge_norm = LayerNormSV(s_hidden, v_hidden)\n",
    "\n",
    "        # Message combiner: merge src node with edge message -> (s_h, v_h)\n",
    "        self.msg_gvp = GVP(s_node + s_hidden, v_node + v_hidden, s_hidden, v_hidden, dropout=dropout)\n",
    "        self.msg_norm = LayerNormSV(s_hidden, v_hidden)\n",
    "\n",
    "        # Node updater: (s_node + s_h, v_node + v_h) -> (s_node, v_node)\n",
    "        self.up_gvp = GVP(s_node + s_hidden, v_node + v_hidden, s_node, v_node, dropout=dropout)\n",
    "        self.up_norm = LayerNormSV(s_node, v_node)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        s: torch.Tensor, v: torch.Tensor,\n",
    "        edge_index: torch.Tensor,\n",
    "        e_s: torch.Tensor, e_v: torch.Tensor\n",
    "    ) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        s: [N, S_node], v: [N, V_node, 3]\n",
    "        edge_index: [2, E] (src, dst)\n",
    "        e_s: [E, S_edge], e_v: [E, V_edge, 3]\n",
    "        \"\"\"\n",
    "        src, dst = edge_index\n",
    "\n",
    "        # Encode edges\n",
    "        e_s_h, e_v_h = self.edge_gvp(e_s, e_v)\n",
    "        e_s_h, e_v_h = self.edge_norm(e_s_h, e_v_h)\n",
    "\n",
    "        # Build messages from src nodes + edge encodings\n",
    "        msg_s_in = torch.cat([s[src], e_s_h], dim=-1)              # [E, S_node + S_h]\n",
    "        msg_v_in = torch.cat([v[src], e_v_h], dim=1)               # [E, V_node + V_h, 3]\n",
    "        m_s, m_v = self.msg_gvp(msg_s_in, msg_v_in)\n",
    "        m_s, m_v = self.msg_norm(m_s, m_v)\n",
    "\n",
    "        # --- DTYPE FIX for AMP: match message dtypes to accumulators ---\n",
    "        m_s = m_s.to(s.dtype)\n",
    "        m_v = m_v.to(v.dtype)\n",
    "\n",
    "        # Aggregate to destinations\n",
    "        N = s.size(0)\n",
    "        S_h = m_s.size(-1)\n",
    "        V_h = m_v.size(1)\n",
    "\n",
    "        agg_s = torch.zeros((N, S_h), device=s.device, dtype=s.dtype)\n",
    "        agg_v = torch.zeros((N, V_h, 3), device=v.device, dtype=v.dtype)\n",
    "\n",
    "        agg_s.index_add_(0, dst, m_s)\n",
    "        agg_v.index_add_(0, dst, m_v)\n",
    "\n",
    "\n",
    "        # Update nodes (residual)\n",
    "        up_s_in = torch.cat([s, agg_s], dim=-1)\n",
    "        up_v_in = torch.cat([v, agg_v], dim=1)\n",
    "        s_new, v_new = self.up_gvp(up_s_in, up_v_in)\n",
    "        s_new, v_new = self.up_norm(s_new, v_new)\n",
    "\n",
    "        return s_new, v_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab60ec02",
   "metadata": {},
   "source": [
    "Cell 19 — Input embeddings & initial projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4189d4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19: AA embedding and initial scalar/vector projections\n",
    "\n",
    "class InputSVProjector(nn.Module):\n",
    "    \"\"\"\n",
    "    Projects raw node scalar/vector features to model's working dims.\n",
    "    Also embeds amino acid indices and concatenates into scalar stream.\n",
    "    \"\"\"\n",
    "    def __init__(self, s_in: int, v_in: int, s_node: int, v_node: int, aa_vocab: int = 21, aa_emb: int = 16, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.aa_emb = nn.Embedding(aa_vocab, aa_emb)\n",
    "        self.pre_s = nn.Linear(s_in + aa_emb, s_node)\n",
    "        self.pre_v = nn.Linear(v_in, v_node) if v_in > 0 and v_node > 0 else None\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, node_scalar: torch.Tensor, node_vector: torch.Tensor, aa_index: torch.Tensor):\n",
    "        # node_scalar: [N, S_in], node_vector: [N, V_in, 3], aa_index: [N]\n",
    "        aa_e = self.aa_emb(aa_index)                         # [N, aa_emb]\n",
    "        s_in = torch.cat([node_scalar, aa_e], dim=-1)\n",
    "        s = F.silu(self.pre_s(s_in))\n",
    "        s = self.drop(s)\n",
    "\n",
    "        if self.pre_v is not None and node_vector.size(1) > 0:\n",
    "            # (N, V_in, 3) -> linear on channel dim\n",
    "            # reshape to (N,3,V_in) x (V_in,V_node) -> (N,3,V_node) -> (N,V_node,3)\n",
    "            W = self.pre_v.weight   # [V_out, V_in]\n",
    "            b = self.pre_v.bias     # [V_out] or None\n",
    "            v = torch.einsum('bvc,ov->boc', node_vector, W)  # -> (N,V_out,3)\n",
    "            if b is not None:\n",
    "                v = v + b.view(1, -1, 1)\n",
    "            v = self.drop(v)\n",
    "        else:\n",
    "            v = node_vector.new_zeros((node_vector.size(0), 0, 3))\n",
    "\n",
    "        return s, v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338d738",
   "metadata": {},
   "source": [
    "Cell 20 — Attention pooling with pLDDT gating + mean-pool concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0bb15fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20: Attention pooling with optional pLDDT gating and mean-pool concat\n",
    "\n",
    "class AttnPool(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, gate_plddt: bool = True):\n",
    "        super().__init__()\n",
    "        self.gate_plddt = gate_plddt\n",
    "        self.attn = nn.Linear(in_dim, 1)\n",
    "        self.proj = nn.Linear(in_dim, out_dim)\n",
    "\n",
    "        # gate maps pLDDT (0..100) -> [0,1] via affine+sigmoid\n",
    "        self.gate_a = nn.Parameter(torch.tensor(0.05))  # scale\n",
    "        self.gate_b = nn.Parameter(torch.tensor(-3.0))  # bias\n",
    "\n",
    "    def forward(self, h: torch.Tensor, plddt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: [L, H], plddt: [L]\n",
    "        returns pooled [out_dim]\n",
    "        \"\"\"\n",
    "        a = self.attn(h).squeeze(-1)                    # [L]\n",
    "        if self.gate_plddt:\n",
    "            g = torch.sigmoid(self.gate_a * (plddt/100.0) + self.gate_b)  # [L]\n",
    "            a = a + torch.log(g.clamp_min(1e-6))        # log-space gating\n",
    "        w = torch.softmax(a, dim=0)                     # [L]\n",
    "        pooled = (w.unsqueeze(-1) * h).sum(dim=0)       # [H]\n",
    "        return self.proj(pooled)\n",
    "\n",
    "class GlobalReadout(nn.Module):\n",
    "    \"\"\"\n",
    "    Final global embedding = concat(attention_pool, mean_pool) -> linear to target dim.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim: int, out_dim: int, use_attention: bool = True, gate_plddt: bool = True, concat_mean: bool = True):\n",
    "        super().__init__()\n",
    "        self.use_attention = use_attention\n",
    "        self.concat_mean = concat_mean\n",
    "        if use_attention:\n",
    "            self.attn = AttnPool(in_dim, in_dim, gate_plddt=gate_plddt)\n",
    "        concat_dim = in_dim + (in_dim if use_attention and concat_mean else 0)\n",
    "        self.proj = nn.Linear(concat_dim if use_attention else in_dim, out_dim)\n",
    "\n",
    "    def forward(self, h: torch.Tensor, plddt: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        h: [L, H]\n",
    "        \"\"\"\n",
    "        parts = []\n",
    "        if self.use_attention:\n",
    "            ha = self.attn(h, plddt)     # [H]\n",
    "            parts.append(ha)\n",
    "            if self.concat_mean:\n",
    "                parts.append(h.mean(dim=0))\n",
    "            hcat = torch.cat(parts, dim=-1)\n",
    "            return self.proj(hcat)\n",
    "        else:\n",
    "            return self.proj(h.mean(dim=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9d61cf",
   "metadata": {},
   "source": [
    "Cell 21 — Full ProteinEncoder (10× GVPConv + residue & global heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "96aa5d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21: ProteinEncoder assembly\n",
    "\n",
    "class ProteinGVPEncoder(nn.Module):\n",
    "    def __init__(self, cfg: dict):\n",
    "        super().__init__()\n",
    "        enc = cfg[\"encoder\"]; mdl = cfg[\"model\"]; pool = cfg[\"pooling\"]\n",
    "\n",
    "        self.s_node = mdl[\"node_scalar_dim\"]\n",
    "        self.v_node = mdl[\"node_vector_dim\"]\n",
    "        self.s_edge = enc[\"rbf_bins\"] + (6 if enc[\"use_seq_sep_buckets\"] else 0) + (1 if enc[\"add_contact_flag\"] else 0)\n",
    "        self.v_edge = 1  # from edge direction unit vector\n",
    "\n",
    "        self.res_dim = mdl[\"residue_embed_dim\"]\n",
    "        self.glob_dim = mdl[\"global_embed_dim\"]\n",
    "        self.num_layers = mdl[\"num_layers\"]\n",
    "        self.dropout = mdl.get(\"scalar_dropout\", 0.1)\n",
    "\n",
    "        # Input projector (raw -> working dims)\n",
    "        # Infer raw dims from your graph cache design:\n",
    "        # node_scalar raw = 26 (from your print), node_vector raw = 5\n",
    "        self.input_proj = InputSVProjector(\n",
    "            s_in=26, v_in=5, s_node=self.s_node, v_node=self.v_node, aa_vocab=21, aa_emb=16, dropout=self.dropout\n",
    "        )\n",
    "\n",
    "        # Stack of GVPConv layers\n",
    "        layers = []\n",
    "        for _ in range(self.num_layers):\n",
    "            layers.append(GVPConv(\n",
    "                s_node=self.s_node, v_node=self.v_node,\n",
    "                s_edge=self.s_edge, v_edge=self.v_edge,\n",
    "                s_hidden=self.s_node, v_hidden=self.v_node,\n",
    "                dropout=self.dropout\n",
    "            ))\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "        # Residue head: collapse (s,v) to residue embedding\n",
    "        self.res_scalar_head = nn.Linear(self.s_node, self.res_dim)\n",
    "        self.res_vector_head = nn.Linear(self.v_node, self.res_dim) if self.v_node > 0 else None\n",
    "\n",
    "        # Global readout\n",
    "        self.readout = GlobalReadout(\n",
    "            in_dim=self.res_dim,\n",
    "            out_dim=self.glob_dim,\n",
    "            use_attention=bool(pool[\"use_attention_pool\"]),\n",
    "            gate_plddt=bool(pool[\"plddt_gate_attention\"]),\n",
    "            concat_mean=bool(pool[\"concat_mean_pool\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, batch: dict) -> dict:\n",
    "        \"\"\"\n",
    "        batch keys:\n",
    "          node_scalar [L,S_n_raw], node_vector [L,V_n_raw,3], aa_index [L],\n",
    "          edge_index [2,E], edge_scalar [E,S_e_raw], edge_vector [E,V_e_raw,3],\n",
    "          plddt [L]\n",
    "        \"\"\"\n",
    "        ns = batch[\"node_scalar\"]; nv = batch[\"node_vector\"]; aa = batch[\"aa_index\"]\n",
    "        ei = batch[\"edge_index\"]; es = batch[\"edge_scalar\"]; ev = batch[\"edge_vector\"]\n",
    "        plddt = batch[\"plddt\"]\n",
    "\n",
    "        # Project inputs\n",
    "        s, v = self.input_proj(ns, nv, aa)\n",
    "\n",
    "        # Edge linear pre-map to working dims (if needed, we already set s_edge/v_edge consistent)\n",
    "        e_s = es\n",
    "        e_v = ev\n",
    "\n",
    "        # GVPConv stack\n",
    "        for layer in self.layers:\n",
    "            s, v = layer(s, v, ei, e_s, e_v)\n",
    "\n",
    "        # Residue embeddings from s (+ vector magnitude summary)\n",
    "        r_s = self.res_scalar_head(s)                         # [L, res_dim]\n",
    "        if self.res_vector_head is not None and v.size(1) > 0:\n",
    "            v_mag = torch.linalg.norm(v, dim=-1)             # [L, V]\n",
    "            r_v = self.res_vector_head(v_mag)                # [L, res_dim]\n",
    "            r = F.silu(r_s + r_v)\n",
    "        else:\n",
    "            r = F.silu(r_s)\n",
    "\n",
    "        # Global embedding\n",
    "        g = self.readout(r, plddt)                           # [glob_dim]\n",
    "\n",
    "        return {\"residue_emb\": r, \"global_emb\": g}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d10a7e",
   "metadata": {},
   "source": [
    "Cell 22 — Inference helper: load .npz → tensors → encode one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed42b5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residue emb shape: (596, 256)\n",
      "Global  emb shape: (1024,)\n"
     ]
    }
   ],
   "source": [
    "# Cell 22: Encode one cached graph (.npz) to residue/global embeddings\n",
    "\n",
    "def load_graph_npz(path: Path, device: torch.device) -> dict:\n",
    "    data = np.load(path, allow_pickle=True)\n",
    "    ns = torch.from_numpy(data[\"node_scalar\"]).to(device=device, dtype=torch.float32)\n",
    "    nv = torch.from_numpy(data[\"node_vector\"]).to(device=device, dtype=torch.float32)\n",
    "    aa = torch.from_numpy(data[\"aa_index\"]).to(device=device, dtype=torch.long)\n",
    "    vi = torch.from_numpy(data[\"valid_mask\"]).to(device=device, dtype=torch.bool)  # not used directly now\n",
    "    plddt = torch.from_numpy(data[\"plddt\"]).to(device=device, dtype=torch.float32)\n",
    "    ei = torch.from_numpy(data[\"edge_index\"]).to(device=device, dtype=torch.long)\n",
    "    es = torch.from_numpy(data[\"edge_scalar\"]).to(device=device, dtype=torch.float32)\n",
    "    ev = torch.from_numpy(data[\"edge_vector\"]).to(device=device, dtype=torch.float32)\n",
    "    return {\n",
    "        \"node_scalar\": ns, \"node_vector\": nv, \"aa_index\": aa,\n",
    "        \"edge_index\": ei, \"edge_scalar\": es, \"edge_vector\": ev,\n",
    "        \"plddt\": plddt, \"valid_mask\": vi\n",
    "    }\n",
    "\n",
    "# Build model\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ProteinGVPEncoder(CFG).to(DEVICE)\n",
    "\n",
    "# Optional: compile for speed\n",
    "# if USE_COMPILE and hasattr(torch, \"compile\"):\n",
    "#     model = torch.compile(model)\n",
    "\n",
    "# AMP autocast dtype from config\n",
    "amp_dtype = AMP_DTYPE\n",
    "\n",
    "# --- Dry run on A0PJK1 ---\n",
    "graph_npz = GRAPH_DIR / \"A0PJK1.graph.npz\"\n",
    "assert graph_npz.exists(), f\"Missing graph: {graph_npz}\"\n",
    "\n",
    "batch = load_graph_npz(graph_npz, DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    with torch.autocast(device_type=\"cuda\", dtype=amp_dtype) if DEVICE.type == \"cuda\" else torch.no_grad():\n",
    "        out = model(batch)\n",
    "re, ge = out[\"residue_emb\"], out[\"global_emb\"]\n",
    "print(\"Residue emb shape:\", tuple(re.shape))\n",
    "print(\"Global  emb shape:\", tuple(ge.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9085c9",
   "metadata": {},
   "source": [
    "Part 4 — Batch-encode all proteins and write the final Parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acaf0b68",
   "metadata": {},
   "source": [
    "Cell 23 — Load main Parquet & list unique proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e945fff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique proteins: 2385\n"
     ]
    }
   ],
   "source": [
    "# Cell 23: List unique UniProt IDs from main dataset\n",
    "MAIN_DF = pd.read_parquet(MAIN_PARQUET)\n",
    "uni_prots = pd.unique(MAIN_DF[\"target_uniprot_id\"]).tolist()\n",
    "print(\"Unique proteins:\", len(uni_prots))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eeaa38b",
   "metadata": {},
   "source": [
    "Cell 24 — Helper: ensure graph exists, then encode one protein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "491ed673",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24: Ensure graph exists -> load -> encode -> return row dict\n",
    "from tqdm import tqdm\n",
    "\n",
    "def ensure_graph(uniprot_id: str) -> Path:\n",
    "    npz_path = GRAPH_DIR / f\"{uniprot_id}.graph.npz\"\n",
    "    if not npz_path.exists():\n",
    "        # Build it now (uses PDB in PDB_DIR)\n",
    "        _ = build_and_cache_graph_for_uniprot(uniprot_id)\n",
    "    return npz_path\n",
    "\n",
    "@torch.no_grad()\n",
    "def encode_one(uniprot_id: str) -> Optional[dict]:\n",
    "    npz_path = ensure_graph(uniprot_id)\n",
    "    if not npz_path.exists():\n",
    "        print(f\"[WARN] Graph missing for {uniprot_id}, skipping.\")\n",
    "        return None\n",
    "\n",
    "    batch = load_graph_npz(npz_path, DEVICE)\n",
    "    model.eval()\n",
    "    # autocast for speed; you already have AMP dtype set\n",
    "    if DEVICE.type == \"cuda\":\n",
    "        ctx = torch.autocast(device_type=\"cuda\", dtype=amp_dtype)\n",
    "    else:\n",
    "        from contextlib import nullcontext\n",
    "        ctx = nullcontext()\n",
    "\n",
    "    with ctx:\n",
    "        out = model(batch)\n",
    "    r = out[\"residue_emb\"]           # [L, 256]\n",
    "    g = out[\"global_emb\"]            # [1024]\n",
    "\n",
    "    # Metadata\n",
    "    L = int(r.size(0))\n",
    "    mean_plddt = float(batch[\"plddt\"].mean().item())\n",
    "    # Read md5 & meta from npz\n",
    "    meta = json.loads(np.load(npz_path, allow_pickle=True)[\"meta\"].item().decode(\"utf-8\"))\n",
    "    pdb_md5 = meta[\"pdb_md5\"]\n",
    "\n",
    "    return {\n",
    "        \"uniprot_id\": uniprot_id,\n",
    "        \"length\": L,\n",
    "        \"mean_pLDDT\": round(mean_plddt, 2),\n",
    "        \"embedding_dim\": int(g.numel()),\n",
    "        \"encoder_version\": CFG[\"repro\"][\"encoder_version\"],\n",
    "        \"pdb_md5\": pdb_md5,\n",
    "        \"embedding\": g.detach().float().cpu().tolist(),  # store as list in parquet\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88d6300",
   "metadata": {},
   "source": [
    "Cell 25 — Run all, write Parquet + meta files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f15fdaea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding proteins: 100%|██████████| 2385/2385 [21:41<00:00,  1.83it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: 2382 Failed: 3\n",
      "Wrote: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\protein_embeddings.parquet\n",
      "Saved: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\gvp_meta\\build_config.json\n",
      "Saved: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\gvp_meta\\pdb_checksums.csv\n",
      "Mean embedding norm: 7.591435501926689\n",
      "Failures (first 5): [('Q6LAP9', 'kth(=36) out of bounds (17)'), ('Q9UE13', 'kth(=36) out of bounds (35)'), ('O43519', 'kth(=36) out of bounds (23)')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 25: Batch encode and write outputs\n",
    "rows = []\n",
    "fail = []\n",
    "for up in tqdm(uni_prots, desc=\"Encoding proteins\"):\n",
    "    try:\n",
    "        row = encode_one(up)\n",
    "        if row is not None:\n",
    "            rows.append(row)\n",
    "    except Exception as e:\n",
    "        fail.append((up, str(e)))\n",
    "\n",
    "df_out = pd.DataFrame(rows)\n",
    "print(\"Encoded:\", len(df_out), \"Failed:\", len(fail))\n",
    "\n",
    "# Write final embeddings parquet\n",
    "OUT_PARQUET.parent.mkdir(parents=True, exist_ok=True)\n",
    "df_out.to_parquet(OUT_PARQUET, index=False)\n",
    "print(\"Wrote:\", OUT_PARQUET)\n",
    "\n",
    "# Save meta/build_config.json\n",
    "build_cfg_path = META_DIR / \"build_config.json\"\n",
    "with open(build_cfg_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CFG, f, indent=2)\n",
    "print(\"Saved:\", build_cfg_path)\n",
    "\n",
    "# Save PDB checksums (for reproducibility)\n",
    "chk_rows = []\n",
    "for up in df_out[\"uniprot_id\"].tolist():\n",
    "    gpath = GRAPH_DIR / f\"{up}.graph.npz\"\n",
    "    meta = json.loads(np.load(gpath, allow_pickle=True)[\"meta\"].item().decode(\"utf-8\"))\n",
    "    chk_rows.append({\"uniprot_id\": up, \"pdb_md5\": meta[\"pdb_md5\"], \"graph_npz\": str(gpath)})\n",
    "chk_df = pd.DataFrame(chk_rows)\n",
    "chk_path = META_DIR / \"pdb_checksums.csv\"\n",
    "chk_df.to_csv(chk_path, index=False)\n",
    "print(\"Saved:\", chk_path)\n",
    "\n",
    "# (Optional) brief stats\n",
    "if len(df_out):\n",
    "    print(\"Mean embedding norm:\", float(np.mean([np.linalg.norm(np.array(x)) for x in df_out[\"embedding\"]])))\n",
    "if fail:\n",
    "    print(\"Failures (first 5):\", fail[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2361c0",
   "metadata": {},
   "source": [
    "Identify missing proteins (no graph / no GVP embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "276aefe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing proteins: 3\n",
      "['Q6LAP9', 'Q9UE13', 'O43519']\n"
     ]
    }
   ],
   "source": [
    "# Cell 37: Which UniProt IDs are missing from embeddings?\n",
    "main_df = pd.read_parquet(MAIN_PARQUET)\n",
    "all_up = pd.unique(main_df[\"target_uniprot_id\"]).tolist()\n",
    "\n",
    "emb_parquet = OUT_PARQUET\n",
    "if emb_parquet.exists():\n",
    "    emb_df = pd.read_parquet(emb_parquet)\n",
    "    have_up = set(emb_df[\"uniprot_id\"].tolist())\n",
    "else:\n",
    "    emb_df = pd.DataFrame(columns=[\"uniprot_id\"])\n",
    "    have_up = set()\n",
    "\n",
    "missing_up = [u for u in all_up if u not in have_up]\n",
    "print(\"Missing proteins:\", len(missing_up))\n",
    "print(missing_up[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0f679c",
   "metadata": {},
   "source": [
    "Load sequences for the missing UniProt IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3f475490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing with sequences available: 3\n",
      "[('Q6LAP9', 'MWLRAFILATLSASAAW'), ('Q9UE13', 'NASPSELRDLLSEFNVLKQVNHPHVIKLYGACSQD'), ('O43519', 'GEGDVRCRGAASAVAAAAAAARQ')]\n"
     ]
    }
   ],
   "source": [
    "# Cell 38: Fetch sequences for missing proteins\n",
    "seq_map = (main_df[[\"target_uniprot_id\",\"sequence\"]]\n",
    "           .drop_duplicates(\"target_uniprot_id\")\n",
    "           .set_index(\"target_uniprot_id\")[\"sequence\"]\n",
    "           .to_dict())\n",
    "\n",
    "missing_with_seq = [(u, seq_map.get(u, None)) for u in missing_up]\n",
    "missing_with_seq = [(u, s) for (u, s) in missing_with_seq if isinstance(s, str) and len(s) > 0]\n",
    "\n",
    "print(\"Missing with sequences available:\", len(missing_with_seq))\n",
    "print(missing_with_seq[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694eeffa",
   "metadata": {},
   "source": [
    "ESM2 loader (pick the size you already use) + sequence embedding helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c2f09fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/models/esm2_t33_650M_UR50D.pt\" to C:\\Users\\Fahmid/.cache\\torch\\hub\\checkpoints\\esm2_t33_650M_UR50D.pt\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/fair-esm/regression/esm2_t33_650M_UR50D-contact-regression.pt\" to C:\\Users\\Fahmid/.cache\\torch\\hub\\checkpoints\\esm2_t33_650M_UR50D-contact-regression.pt\n"
     ]
    }
   ],
   "source": [
    "# Cell 39: ESM2 model + helpers\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import esm  # fair-esm\n",
    "except ImportError as e:\n",
    "    raise RuntimeError(\"Please `pip install fair-esm` in your environment before running this cell.\") from e\n",
    "\n",
    "ESM_MODEL_NAME = \"esm2_t33_650M_UR50D\"  # or \"esm2_t12_35M_UR50D\" if you want speed\n",
    "model, alphabet = esm.pretrained.esm2_t33_650M_UR50D() if ESM_MODEL_NAME==\"esm2_t33_650M_UR50D\" else esm.pretrained.esm2_t12_35M_UR50D()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model = model.eval().to(DEVICE)\n",
    "\n",
    "# Autocast on GPU\n",
    "def esm_embed_sequence(uniprot_id: str, seq: str) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns a single vector: mean-pooled per-residue ESM2 representation (excluding BOS/EOS).\n",
    "    Shape depends on model: t33 -> 1280 dims; t12 -> 480 dims.\n",
    "    \"\"\"\n",
    "    data = [(uniprot_id, seq)]\n",
    "    batch_labels, batch_strs, batch_tokens = batch_converter(data)\n",
    "    batch_tokens = batch_tokens.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        with torch.autocast(device_type=\"cuda\", dtype=AMP_DTYPE) if DEVICE.type==\"cuda\" else torch.no_grad():\n",
    "            out = model(batch_tokens, repr_layers=[model.num_layers], return_contacts=False)\n",
    "    token_reprs = out[\"representations\"][model.num_layers][0]            # [L+2, D]\n",
    "    # slice off BOS/EOS\n",
    "    reps = token_reprs[1:1+len(seq)]\n",
    "    emb = reps.mean(dim=0).float().cpu().numpy()                         # [D]\n",
    "    return emb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29ea6a1",
   "metadata": {},
   "source": [
    "Learn a linear map ESM→GVP (fits once on proteins you already have)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1632f729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration pairs: (200, 1280) (200, 1024)\n",
      "W shape: (1280, 1024)\n"
     ]
    }
   ],
   "source": [
    "# Cell 40: Fit linear projection from ESM space to GVP 1024-D space\n",
    "# Build a small calibration set from proteins where we have both:\n",
    "cal_ids = emb_df[\"uniprot_id\"].tolist()\n",
    "np.random.seed(1337)\n",
    "np.random.shuffle(cal_ids)\n",
    "cal_ids = cal_ids[:min(200, len(cal_ids))]  # up to 200 for speed; increase if you want\n",
    "\n",
    "if not len(cal_ids):\n",
    "    raise RuntimeError(\"No existing GVP embeddings found to fit ESM→GVP map. Encode some proteins first.\")\n",
    "\n",
    "# Gather pairs (ESM, GVP)\n",
    "X_list, Y_list = [], []\n",
    "for up in cal_ids:\n",
    "    seq = seq_map.get(up, None)\n",
    "    if not isinstance(seq, str) or not len(seq):\n",
    "        continue\n",
    "    try:\n",
    "        x = esm_embed_sequence(up, seq)          # [D_esm]\n",
    "        y = emb_df.loc[emb_df[\"uniprot_id\"]==up, \"embedding\"].iloc[0]\n",
    "        y = np.array(y, dtype=np.float32)        # [1024]\n",
    "        if x.ndim==1 and y.ndim==1:\n",
    "            X_list.append(x)\n",
    "            Y_list.append(y)\n",
    "    except Exception as e:\n",
    "        # skip problematic sequences if any\n",
    "        pass\n",
    "\n",
    "X = np.stack(X_list, axis=0).astype(np.float32)   # [N, D_esm]\n",
    "Y = np.stack(Y_list, axis=0).astype(np.float32)   # [N, 1024]\n",
    "\n",
    "print(\"Calibration pairs:\", X.shape, Y.shape)\n",
    "\n",
    "# Solve least squares: X W = Y -> W = (X^T X)^-1 X^T Y\n",
    "# Use torch for GPU-accelerated solve (then move back to CPU for storage).\n",
    "Xt = torch.from_numpy(X).to(DEVICE)\n",
    "Yt = torch.from_numpy(Y).to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    # regularized least squares (ridge) for stability\n",
    "    lam = 1e-5\n",
    "    A = Xt.T @ Xt + lam * torch.eye(Xt.shape[1], device=DEVICE, dtype=Xt.dtype)\n",
    "    B = Xt.T @ Yt\n",
    "    W = torch.linalg.solve(A, B)  # [D_esm, 1024]\n",
    "W_cpu = W.float().cpu().numpy()\n",
    "print(\"W shape:\", W_cpu.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f90fa8",
   "metadata": {},
   "source": [
    "Embed the missing proteins and write them into the Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "953a6802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence-only embeddings: 3\n",
      "Updated: F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\protein_embeddings.parquet\n"
     ]
    }
   ],
   "source": [
    "# Cell 41: Apply ESM → project to 1024 → append to protein_embeddings.parquet\n",
    "rows = []\n",
    "for (up, seq) in missing_with_seq:\n",
    "    try:\n",
    "        x = esm_embed_sequence(up, seq)                 # [D_esm]\n",
    "        gvp_1024 = (torch.from_numpy(x).to(torch.float32) @ torch.from_numpy(W_cpu)).numpy()  # [1024]\n",
    "        # metadata\n",
    "        L = len(seq)\n",
    "        mean_plddt = float(\"nan\")  # no structure; mark NaN\n",
    "        row = {\n",
    "            \"uniprot_id\": up,\n",
    "            \"length\": int(L),\n",
    "            \"mean_pLDDT\": mean_plddt,\n",
    "            \"embedding_dim\": int(gvp_1024.shape[0]),\n",
    "            \"encoder_version\": CFG[\"repro\"][\"encoder_version\"] + \"+seq_fallback_\" + ESM_MODEL_NAME,\n",
    "            \"pdb_md5\": \"NA_sequence_only\",\n",
    "            \"embedding\": gvp_1024.astype(np.float32).tolist(),\n",
    "            \"source\": \"sequence_only\",\n",
    "        }\n",
    "        rows.append(row)\n",
    "    except Exception as e:\n",
    "        print(f\"[SKIP] {up} failed with: {e}\")\n",
    "\n",
    "add_df = pd.DataFrame(rows)\n",
    "print(\"Sequence-only embeddings:\", len(add_df))\n",
    "\n",
    "# Merge into existing parquet (append)\n",
    "if emb_parquet.exists() and len(add_df):\n",
    "    base = pd.read_parquet(emb_parquet)\n",
    "    # de-dup if re-running\n",
    "    base = base[~base[\"uniprot_id\"].isin(add_df[\"uniprot_id\"])]\n",
    "    out = pd.concat([base, add_df], ignore_index=True)\n",
    "    out.to_parquet(emb_parquet, index=False)\n",
    "    print(\"Updated:\", emb_parquet)\n",
    "elif len(add_df):\n",
    "    add_df.to_parquet(emb_parquet, index=False)\n",
    "    print(\"Wrote new:\", emb_parquet)\n",
    "else:\n",
    "    print(\"Nothing to append.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
