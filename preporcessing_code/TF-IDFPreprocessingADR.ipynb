{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e08bacc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF ADR builder (safe defaults), notebook-friendly (no CLI).\n",
    "# ---------------------------------------------------------------\n",
    "# Requirements:\n",
    "#   pip install pyarrow scikit-learn\n",
    "#\n",
    "# Inputs expected in memory:\n",
    "#   dti_df: columns [\"rxcui\", ...]\n",
    "#   adr_df: columns [\"rxnorm_ingredient_id\", \"meddra_id\", \"meddra_name\"]\n",
    "#\n",
    "# Output directory (Windows path OK):\n",
    "#   r\"F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\TFIDF_ADR_vectors\"\n",
    "\n",
    "from __future__ import annotations\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Sequence, Dict, Tuple\n",
    "import os, json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "from scipy import sparse as sp\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "94ed970c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Options (safe defaults)\n",
    "# =========================\n",
    "@dataclass\n",
    "class TFIDFOptions:\n",
    "    # Column names\n",
    "    rxcui_col_dti: str = \"rxcui\"\n",
    "    rxcui_col_adr: str = \"rxnorm_ingredient_id\"\n",
    "    meddra_id_col: str = \"meddra_id\"\n",
    "    meddra_name_col: str = \"meddra_name\"  # only for preview\n",
    "\n",
    "    # Preprocessing\n",
    "    intersect_with_dti: bool = True\n",
    "    drop_duplicates_pairs: bool = True  # drop duplicate (rxcui, meddra_id)\n",
    "\n",
    "    # TF-IDF hyperparams (safe defaults)\n",
    "    norm: Optional[str] = \"l2\"          # None, \"l1\", or \"l2\"\n",
    "    sublinear_tf: bool = False\n",
    "    smooth_idf: bool = True\n",
    "\n",
    "    # Column filtering (computed on TRAIN drugs only, avoids leakage)\n",
    "    # If float in (0,1] => fraction of train drugs; if int >=1 => absolute count\n",
    "    min_df: float | int = 1\n",
    "    max_df: float | int = 1.0           # 1.0 keeps all\n",
    "\n",
    "    # Saving\n",
    "    output_dir: str = r\"F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\TFIDF_ADR_vectors\"\n",
    "    save_long: bool = True\n",
    "    save_wide: bool = True\n",
    "\n",
    "    # Dtypes\n",
    "    float_dtype: str = \"float32\"\n",
    "\n",
    "    # Previews\n",
    "    preview_n_drugs: int = 3  # set 0 to skip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0328eb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Helpers\n",
    "# =========================\n",
    "def _ensure_str_series(s: pd.Series) -> pd.Series:\n",
    "    return s.astype(str).str.strip()\n",
    "\n",
    "def _compute_df_thresholds(min_df, max_df, n_docs: int) -> Tuple[int, int]:\n",
    "    \"\"\"Return absolute (min_df_abs, max_df_abs) based on number of train docs.\"\"\"\n",
    "    if isinstance(min_df, float) and 0 < min_df <= 1:\n",
    "        min_df_abs = int(np.ceil(min_df * n_docs))\n",
    "    elif isinstance(min_df, int) and min_df >= 1:\n",
    "        min_df_abs = min_df\n",
    "    else:\n",
    "        min_df_abs = 1\n",
    "\n",
    "    if isinstance(max_df, float) and 0 < max_df <= 1:\n",
    "        max_df_abs = int(np.floor(max_df * n_docs))\n",
    "    elif isinstance(max_df, int) and max_df >= 1:\n",
    "        max_df_abs = max_df\n",
    "    else:\n",
    "        max_df_abs = n_docs\n",
    "\n",
    "    min_df_abs = max(1, min_df_abs)\n",
    "    max_df_abs = max(min_df_abs, min(n_docs, max_df_abs))\n",
    "    return min_df_abs, max_df_abs\n",
    "\n",
    "def _to_long_parquet(X: sp.spmatrix, drugs: np.ndarray, adrs: np.ndarray,\n",
    "                     out_path: str, float_dtype=\"float32\"):\n",
    "    \"\"\"Save sparse matrix to long (tidy) parquet with columns [rxcui, meddra_id, tfidf].\"\"\"\n",
    "    X = X.tocoo(copy=False)\n",
    "    df_long = pd.DataFrame({\n",
    "        \"rxcui\": drugs[X.row],\n",
    "        \"meddra_id\": adrs[X.col],\n",
    "        \"tfidf\": X.data.astype(float_dtype)\n",
    "    })\n",
    "    df_long.to_parquet(out_path, index=False)\n",
    "\n",
    "def _to_wide_parquet(X: sp.spmatrix, drugs: np.ndarray, adrs: np.ndarray,\n",
    "                     out_path: str, float_dtype=\"float32\", chunk_cols: int = 2000):\n",
    "    \"\"\"\n",
    "    Save to wide parquet: index=rxcui, columns=meddra_<id>.\n",
    "    Writes in column chunks to keep memory friendly.\n",
    "    \"\"\"\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    n_drugs, n_adrs = X.shape\n",
    "    X = X.tocsr(copy=False)\n",
    "\n",
    "    drug_index = pd.Index(drugs, name=\"rxcui\")\n",
    "    parts = []\n",
    "    for start in range(0, n_adrs, chunk_cols):\n",
    "        end = min(start + chunk_cols, n_adrs)\n",
    "        X_chunk = X[:, start:end].toarray().astype(float_dtype, copy=False)  # dense per chunk\n",
    "        cols = [f\"meddra_{int(mid)}\" for mid in adrs[start:end]]\n",
    "        df_chunk = pd.DataFrame(X_chunk, index=drug_index, columns=cols)\n",
    "        parts.append(df_chunk)\n",
    "\n",
    "    df_wide = pd.concat(parts, axis=1)\n",
    "    df_wide.reset_index().to_parquet(out_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49086635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# Core builder\n",
    "# =========================\n",
    "def build_and_save_tfidf_parquets(\n",
    "    dti_df: pd.DataFrame,\n",
    "    adr_df: pd.DataFrame,\n",
    "    options: TFIDFOptions = TFIDFOptions(),\n",
    "    train_rxcui: Optional[Sequence[str]] = None,\n",
    "    val_rxcui: Optional[Sequence[str]] = None,\n",
    "    test_rxcui: Optional[Sequence[str]] = None,\n",
    "    per_split_subdirs: bool = True,   # NEW: write split files to subfolders\n",
    "):\n",
    "    \"\"\"\n",
    "    Build TF-IDF features for ADRs and save Parquet artifacts (safe defaults).\n",
    "    If train/val/test lists are provided, IDF is fit on train only and split-specific files\n",
    "    are saved under <output_dir>/<split>/ (when per_split_subdirs=True).\n",
    "    Otherwise, a single 'all' TF-IDF is produced in <output_dir>/.\n",
    "    \"\"\"\n",
    "    os.makedirs(options.output_dir, exist_ok=True)\n",
    "\n",
    "    # --- Validate presence of required columns ---\n",
    "    for col in [options.rxcui_col_dti]:\n",
    "        assert col in dti_df.columns, f\"DTI is missing column: {col}\"\n",
    "    for col in [options.rxcui_col_adr, options.meddra_id_col]:\n",
    "        assert col in adr_df.columns, f\"ADR is missing column: {col}\"\n",
    "\n",
    "    # --- Normalize types ---\n",
    "    dti = dti_df.copy()\n",
    "    adr = adr_df.copy()\n",
    "    dti[options.rxcui_col_dti] = _ensure_str_series(dti[options.rxcui_col_dti])\n",
    "    adr[options.rxcui_col_adr] = _ensure_str_series(adr[options.rxcui_col_adr])\n",
    "\n",
    "    # --- Keep only ADR rows that appear in DTI ---\n",
    "    if options.intersect_with_dti:\n",
    "        rxcui_keep = set(dti[options.rxcui_col_dti].unique().tolist())\n",
    "        adr = adr[adr[options.rxcui_col_adr].isin(rxcui_keep)].copy()\n",
    "\n",
    "    # --- Drop duplicate (drug, ADR) pairs ---\n",
    "    if options.drop_duplicates_pairs:\n",
    "        adr = adr.drop_duplicates(\n",
    "            subset=[options.rxcui_col_adr, options.meddra_id_col],\n",
    "            keep=\"first\"\n",
    "        )\n",
    "\n",
    "    # --- Build vocabularies ---\n",
    "    drugs = np.array(sorted(dti[options.rxcui_col_dti].unique()), dtype=object)\n",
    "    adrs_unique = np.array(sorted(adr[options.meddra_id_col].astype(np.int64).unique()), dtype=np.int64)\n",
    "\n",
    "    drug_to_row = {r: i for i, r in enumerate(drugs)}\n",
    "    adr_to_col  = {int(a): j for j, a in enumerate(adrs_unique)}\n",
    "\n",
    "    # Save index maps in ROOT\n",
    "    pd.DataFrame({\"rxcui\": drugs, \"row_id\": np.arange(len(drugs), dtype=int)}).to_parquet(\n",
    "        os.path.join(options.output_dir, \"drug_index.parquet\"), index=False\n",
    "    )\n",
    "    pd.DataFrame({\"meddra_id\": adrs_unique, \"col_id\": np.arange(len(adrs_unique), dtype=int)}).to_parquet(\n",
    "        os.path.join(options.output_dir, \"adr_index.parquet\"), index=False\n",
    "    )\n",
    "\n",
    "    # --- Presence matrix (CSR) ---\n",
    "    rows = adr[options.rxcui_col_adr].map(drug_to_row).to_numpy()\n",
    "    cols = adr[options.meddra_id_col].astype(np.int64).map(adr_to_col).to_numpy()\n",
    "    data = np.ones_like(rows, dtype=np.float32)\n",
    "\n",
    "    mask = (~pd.isna(rows)) & (~pd.isna(cols))\n",
    "    rows = rows[mask].astype(np.int64, copy=False)\n",
    "    cols = cols[mask].astype(np.int64, copy=False)\n",
    "    data = data[mask]\n",
    "\n",
    "    n_drugs = len(drugs)\n",
    "    n_adrs  = len(adrs_unique)\n",
    "    presence = sp.coo_matrix((data, (rows, cols)), shape=(n_drugs, n_adrs), dtype=np.float32).tocsr()\n",
    "\n",
    "    # --- Splits ---\n",
    "    splits: Dict[str, np.ndarray] = {}\n",
    "    if train_rxcui is None and val_rxcui is None and test_rxcui is None:\n",
    "        splits[\"all\"] = np.arange(n_drugs, dtype=int)\n",
    "        train_mask = splits[\"all\"]\n",
    "    else:\n",
    "        def _map_list_to_idx(lst: Sequence[str]) -> np.ndarray:\n",
    "            return np.array([drug_to_row[r] for r in lst if r in drug_to_row], dtype=int)\n",
    "        if train_rxcui is not None:\n",
    "            splits[\"train\"] = _map_list_to_idx(train_rxcui)\n",
    "        if val_rxcui is not None:\n",
    "            splits[\"val\"] = _map_list_to_idx(val_rxcui)\n",
    "        if test_rxcui is not None:\n",
    "            splits[\"test\"] = _map_list_to_idx(test_rxcui)\n",
    "        if \"train\" not in splits or len(splits[\"train\"]) == 0:\n",
    "            raise ValueError(\"When providing splits, train_rxcui must be non-empty.\")\n",
    "        train_mask = splits[\"train\"]\n",
    "\n",
    "    # --- Column filtering (TRAIN only) ---\n",
    "    presence_train = presence[train_mask]\n",
    "    df_train = np.asarray((presence_train > 0).sum(axis=0)).ravel().astype(int)\n",
    "    N_train = presence_train.shape[0]\n",
    "    min_df_abs, max_df_abs = _compute_df_thresholds(options.min_df, options.max_df, N_train)\n",
    "    keep_col_mask = (df_train >= min_df_abs) & (df_train <= max_df_abs)\n",
    "    kept_cols = np.where(keep_col_mask)[0]\n",
    "\n",
    "    presence   = presence[:, kept_cols]\n",
    "    adrs_kept  = adrs_unique[kept_cols]\n",
    "    df_kept    = df_train[kept_cols]\n",
    "\n",
    "    # --- Fit TF-IDF on TRAIN only ---\n",
    "    tfidf_transformer = TfidfTransformer(\n",
    "        norm=options.norm,\n",
    "        use_idf=True,\n",
    "        smooth_idf=options.smooth_idf,\n",
    "        sublinear_tf=options.sublinear_tf,\n",
    "    )\n",
    "    tfidf_transformer.fit(presence_train[:, kept_cols])\n",
    "\n",
    "    # Save IDF table in ROOT\n",
    "    idf_vals = tfidf_transformer.idf_.astype(options.float_dtype)\n",
    "    idf_table = pd.DataFrame({\n",
    "        \"meddra_id\": adrs_kept.astype(np.int64),\n",
    "        \"df\": df_kept.astype(np.int32),\n",
    "        \"idf\": idf_vals\n",
    "    })\n",
    "    idf_table.to_parquet(os.path.join(options.output_dir, \"idf_table.parquet\"), index=False)\n",
    "\n",
    "    # Optional name map for previews\n",
    "    meddra_name_map = None\n",
    "    if options.meddra_name_col in adr.columns:\n",
    "        tmp = adr[[options.meddra_id_col, options.meddra_name_col]].drop_duplicates(subset=[options.meddra_id_col])\n",
    "        meddra_name_map = dict(zip(tmp[options.meddra_id_col].astype(np.int64), tmp[options.meddra_name_col].astype(str)))\n",
    "\n",
    "    # --- Transform and save per split ---\n",
    "    all_stats = []\n",
    "\n",
    "    def _split_dir(split_name: str) -> str:\n",
    "        if \"all\" == split_name or not per_split_subdirs:\n",
    "            return options.output_dir\n",
    "        d = os.path.join(options.output_dir, split_name)\n",
    "        os.makedirs(d, exist_ok=True)\n",
    "        return d\n",
    "\n",
    "    def _save_for_mask(mask_idx: np.ndarray, split_name: str):\n",
    "        X = tfidf_transformer.transform(presence[mask_idx, :]).astype(options.float_dtype)\n",
    "        out_dir = _split_dir(split_name)\n",
    "\n",
    "        # Save Parquets inside split folder\n",
    "        if options.save_long:\n",
    "            _to_long_parquet(\n",
    "                X, drugs=drugs[mask_idx], adrs=adrs_kept,\n",
    "                out_path=os.path.join(out_dir, \"tfidf_long.parquet\"),\n",
    "                float_dtype=options.float_dtype\n",
    "            )\n",
    "        if options.save_wide:\n",
    "            _to_wide_parquet(\n",
    "                X, drugs=drugs[mask_idx], adrs=adrs_kept,\n",
    "                out_path=os.path.join(out_dir, \"tfidf_wide.parquet\"),\n",
    "                float_dtype=options.float_dtype\n",
    "            )\n",
    "\n",
    "        # Save preview in split folder\n",
    "        if meddra_name_map is not None and options.preview_n_drugs > 0:\n",
    "            sample_idx = mask_idx[:min(options.preview_n_drugs, len(mask_idx))]\n",
    "            previews = []\n",
    "            X_csr = X.tocsr()\n",
    "            for ridx in range(len(sample_idx)):\n",
    "                row = X_csr[ridx]\n",
    "                if row.nnz == 0:\n",
    "                    continue\n",
    "                top_local = np.argsort(row.data)[::-1][:10]\n",
    "                cols_local = row.indices[top_local]\n",
    "                scores = row.data[top_local]\n",
    "                for c, s in zip(cols_local, scores):\n",
    "                    mid = int(adrs_kept[c])\n",
    "                    previews.append({\n",
    "                        \"split\": split_name,\n",
    "                        \"rxcui\": drugs[sample_idx[ridx]],\n",
    "                        \"meddra_id\": mid,\n",
    "                        \"meddra_name\": meddra_name_map.get(mid, \"\"),\n",
    "                        \"tfidf\": float(s)\n",
    "                    })\n",
    "            if previews:\n",
    "                pd.DataFrame(previews).to_parquet(\n",
    "                    os.path.join(out_dir, \"preview_top_tfidf.parquet\"),\n",
    "                    index=False\n",
    "                )\n",
    "\n",
    "        density = X.nnz / (X.shape[0] * X.shape[1]) if X.shape[0] and X.shape[1] else 0.0\n",
    "        split_stats = {\"split\": split_name, \"n_drugs\": int(X.shape[0]), \"n_adrs\": int(X.shape[1]),\n",
    "                       \"nnz\": int(X.nnz), \"density\": float(density)}\n",
    "        # also save split stats to its folder\n",
    "        with open(os.path.join(out_dir, \"stats.json\"), \"w\") as f:\n",
    "            json.dump(split_stats, f, indent=2)\n",
    "        return split_stats\n",
    "\n",
    "    # Process splits\n",
    "    if \"all\" in (splits.keys()):\n",
    "        all_stats.append(_save_for_mask(splits[\"all\"], \"all\"))\n",
    "    else:\n",
    "        for split_name, mask_idx in splits.items():\n",
    "            all_stats.append(_save_for_mask(mask_idx, split_name))\n",
    "\n",
    "    # --- Global stats in ROOT ---\n",
    "    global_stats = {\n",
    "        \"n_drugs_total\": int(n_drugs),\n",
    "        \"n_adrs_original\": int(n_adrs),\n",
    "        \"n_adrs_kept\": int(len(adrs_kept)),\n",
    "        \"min_df_abs\": int(min_df_abs),\n",
    "        \"max_df_abs\": int(max_df_abs),\n",
    "        \"splits\": all_stats,\n",
    "        \"norm\": options.norm,\n",
    "        \"sublinear_tf\": options.sublinear_tf,\n",
    "        \"smooth_idf\": options.smooth_idf,\n",
    "        \"note\": \"IDF fit on train only if splits provided; per-split files saved to subfolders.\"\n",
    "    }\n",
    "    with open(os.path.join(options.output_dir, \"global_stats.json\"), \"w\") as f:\n",
    "        json.dump(global_stats, f, indent=2)\n",
    "\n",
    "    return {\"adrs_kept\": adrs_kept, \"drugs\": drugs, \"idf_table\": idf_table, \"stats\": global_stats}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bac16ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8a562857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34741 entries, 0 to 34740\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   drug_chembl_id     34741 non-null  object\n",
      " 1   target_uniprot_id  34741 non-null  object\n",
      " 2   label              34741 non-null  int64 \n",
      " 3   smiles             34741 non-null  object\n",
      " 4   sequence           34741 non-null  object\n",
      " 5   molfile_3d         34741 non-null  object\n",
      " 6   rxcui              34741 non-null  string\n",
      "dtypes: int64(1), object(5), string(1)\n",
      "memory usage: 1.9+ MB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69474 entries, 0 to 69473\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   rxnorm_ingredient_id  69474 non-null  string\n",
      " 1   meddra_id             69474 non-null  int32 \n",
      " 2   meddra_name           69474 non-null  object\n",
      "dtypes: int32(1), object(1), string(1)\n",
      "memory usage: 1.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# ---- point these to your files ----\n",
    "DTI_PATH = config[\"paths\"][\"DTI_DATASET\"]    # or .csv\n",
    "ADR_PATH = config[\"paths\"][\"ADR_DATASET\"]      # or .csv\n",
    "\n",
    "# Columns we actually need\n",
    "DTI_COLS = [\"drug_chembl_id\",\"target_uniprot_id\",\"label\",\"smiles\",\"sequence\",\"molfile_3d\",\"rxcui\"]\n",
    "ADR_COLS = [\"rxnorm_ingredient_id\",\"meddra_id\",\"meddra_name\"]\n",
    "\n",
    "def _read_any(path: Path, usecols=None) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path, columns=usecols)\n",
    "\n",
    "# Load DTI\n",
    "dti_df = _read_any(\n",
    "    DTI_PATH,\n",
    "    usecols=DTI_COLS,\n",
    ")\n",
    "\n",
    "# Load ADR\n",
    "adr_df = _read_any(\n",
    "    ADR_PATH,\n",
    "    usecols=ADR_COLS,\n",
    ")\n",
    "\n",
    "# Final type hygiene (robust against weird CSVs)\n",
    "dti_df[\"rxcui\"] = dti_df[\"rxcui\"].astype(\"string\").str.strip()\n",
    "adr_df[\"rxnorm_ingredient_id\"] = adr_df[\"rxnorm_ingredient_id\"].astype(\"string\").str.strip()\n",
    "adr_df[\"meddra_id\"] = pd.to_numeric(adr_df[\"meddra_id\"], downcast=\"integer\")\n",
    "\n",
    "print(dti_df.info())\n",
    "print(adr_df.info())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c17d5d",
   "metadata": {},
   "source": [
    "A) One-shot build (no explicit splits).\n",
    "Fits IDF on all drugs, saves tfidf_wide.parquet and tfidf_long.parquet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b4ebb9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_drugs_total': 1028,\n",
       " 'n_adrs_original': 4817,\n",
       " 'n_adrs_kept': 4817,\n",
       " 'min_df_abs': 1,\n",
       " 'max_df_abs': 1028,\n",
       " 'splits': [{'split': 'all',\n",
       "   'n_drugs': 1028,\n",
       "   'n_adrs': 4817,\n",
       "   'nnz': 69474,\n",
       "   'density': 0.014029834349648497}],\n",
       " 'norm': 'l2',\n",
       " 'sublinear_tf': False,\n",
       " 'smooth_idf': True,\n",
       " 'note': 'IDF fit on train only if splits provided; else fit on all.'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = TFIDFOptions()\n",
    "\n",
    "artifacts = build_and_save_tfidf_parquets(\n",
    "    dti_df=dti_df,\n",
    "    adr_df=adr_df,\n",
    "    options=opts,\n",
    "    # No train/val/test lists passed => single \"all\" build\n",
    ")\n",
    "artifacts[\"stats\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff3cbaf",
   "metadata": {},
   "source": [
    "B) Proper split-aware build.\n",
    "Fit IDF on train drugs only, and save split-specific Parquets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8affe09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_drugs_total': 1028,\n",
       " 'n_adrs_original': 4817,\n",
       " 'n_adrs_kept': 4048,\n",
       " 'min_df_abs': 1,\n",
       " 'max_df_abs': 719,\n",
       " 'splits': [{'split': 'train',\n",
       "   'n_drugs': 719,\n",
       "   'n_adrs': 4048,\n",
       "   'nnz': 47047,\n",
       "   'density': 0.01616450988692024},\n",
       "  {'split': 'val',\n",
       "   'n_drugs': 154,\n",
       "   'n_adrs': 4048,\n",
       "   'nnz': 10324,\n",
       "   'density': 0.016561008161798677},\n",
       "  {'split': 'test',\n",
       "   'n_drugs': 155,\n",
       "   'n_adrs': 4048,\n",
       "   'nnz': 11222,\n",
       "   'density': 0.017885375494071147}],\n",
       " 'norm': 'l2',\n",
       " 'sublinear_tf': False,\n",
       " 'smooth_idf': True,\n",
       " 'note': 'IDF fit on train only if splits provided; per-split files saved to subfolders.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opts = TFIDFOptions()\n",
    "\n",
    "# split (replace with deterministic split)\n",
    "from numpy.random import default_rng\n",
    "rng = default_rng(42)\n",
    "all_drugs = dti_df[\"rxcui\"].astype(str).unique()\n",
    "rng.shuffle(all_drugs)\n",
    "n = len(all_drugs)\n",
    "train_drugs = all_drugs[: int(0.70*n)]\n",
    "val_drugs   = all_drugs[int(0.70*n): int(0.85*n)]\n",
    "test_drugs  = all_drugs[int(0.85*n):]\n",
    "\n",
    "artifacts = build_and_save_tfidf_parquets(\n",
    "    dti_df, adr_df, options=opts,\n",
    "    train_rxcui=train_drugs, val_rxcui=val_drugs, test_rxcui=test_drugs,\n",
    "    per_split_subdirs=True,   # <- key line\n",
    ")\n",
    "artifacts[\"stats\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
