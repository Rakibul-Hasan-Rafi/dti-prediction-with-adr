{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4572ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67a56c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d4f4da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DTI Data info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34741 entries, 0 to 34740\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   drug_chembl_id     34741 non-null  object\n",
      " 1   target_uniprot_id  34741 non-null  object\n",
      " 2   label              34741 non-null  int64 \n",
      " 3   smiles             34741 non-null  object\n",
      " 4   sequence           34741 non-null  object\n",
      " 5   molfile_3d         34741 non-null  object\n",
      " 6   rxcui              34741 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.9+ MB\n",
      "None \n",
      "\n",
      "ADR Data info: \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 69474 entries, 0 to 69473\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   rxnorm_ingredient_id  69474 non-null  object\n",
      " 1   meddra_id             69474 non-null  int64 \n",
      " 2   meddra_name           69474 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.6+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dti_pd = pd.read_parquet(config[\"dataPath\"][\"dti_data\"])\n",
    "adr_pd = pd.read_parquet(config[\"dataPath\"][\"adr_data\"])\n",
    "print(\"DTI Data info: \")\n",
    "print(dti_pd.info(),\"\\n\")\n",
    "print(\"ADR Data info: \")\n",
    "print(adr_pd.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f01984e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch_geometric.data import Data, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebf132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create cache directories if they don't exist\n",
    "os.makedirs(config[\"cachePath\"][\"prot_model\"], exist_ok=True)\n",
    "os.makedirs(config[\"cachePath\"][\"drug_model\"], exist_ok=True)\n",
    "\n",
    "# Basic configuration - UPDATED\n",
    "class Config:\n",
    "    def __init__(self, config_dict):\n",
    "        self.data_paths = config_dict[\"dataPath\"]\n",
    "        self.cache_paths = config_dict[\"cachePath\"]\n",
    "        \n",
    "        # Model parameters - WILL BE UPDATED AFTER DATA ANALYSIS\n",
    "        self.protein_feat_dim = None  # Will be set after building first protein graph\n",
    "        self.drug_scalar_dim = None   # Will be set after building first drug graph\n",
    "        self.drug_vector_neighbor_dim = None  # Will be set after building first drug graph  \n",
    "        self.drug_vector_coord_dim = None     # Will be set after building first drug graph\n",
    "        \n",
    "        # Fixed architecture parameters\n",
    "        self.hidden_dim = 128\n",
    "        \n",
    "        # Training parameters\n",
    "        self.batch_size = 32\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "    def update_from_data(self, protein_feat_dim, drug_scalar_dim, drug_vector_neighbor_dim, drug_vector_coord_dim):\n",
    "        \"\"\"Update dimensions after analyzing actual data\"\"\"\n",
    "        self.protein_feat_dim = protein_feat_dim\n",
    "        self.drug_scalar_dim = drug_scalar_dim\n",
    "        self.drug_vector_neighbor_dim = drug_vector_neighbor_dim\n",
    "        self.drug_vector_coord_dim = drug_vector_coord_dim\n",
    "        \n",
    "        print(f\"Configuration updated from data:\")\n",
    "        print(f\"  protein_feat_dim: {self.protein_feat_dim}\")\n",
    "        print(f\"  drug_scalar_dim: {self.drug_scalar_dim}\")\n",
    "        print(f\"  drug_vector_neighbor_dim: {self.drug_vector_neighbor_dim}\")\n",
    "        print(f\"  drug_vector_coord_dim: {self.drug_vector_coord_dim}\")\n",
    "\n",
    "# Initialize config\n",
    "cfg = Config(config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e09e49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinGraphBuilder:\n",
    "    def __init__(self, prot_3d_dir):\n",
    "        self.prot_3d_dir = prot_3d_dir\n",
    "        self.amino_acid_dict = self._create_aa_embedding_dict()\n",
    "    \n",
    "    def _create_aa_embedding_dict(self):\n",
    "        \"\"\"Create amino acid feature dictionary\"\"\"\n",
    "        # Simple one-hot encoding for starters\n",
    "        aa_list = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "        aa_dict = {}\n",
    "        for i, aa in enumerate(aa_list):\n",
    "            # One-hot + some basic properties\n",
    "            features = np.zeros(20)\n",
    "            features[i] = 1.0\n",
    "            aa_dict[aa] = features\n",
    "        return aa_dict\n",
    "    \n",
    "    def get_alphafold_path(self, uniprot_id):\n",
    "        \"\"\"Get AlphaFold PDB file path for a given UniProt ID\"\"\"\n",
    "        pdb_path = os.path.join(self.prot_3d_dir, f\"{uniprot_id}.pdb\")\n",
    "        if os.path.exists(pdb_path):\n",
    "            return pdb_path\n",
    "        else:\n",
    "            print(f\"Warning: AlphaFold file not found for {uniprot_id}\")\n",
    "            return None\n",
    "    \n",
    "    def parse_alphafold_pdb(self, pdb_path):\n",
    "        \"\"\"Parse AlphaFold PDB file to extract coordinates and pLDDT scores\"\"\"\n",
    "        try:\n",
    "            coords = []\n",
    "            plddt_scores = []\n",
    "            \n",
    "            with open(pdb_path, 'r') as f:\n",
    "                for line in f:\n",
    "                    if line.startswith('ATOM') and 'CA' in line:\n",
    "                        # Extract coordinates\n",
    "                        x = float(line[30:38])\n",
    "                        y = float(line[38:46])\n",
    "                        z = float(line[46:54])\n",
    "                        coords.append([x, y, z])\n",
    "                        \n",
    "                        # Extract pLDDT score (B-factor column)\n",
    "                        plddt = float(line[60:66])\n",
    "                        plddt_scores.append(plddt)\n",
    "            \n",
    "            return np.array(coords), np.array(plddt_scores)\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing PDB file {pdb_path}: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    def build_spatial_edges(self, coords, cutoff=8.0):\n",
    "        \"\"\"Build edges based on spatial proximity\"\"\"\n",
    "        n_nodes = len(coords)\n",
    "        edges = []\n",
    "        \n",
    "        for i in range(n_nodes):\n",
    "            for j in range(i + 1, n_nodes):\n",
    "                distance = np.linalg.norm(coords[i] - coords[j])\n",
    "                if distance < cutoff:\n",
    "                    edges.append([i, j])\n",
    "                    edges.append([j, i])  # Undirected graph\n",
    "        \n",
    "        if len(edges) == 0:\n",
    "            # Fallback: connect sequential residues\n",
    "            edges = [[i, i+1] for i in range(n_nodes-1)]\n",
    "            edges += [[i+1, i] for i in range(n_nodes-1)]\n",
    "        \n",
    "        return torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    def build_protein_graph(self, uniprot_id, sequence):\n",
    "        \"\"\"Build protein graph from sequence and AlphaFold data\"\"\"\n",
    "        # Try to get AlphaFold structure\n",
    "        pdb_path = self.get_alphafold_path(uniprot_id)\n",
    "        coords, plddt_scores = None, None\n",
    "        \n",
    "        if pdb_path:\n",
    "            coords, plddt_scores = self.parse_alphafold_pdb(pdb_path)\n",
    "        \n",
    "        # Build node features\n",
    "        node_features = []\n",
    "        for i, aa in enumerate(sequence):\n",
    "            if aa in self.amino_acid_dict:\n",
    "                aa_features = self.amino_acid_dict[aa]\n",
    "            else:\n",
    "                # Unknown amino acid - use average\n",
    "                aa_features = np.zeros(20)\n",
    "            \n",
    "            # Add structural features if available\n",
    "            structural_features = []\n",
    "            if plddt_scores is not None and i < len(plddt_scores):\n",
    "                structural_features.append(plddt_scores[i] / 100.0)  # Normalize pLDDT\n",
    "            else:\n",
    "                structural_features.append(0.5)  # Default confidence\n",
    "            \n",
    "            if coords is not None and i < len(coords):\n",
    "                # Add some basic structural features\n",
    "                if i > 0 and i < len(coords) - 1:\n",
    "                    # Simple pseudo-dihedral (simplified)\n",
    "                    vec1 = coords[i] - coords[i-1]\n",
    "                    vec2 = coords[i+1] - coords[i]\n",
    "                    if np.linalg.norm(vec1) > 0 and np.linalg.norm(vec2) > 0:\n",
    "                        cos_angle = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "                        structural_features.append(cos_angle)\n",
    "                    else:\n",
    "                        structural_features.append(0.0)\n",
    "                else:\n",
    "                    structural_features.append(0.0)\n",
    "            else:\n",
    "                structural_features.append(0.0)\n",
    "            \n",
    "            # Combine all features\n",
    "            features = np.concatenate([aa_features, structural_features])\n",
    "            node_features.append(features)\n",
    "        \n",
    "        node_features = np.array(node_features)\n",
    "        \n",
    "        # Build edges\n",
    "        if coords is not None:\n",
    "            edge_index = self.build_spatial_edges(coords, cutoff=8.0)\n",
    "        else:\n",
    "            # Fallback: sequence-based edges\n",
    "            n_residues = len(sequence)\n",
    "            edges = []\n",
    "            for i in range(n_residues - 1):\n",
    "                edges.append([i, i + 1])\n",
    "                edges.append([i + 1, i])\n",
    "            edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "        \n",
    "        # Update feature dimension in config\n",
    "        cfg.protein_feat_dim = node_features.shape[1]\n",
    "        \n",
    "        return Data(\n",
    "            x=torch.tensor(node_features, dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            uniprot_id=uniprot_id,\n",
    "            sequence=sequence\n",
    "        )\n",
    "\n",
    "# Initialize protein graph builder\n",
    "protein_builder = ProteinGraphBuilder(cfg.data_paths[\"prot_3d_dir\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a63b124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrugGVPBuilder:\n",
    "    def __init__(self):\n",
    "        self.atom_features_dict = self._create_atom_feature_dict()\n",
    "    \n",
    "    def _create_atom_feature_dict(self):\n",
    "        \"\"\"Create atom feature dictionary\"\"\"\n",
    "        # Basic atom types\n",
    "        atom_types = ['C', 'N', 'O', 'S', 'F', 'Cl', 'Br', 'I', 'P']\n",
    "        feature_dict = {}\n",
    "        \n",
    "        for atom in atom_types:\n",
    "            # One-hot for common atoms + zeros for others\n",
    "            features = [1.0 if a == atom else 0.0 for a in atom_types]\n",
    "            feature_dict[atom] = features\n",
    "        \n",
    "        # Default for other atoms\n",
    "        feature_dict['OTHER'] = [0.0] * len(atom_types)\n",
    "        \n",
    "        return feature_dict\n",
    "    \n",
    "    def get_atom_features(self, atom):\n",
    "        \"\"\"Get features for an atom\"\"\"\n",
    "        symbol = atom.GetSymbol()\n",
    "        if symbol in self.atom_features_dict:\n",
    "            base_features = self.atom_features_dict[symbol]\n",
    "        else:\n",
    "            base_features = self.atom_features_dict['OTHER']\n",
    "        \n",
    "        # Additional features\n",
    "        additional_features = [\n",
    "            atom.GetDegree() / 4.0,  # Normalized degree\n",
    "            atom.GetFormalCharge() / 2.0,  # Normalized charge\n",
    "            float(atom.GetIsAromatic()),\n",
    "            atom.GetNumImplicitHs() / 4.0,  # Normalized H count\n",
    "            atom.GetMass() / 100.0  # Normalized mass\n",
    "        ]\n",
    "        \n",
    "        return base_features + additional_features\n",
    "    \n",
    "    def get_neighbor_vectors(self, mol, atom_idx, conformer):\n",
    "        \"\"\"Get direction vectors to neighbors for geometric features\"\"\"\n",
    "        atom = mol.GetAtomWithIdx(atom_idx)\n",
    "        neighbors = atom.GetNeighbors()\n",
    "        \n",
    "        vectors = []\n",
    "        for neighbor in neighbors:\n",
    "            nbr_idx = neighbor.GetIdx()\n",
    "            if nbr_idx != atom_idx:\n",
    "                atom_pos = conformer.GetAtomPosition(atom_idx)\n",
    "                nbr_pos = conformer.GetAtomPosition(nbr_idx)\n",
    "                vec = [nbr_pos.x - atom_pos.x, nbr_pos.y - atom_pos.y, nbr_pos.z - atom_pos.z]\n",
    "                vectors.append(vec)\n",
    "        \n",
    "        # Pad to fixed dimension\n",
    "        max_neighbors = 4\n",
    "        while len(vectors) < max_neighbors:\n",
    "            vectors.append([0.0, 0.0, 0.0])\n",
    "        \n",
    "        return vectors[:max_neighbors]\n",
    "    \n",
    "    def smiles_to_gvp_data(self, smiles, drug_id):\n",
    "        \"\"\"Convert SMILES to GVP-ready data\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                raise ValueError(f\"Invalid SMILES: {smiles}\")\n",
    "            \n",
    "            # Add hydrogens and generate 3D coordinates\n",
    "            mol = Chem.AddHs(mol)\n",
    "            AllChem.EmbedMolecule(mol, AllChem.ETKDG())\n",
    "            \n",
    "            conformer = mol.GetConformers()[0]\n",
    "            \n",
    "            scalar_feats = []\n",
    "            vector_feats = []\n",
    "            positions = []\n",
    "            \n",
    "            for atom in mol.GetAtoms():\n",
    "                idx = atom.GetIdx()\n",
    "                \n",
    "                # Scalar features\n",
    "                scalar_feat = self.get_atom_features(atom)\n",
    "                scalar_feats.append(scalar_feat)\n",
    "                \n",
    "                # Vector features (neighbor directions)\n",
    "                vector_feat = self.get_neighbor_vectors(mol, idx, conformer)\n",
    "                vector_feats.append(vector_feat)\n",
    "                \n",
    "                # Atom positions\n",
    "                pos = conformer.GetAtomPosition(idx)\n",
    "                positions.append([pos.x, pos.y, pos.z])\n",
    "            \n",
    "            scalar_feats = np.array(scalar_feats)\n",
    "            vector_feats = np.array(vector_feats)\n",
    "            positions = np.array(positions)\n",
    "            \n",
    "            # Update drug feature dimensions in config\n",
    "            cfg.drug_scalar_dim = scalar_feats.shape[1]\n",
    "            cfg.drug_vector_dim = vector_feats.shape[2]  # Should be 3\n",
    "            \n",
    "            return {\n",
    "                'scalar_feats': torch.tensor(scalar_feats, dtype=torch.float),\n",
    "                'vector_feats': torch.tensor(vector_feats, dtype=torch.float),\n",
    "                'positions': torch.tensor(positions, dtype=torch.float),\n",
    "                'drug_id': drug_id,\n",
    "                'smiles': smiles\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing drug {drug_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "# Initialize drug graph builder\n",
    "drug_builder = DrugGVPBuilder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30936b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing protein graph builder...\n",
      "Protein graph: Data(x=[554, 22], edge_index=[2, 5418], uniprot_id='O15245', sequence='MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTPDHHCQSPGVAELSQRCGWSPAEELNYTVPGLGPAGEAFLGQCRRYEVDWNQSALSCVDPLASLATNRSHLPLGPCQDGWVYDTPGSSIVTEFNLVCADSWKLDLFQSCLNAGFLFGSLGVGYFADRFGRKLCLLGTVLVNAVSGVLMAFSPNYMSMLLFRLLQGLVSKGNWMAGYTLITEFVGSGSRRTVAIMYQMAFTVGLVALTGLAYALPHWRWLQLAVSLPTFLFLLYYWCVPESPRWLLSQKRNTEAIKIMDHIAQKNGKLPPADLKMLSLEEDVTEKLSPSFADLFRTPRLRKRTFILMYLWFTDSVLYQGLILHMGATSGNLYLDFLYSALVEIPGAFIALITIDRVGRIYPMAMSNLLAGAACLVMIFISPDLHWLNIIIMCVGRMGITIAIQMICLVNAELYPTFVRNLGVMVCSSLCDIGGIITPFIVFRLREVWQALPLILFAVLGLLAAGVTLLLPETKGVALPETMKDAENLGRKAKPKENTIYLKVQTSEPSGT')\n",
      "Protein node features shape: torch.Size([554, 22])\n",
      "Protein edges: torch.Size([2, 5418])\n",
      "\n",
      "Testing drug graph builder...\n",
      "Drug scalar features shape: torch.Size([52, 14])\n",
      "Drug vector features shape: torch.Size([52, 4, 3])\n",
      "\n",
      "Final feature dimensions:\n",
      "Protein feature dim: 22\n",
      "Drug scalar dim: 14\n",
      "Drug vector dim: 3\n"
     ]
    }
   ],
   "source": [
    "# Test with a small sample\n",
    "sample_protein = dti_pd.iloc[0]\n",
    "sample_drug = dti_pd.iloc[0]\n",
    "\n",
    "print(\"Testing protein graph builder...\")\n",
    "protein_graph = protein_builder.build_protein_graph(\n",
    "    sample_protein['target_uniprot_id'], \n",
    "    sample_protein['sequence']\n",
    ")\n",
    "print(f\"Protein graph: {protein_graph}\")\n",
    "print(f\"Protein node features shape: {protein_graph.x.shape}\")\n",
    "print(f\"Protein edges: {protein_graph.edge_index.shape}\")\n",
    "\n",
    "print(\"\\nTesting drug graph builder...\")\n",
    "drug_data = drug_builder.smiles_to_gvp_data(\n",
    "    sample_drug['smiles'],\n",
    "    sample_drug['drug_chembl_id']\n",
    ")\n",
    "if drug_data:\n",
    "    print(f\"Drug scalar features shape: {drug_data['scalar_feats'].shape}\")\n",
    "    print(f\"Drug vector features shape: {drug_data['vector_feats'].shape}\")\n",
    "\n",
    "print(f\"\\nFinal feature dimensions:\")\n",
    "print(f\"Protein feature dim: {cfg.protein_feat_dim}\")\n",
    "print(f\"Drug scalar dim: {cfg.drug_scalar_dim}\")\n",
    "print(f\"Drug vector dim: {cfg.drug_vector_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1b2cf0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Creating threaded cached dataset...\n",
      "üß¨ Threaded Cached Dataset Initialized:\n",
      "   üìä Total samples: 34,741\n",
      "   üß™ Unique proteins: 2,385\n",
      "   üíä Unique drugs: 1,028\n",
      "   üßµ Max workers: 2\n",
      "   üìÅ Protein cache: ./Cache\\proteins\n",
      "   üìÅ Drug cache: ./Cache\\drugs\n",
      "   ü©∫ Number of side effects: 4,817\n",
      "üî® Precomputing missing graphs with threading...\n",
      "üìã Found 2,385 unique proteins and 1,028 unique drugs\n",
      "üíä Processing 1 drugs with 2 workers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üíä Building drugs: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.05it/s, success=1, current=CHEMBL2110...]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Precomputation complete!\n",
      "üíæ Cache sizes:\n",
      "   üß™ Protein cache: 310.5 MB\n",
      "   üíä Drug cache: 7.0 MB\n",
      "   üíΩ Total cache: 317.6 MB\n",
      "\n",
      "üß™ Testing threaded cached dataset...\n",
      "‚úÖ Sample loaded successfully!\n",
      "\n",
      "üìä Cache Statistics:\n",
      "   üß™ Protein cache: 1 hits, 0 misses (100.00%)\n",
      "   üíä Drug cache: 1 hits, 1 misses (50.00%)\n",
      "   üìà Total requests: 3\n",
      "\n",
      "üéâ Threaded cached dataset ready for training!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# CACHED DTI DATASET WITH PROGRESS BARS FOR GRAPH BUILDING\n",
    "# =============================================================================\n",
    "import pickle\n",
    "import hashlib\n",
    "import concurrent.futures\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ThreadedCachedDTIDataset(Dataset):\n",
    "    def __init__(self, dti_df, protein_builder, drug_builder, adr_df=None, \n",
    "                 cache_dir=\"./Cache\", force_rebuild=False, show_progress=True,\n",
    "                 max_workers=None):\n",
    "        self.dti_df = dti_df.reset_index(drop=True)\n",
    "        self.protein_builder = protein_builder\n",
    "        self.drug_builder = drug_builder\n",
    "        self.adr_df = adr_df\n",
    "        self.cache_dir = cache_dir\n",
    "        self.force_rebuild = force_rebuild\n",
    "        self.show_progress = show_progress\n",
    "        self.max_workers = max_workers or min(32, (os.cpu_count() or 1) + 4)\n",
    "        \n",
    "        # Create cache directories\n",
    "        self.protein_cache_dir = os.path.join(cache_dir, \"proteins\")\n",
    "        self.drug_cache_dir = os.path.join(cache_dir, \"drugs\")\n",
    "        os.makedirs(self.protein_cache_dir, exist_ok=True)\n",
    "        os.makedirs(self.drug_cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Thread-safe data structures\n",
    "        self.cache_stats = {\n",
    "            'protein_hits': 0,\n",
    "            'protein_misses': 0,\n",
    "            'drug_hits': 0,\n",
    "            'drug_misses': 0\n",
    "        }\n",
    "        self._stats_lock = threading.Lock()\n",
    "        \n",
    "        # Build side effect mapping\n",
    "        self.side_effect_mapping = self._build_side_effect_mapping()\n",
    "        \n",
    "        print(f\"üß¨ Threaded Cached Dataset Initialized:\")\n",
    "        print(f\"   üìä Total samples: {len(self.dti_df):,}\")\n",
    "        print(f\"   üß™ Unique proteins: {self.dti_df['target_uniprot_id'].nunique():,}\")\n",
    "        print(f\"   üíä Unique drugs: {self.dti_df['drug_chembl_id'].nunique():,}\")\n",
    "        print(f\"   üßµ Max workers: {self.max_workers}\")\n",
    "        print(f\"   üìÅ Protein cache: {self.protein_cache_dir}\")\n",
    "        print(f\"   üìÅ Drug cache: {self.drug_cache_dir}\")\n",
    "        print(f\"   ü©∫ Number of side effects: {self.side_effect_mapping['num_side_effects']:,}\")\n",
    "        \n",
    "        # Precompute all missing graphs with threading\n",
    "        self.precompute_all_graphs()\n",
    "    \n",
    "    def _build_side_effect_mapping(self):\n",
    "        \"\"\"Build mapping from drug to side effects\"\"\"\n",
    "        if self.adr_df is None:\n",
    "            return {'drug_to_se': {}, 'se_id_to_idx': {}, 'num_side_effects': 0}\n",
    "        \n",
    "        side_effect_dict = {}\n",
    "        for _, row in self.adr_df.iterrows():\n",
    "            drug_id = str(row['rxnorm_ingredient_id']).strip()\n",
    "            se_id = row['meddra_id']\n",
    "            \n",
    "            if drug_id not in side_effect_dict:\n",
    "                side_effect_dict[drug_id] = set()\n",
    "            side_effect_dict[drug_id].add(se_id)\n",
    "        \n",
    "        all_se_ids = sorted(set(self.adr_df['meddra_id']))\n",
    "        se_id_to_idx = {se_id: idx for idx, se_id in enumerate(all_se_ids)}\n",
    "        \n",
    "        return {\n",
    "            'drug_to_se': side_effect_dict,\n",
    "            'se_id_to_idx': se_id_to_idx,\n",
    "            'num_side_effects': len(all_se_ids)\n",
    "        }\n",
    "    \n",
    "    def _get_protein_cache_path(self, uniprot_id, sequence):\n",
    "        \"\"\"Generate cache file path for protein\"\"\"\n",
    "        content = f\"{uniprot_id}_{sequence}\"\n",
    "        hash_obj = hashlib.md5(content.encode())\n",
    "        return os.path.join(self.protein_cache_dir, f\"{hash_obj.hexdigest()}.pkl\")\n",
    "    \n",
    "    def _get_drug_cache_path(self, drug_id, smiles):\n",
    "        \"\"\"Generate cache file path for drug\"\"\"\n",
    "        content = f\"{drug_id}_{smiles}\"\n",
    "        hash_obj = hashlib.md5(content.encode())\n",
    "        return os.path.join(self.drug_cache_dir, f\"{hash_obj.hexdigest()}.pkl\")\n",
    "    \n",
    "    def _process_protein(self, uniprot_id, sequence):\n",
    "        \"\"\"Process a single protein (thread-safe)\"\"\"\n",
    "        cache_path = self._get_protein_cache_path(uniprot_id, sequence)\n",
    "        \n",
    "        # Try to load from cache\n",
    "        if not self.force_rebuild and os.path.exists(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    with self._stats_lock:\n",
    "                        self.cache_stats['protein_hits'] += 1\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                if self.show_progress:\n",
    "                    print(f\"Error loading cached protein {uniprot_id}: {e}\")\n",
    "        \n",
    "        # Build and cache\n",
    "        with self._stats_lock:\n",
    "            self.cache_stats['protein_misses'] += 1\n",
    "        \n",
    "        protein_graph = self.protein_builder.build_protein_graph(uniprot_id, sequence)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_path, 'wb') as f:\n",
    "                pickle.dump(protein_graph, f)\n",
    "        except Exception as e:\n",
    "            if self.show_progress:\n",
    "                print(f\"Error caching protein {uniprot_id}: {e}\")\n",
    "        \n",
    "        return protein_graph\n",
    "    \n",
    "    def _process_drug(self, drug_id, smiles):\n",
    "        \"\"\"Process a single drug (thread-safe)\"\"\"\n",
    "        cache_path = self._get_drug_cache_path(drug_id, smiles)\n",
    "        \n",
    "        # Try to load from cache\n",
    "        if not self.force_rebuild and os.path.exists(cache_path):\n",
    "            try:\n",
    "                with open(cache_path, 'rb') as f:\n",
    "                    with self._stats_lock:\n",
    "                        self.cache_stats['drug_hits'] += 1\n",
    "                    return pickle.load(f)\n",
    "            except Exception as e:\n",
    "                if self.show_progress:\n",
    "                    print(f\"Error loading cached drug {drug_id}: {e}\")\n",
    "        \n",
    "        # Build and cache\n",
    "        with self._stats_lock:\n",
    "            self.cache_stats['drug_misses'] += 1\n",
    "        \n",
    "        drug_data = self.drug_builder.smiles_to_gvp_data(smiles, drug_id)\n",
    "        \n",
    "        if drug_data is not None:\n",
    "            try:\n",
    "                with open(cache_path, 'wb') as f:\n",
    "                    pickle.dump(drug_data, f)\n",
    "            except Exception as e:\n",
    "                if self.show_progress:\n",
    "                    print(f\"Error caching drug {drug_id}: {e}\")\n",
    "        \n",
    "        return drug_data\n",
    "    \n",
    "    def precompute_all_graphs(self):\n",
    "        \"\"\"Precompute all missing graphs with threading\"\"\"\n",
    "        print(\"üî® Precomputing missing graphs with threading...\")\n",
    "        \n",
    "        # Get unique proteins and drugs\n",
    "        unique_proteins = self.dti_df[['target_uniprot_id', 'sequence']].drop_duplicates()\n",
    "        unique_drugs = self.dti_df[['drug_chembl_id', 'smiles']].drop_duplicates()\n",
    "        \n",
    "        print(f\"üìã Found {len(unique_proteins):,} unique proteins and {len(unique_drugs):,} unique drugs\")\n",
    "        \n",
    "        # Process proteins with threading\n",
    "        protein_tasks = []\n",
    "        for _, row in unique_proteins.iterrows():\n",
    "            uniprot_id, sequence = row['target_uniprot_id'], row['sequence']\n",
    "            cache_path = self._get_protein_cache_path(uniprot_id, sequence)\n",
    "            if self.force_rebuild or not os.path.exists(cache_path):\n",
    "                protein_tasks.append((uniprot_id, sequence))\n",
    "        \n",
    "        if protein_tasks:\n",
    "            print(f\"üß™ Processing {len(protein_tasks):,} proteins with {self.max_workers} workers...\")\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all protein tasks\n",
    "                future_to_protein = {\n",
    "                    executor.submit(self._process_protein, uniprot_id, sequence): (uniprot_id, sequence)\n",
    "                    for uniprot_id, sequence in protein_tasks\n",
    "                }\n",
    "                \n",
    "                # Track progress with tqdm\n",
    "                if self.show_progress:\n",
    "                    with tqdm(total=len(future_to_protein), desc=\"üß™ Building proteins\") as pbar:\n",
    "                        for future in concurrent.futures.as_completed(future_to_protein):\n",
    "                            uniprot_id, sequence = future_to_protein[future]\n",
    "                            try:\n",
    "                                future.result()\n",
    "                                pbar.update(1)\n",
    "                                pbar.set_postfix({'current': uniprot_id[:10] + \"...\"})\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ùå Error processing protein {uniprot_id}: {e}\")\n",
    "                                pbar.update(1)\n",
    "                else:\n",
    "                    # Without progress bar, just wait for completion\n",
    "                    for future in concurrent.futures.as_completed(future_to_protein):\n",
    "                        uniprot_id, sequence = future_to_protein[future]\n",
    "                        try:\n",
    "                            future.result()\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Error processing protein {uniprot_id}: {e}\")\n",
    "        \n",
    "        # Process drugs with threading (this is where threading helps most!)\n",
    "        drug_tasks = []\n",
    "        for _, row in unique_drugs.iterrows():\n",
    "            drug_id, smiles = row['drug_chembl_id'], row['smiles']\n",
    "            cache_path = self._get_drug_cache_path(drug_id, smiles)\n",
    "            if self.force_rebuild or not os.path.exists(cache_path):\n",
    "                drug_tasks.append((drug_id, smiles))\n",
    "        \n",
    "        if drug_tasks:\n",
    "            print(f\"üíä Processing {len(drug_tasks):,} drugs with {self.max_workers} workers...\")\n",
    "            successful_drugs = 0\n",
    "            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "                # Submit all drug tasks\n",
    "                future_to_drug = {\n",
    "                    executor.submit(self._process_drug, drug_id, smiles): (drug_id, smiles)\n",
    "                    for drug_id, smiles in drug_tasks\n",
    "                }\n",
    "                \n",
    "                # Track progress with tqdm\n",
    "                if self.show_progress:\n",
    "                    with tqdm(total=len(future_to_drug), desc=\"üíä Building drugs\") as pbar:\n",
    "                        for future in concurrent.futures.as_completed(future_to_drug):\n",
    "                            drug_id, smiles = future_to_drug[future]\n",
    "                            try:\n",
    "                                result = future.result()\n",
    "                                if result is not None:\n",
    "                                    successful_drugs += 1\n",
    "                                pbar.update(1)\n",
    "                                pbar.set_postfix({\n",
    "                                    'success': successful_drugs,\n",
    "                                    'current': drug_id[:10] + \"...\"\n",
    "                                })\n",
    "                            except Exception as e:\n",
    "                                print(f\"‚ùå Error processing drug {drug_id}: {e}\")\n",
    "                                pbar.update(1)\n",
    "                else:\n",
    "                    # Without progress bar\n",
    "                    for future in concurrent.futures.as_completed(future_to_drug):\n",
    "                        drug_id, smiles = future_to_drug[future]\n",
    "                        try:\n",
    "                            result = future.result()\n",
    "                            if result is not None:\n",
    "                                successful_drugs += 1\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Error processing drug {drug_id}: {e}\")\n",
    "        \n",
    "        print(f\"‚úÖ Precomputation complete!\")\n",
    "        self._show_cache_sizes()\n",
    "    \n",
    "    def _show_cache_sizes(self):\n",
    "        \"\"\"Show the size of cache directories\"\"\"\n",
    "        def get_dir_size(path):\n",
    "            total = 0\n",
    "            for dirpath, dirnames, filenames in os.walk(path):\n",
    "                for f in filenames:\n",
    "                    fp = os.path.join(dirpath, f)\n",
    "                    total += os.path.getsize(fp)\n",
    "            return total\n",
    "        \n",
    "        protein_size = get_dir_size(self.protein_cache_dir) / (1024 * 1024)  # MB\n",
    "        drug_size = get_dir_size(self.drug_cache_dir) / (1024 * 1024)  # MB\n",
    "        \n",
    "        print(f\"üíæ Cache sizes:\")\n",
    "        print(f\"   üß™ Protein cache: {protein_size:.1f} MB\")\n",
    "        print(f\"   üíä Drug cache: {drug_size:.1f} MB\")\n",
    "        print(f\"   üíΩ Total cache: {protein_size + drug_size:.1f} MB\")\n",
    "    \n",
    "    def get_protein_graph(self, uniprot_id, sequence):\n",
    "        \"\"\"Get protein graph (single-threaded for training)\"\"\"\n",
    "        return self._process_protein(uniprot_id, sequence)\n",
    "    \n",
    "    def get_drug_data(self, drug_id, smiles):\n",
    "        \"\"\"Get drug data (single-threaded for training)\"\"\"\n",
    "        return self._process_drug(drug_id, smiles)\n",
    "    \n",
    "    def get_side_effects(self, drug_id):\n",
    "        \"\"\"Get side effect vector for a drug\"\"\"\n",
    "        if not self.side_effect_mapping or self.side_effect_mapping['num_side_effects'] == 0:\n",
    "            return torch.tensor([], dtype=torch.float)\n",
    "        \n",
    "        se_dict = self.side_effect_mapping['drug_to_se']\n",
    "        se_id_to_idx = self.side_effect_mapping['se_id_to_idx']\n",
    "        num_se = self.side_effect_mapping['num_side_effects']\n",
    "        \n",
    "        se_vector = torch.zeros(num_se, dtype=torch.float)\n",
    "        \n",
    "        drug_id_str = str(drug_id).strip()\n",
    "        if drug_id_str in se_dict:\n",
    "            for se_id in se_dict[drug_id_str]:\n",
    "                if se_id in se_id_to_idx:\n",
    "                    se_vector[se_id_to_idx[se_id]] = 1.0\n",
    "        \n",
    "        return se_vector\n",
    "    \n",
    "    def get_cache_stats(self):\n",
    "        \"\"\"Get cache hit/miss statistics\"\"\"\n",
    "        with self._stats_lock:\n",
    "            total_protein = self.cache_stats['protein_hits'] + self.cache_stats['protein_misses']\n",
    "            total_drug = self.cache_stats['drug_hits'] + self.cache_stats['drug_misses']\n",
    "            \n",
    "            stats = {\n",
    "                'protein_hit_rate': self.cache_stats['protein_hits'] / total_protein if total_protein > 0 else 0,\n",
    "                'drug_hit_rate': self.cache_stats['drug_hits'] / total_drug if total_drug > 0 else 0,\n",
    "                'total_requests': total_protein + total_drug,\n",
    "                'protein_hits': self.cache_stats['protein_hits'],\n",
    "                'protein_misses': self.cache_stats['protein_misses'],\n",
    "                'drug_hits': self.cache_stats['drug_hits'],\n",
    "                'drug_misses': self.cache_stats['drug_misses']\n",
    "            }\n",
    "        return stats\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dti_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Fast data loading using persistent cache\"\"\"\n",
    "        row = self.dti_df.iloc[idx]\n",
    "        \n",
    "        # These are now fast cache lookups\n",
    "        protein_graph = self.get_protein_graph(row['target_uniprot_id'], row['sequence'])\n",
    "        drug_data = self.get_drug_data(row['drug_chembl_id'], row['smiles'])\n",
    "        \n",
    "        if drug_data is None:\n",
    "            # Skip invalid drugs\n",
    "            return self.__getitem__((idx + 1) % len(self))\n",
    "        \n",
    "        binding_label = torch.tensor(row['label'], dtype=torch.float)\n",
    "        side_effects = self.get_side_effects(row['rxcui'])\n",
    "        \n",
    "        return {\n",
    "            'protein_data': protein_graph,\n",
    "            'drug_data': drug_data,\n",
    "            'binding_label': binding_label,\n",
    "            'side_effects': side_effects,\n",
    "            'drug_id': row['drug_chembl_id'],\n",
    "            'uniprot_id': row['target_uniprot_id'],\n",
    "            'idx': idx\n",
    "        }\n",
    "\n",
    "# =============================================================================\n",
    "# CREATE THE THREADED CACHED DATASET\n",
    "# =============================================================================\n",
    "\n",
    "print(\"üöÄ Creating threaded cached dataset...\")\n",
    "dataset = ThreadedCachedDTIDataset(\n",
    "    dti_df=dti_pd,\n",
    "    protein_builder=protein_builder,\n",
    "    drug_builder=drug_builder,\n",
    "    adr_df=adr_pd,\n",
    "    cache_dir=\"./Cache\",\n",
    "    force_rebuild=False,\n",
    "    show_progress=True,\n",
    "    max_workers=2  # Adjust based on your CPU. 4-8 is usually good.\n",
    ")\n",
    "\n",
    "# Test the dataset\n",
    "print(\"\\nüß™ Testing threaded cached dataset...\")\n",
    "sample = dataset[0]\n",
    "print(f\"‚úÖ Sample loaded successfully!\")\n",
    "\n",
    "# Show detailed cache statistics\n",
    "cache_stats = dataset.get_cache_stats()\n",
    "print(f\"\\nüìä Cache Statistics:\")\n",
    "print(f\"   üß™ Protein cache: {cache_stats['protein_hits']} hits, {cache_stats['protein_misses']} misses ({cache_stats['protein_hit_rate']:.2%})\")\n",
    "print(f\"   üíä Drug cache: {cache_stats['drug_hits']} hits, {cache_stats['drug_misses']} misses ({cache_stats['drug_hit_rate']:.2%})\")\n",
    "print(f\"   üìà Total requests: {cache_stats['total_requests']}\")\n",
    "\n",
    "print(\"\\nüéâ Threaded cached dataset ready for training!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6181bdb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing fixed dataset...\n",
      "Sample keys: dict_keys(['protein_data', 'drug_data', 'binding_label', 'side_effects', 'drug_id', 'uniprot_id', 'idx'])\n",
      "Binding label: 1.0\n",
      "Side effects shape: torch.Size([4817])\n",
      "Number of side effects in mapping: 4817\n",
      "Actual side effects sum: 77.0\n",
      "Side effects vector: tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "# Test again\n",
    "print(\"\\nTesting fixed dataset...\")\n",
    "sample = dataset[3400]\n",
    "print(f\"Sample keys: {sample.keys()}\")\n",
    "print(f\"Binding label: {sample['binding_label']}\")\n",
    "print(f\"Side effects shape: {sample['side_effects'].shape}\")\n",
    "print(f\"Number of side effects in mapping: {dataset.side_effect_mapping['num_side_effects']}\")\n",
    "print(f\"Actual side effects sum: {sample['side_effects'].sum().item()}\")\n",
    "print(f\"Side effects vector: {sample['side_effects'][:10]}\")  # Show first 10 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2bd26d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìê Feature dimensions from data:\n",
      "   üß™ Protein feature dim: 22\n",
      "   üíä Drug scalar dim: 14\n",
      "   üíä Drug vector neighbor dim: 4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "protein_feat_dim = sample['protein_data'].x.shape[1]  # Should be 22\n",
    "drug_scalar_dim = sample['drug_data']['scalar_feats'].shape[1]  # Should be 14\n",
    "drug_vector_neighbor_dim = sample['drug_data']['vector_feats'].shape[1]  # Should be 4\n",
    "\n",
    "print(f\"üìê Feature dimensions from data:\")\n",
    "print(f\"   üß™ Protein feature dim: {protein_feat_dim}\")\n",
    "print(f\"   üíä Drug scalar dim: {drug_scalar_dim}\")\n",
    "print(f\"   üíä Drug vector neighbor dim: {drug_vector_neighbor_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a81fe20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: SIMPLIFIED MODEL THAT USES PRE-BUILT GRAPHS DIRECTLY\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleDTIModel(nn.Module):\n",
    "    def __init__(self, protein_feat_dim, drug_scalar_dim, drug_vector_neighbor_dim, \n",
    "                 hidden_dim=128, num_side_effects=4817):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Simple encoders (no heavy GVP/GCN - just process the features we already have)\n",
    "        self.protein_encoder = nn.Sequential(\n",
    "            nn.Linear(protein_feat_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Drug encoder uses the precomputed features directly\n",
    "        total_drug_features = drug_scalar_dim + (drug_vector_neighbor_dim * 3)  # scalar + flattened vector\n",
    "        self.drug_encoder = nn.Sequential(\n",
    "            nn.Linear(total_drug_features, hidden_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.num_side_effects = num_side_effects\n",
    "        \n",
    "        # Interaction and prediction heads (same as before)\n",
    "        self.interaction_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.side_effect_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, self.num_side_effects),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, protein_data, drug_data):\n",
    "        # Simple protein encoding: just use mean of node features\n",
    "        protein_embedding = self.protein_encoder(protein_data.x)\n",
    "        protein_embedding = global_mean_pool(protein_embedding, protein_data.batch)\n",
    "        \n",
    "        # Simple drug encoding: flatten scalar + vector features\n",
    "        drug_scalar = drug_data['scalar_feats']\n",
    "        drug_vector_flat = drug_data['vector_feats'].reshape(drug_data['vector_feats'].size(0), -1)\n",
    "        drug_combined = torch.cat([drug_scalar, drug_vector_flat], dim=1)\n",
    "        drug_embedding = self.drug_encoder(drug_combined)\n",
    "        drug_embedding = global_mean_pool(drug_embedding, drug_data['batch'])\n",
    "        \n",
    "        # Interaction and prediction (same as before)\n",
    "        interaction_features = torch.cat([\n",
    "            protein_embedding, drug_embedding, protein_embedding * drug_embedding\n",
    "        ], dim=1)\n",
    "        \n",
    "        combined_rep = self.interaction_net(interaction_features)\n",
    "        binding_score = self.binding_head(combined_rep).squeeze()\n",
    "        side_effects = self.side_effect_head(combined_rep)\n",
    "        \n",
    "        return binding_score, side_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc2539c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Dataset splits:\n",
      "   üöÇ Training: 27,792 samples\n",
      "   üìã Validation: 3,474 samples\n",
      "   üß™ Test: 3,475 samples\n",
      "\n",
      "üß™ Testing data loader...\n",
      "‚úÖ Batch 0 loaded successfully!\n",
      "   üß™ Protein batch: DataBatch(x=[22679, 22], edge_index=[2, 195932], uniprot_id=[32], sequence=[32], batch=[22679], ptr=[33])\n",
      "   üíä Drug scalar: torch.Size([1734, 14])\n",
      "   üíä Drug vector: torch.Size([1734, 4, 3])\n",
      "   üéØ Binding labels: torch.Size([32])\n",
      "   ü©∫ Side effects: torch.Size([32, 4817])\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for our heterogeneous data\"\"\"\n",
    "    protein_data_list = []\n",
    "    drug_data_list = []\n",
    "    binding_labels = []\n",
    "    side_effects_list = []\n",
    "    \n",
    "    protein_batch = []\n",
    "    drug_batch = []\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        # Protein data\n",
    "        protein_data = sample['protein_data']\n",
    "        protein_data.batch = torch.full((protein_data.x.size(0),), i, dtype=torch.long)\n",
    "        protein_data_list.append(protein_data)\n",
    "        \n",
    "        # Drug data  \n",
    "        drug_data = sample['drug_data'].copy()\n",
    "        drug_data['batch'] = torch.full((drug_data['scalar_feats'].size(0),), i, dtype=torch.long)\n",
    "        drug_data_list.append(drug_data)\n",
    "        \n",
    "        # Labels\n",
    "        binding_labels.append(sample['binding_label'])\n",
    "        side_effects_list.append(sample['side_effects'])\n",
    "    \n",
    "    # Batch protein data\n",
    "    from torch_geometric.data import Batch as PyGBatch\n",
    "    batched_protein = PyGBatch.from_data_list(protein_data_list)\n",
    "    \n",
    "    # Batch drug data manually\n",
    "    batched_drug = {\n",
    "        'scalar_feats': torch.cat([d['scalar_feats'] for d in drug_data_list], dim=0),\n",
    "        'vector_feats': torch.cat([d['vector_feats'] for d in drug_data_list], dim=0),\n",
    "        'batch': torch.cat([d['batch'] for d in drug_data_list], dim=0)\n",
    "    }\n",
    "    \n",
    "    batched_binding = torch.stack(binding_labels)\n",
    "    batched_side_effects = torch.stack(side_effects_list)\n",
    "    \n",
    "    return batched_protein, batched_drug, batched_binding, batched_side_effects\n",
    "\n",
    "# Split dataset\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"üìä Dataset splits:\")\n",
    "print(f\"   üöÇ Training: {len(train_dataset):,} samples\")\n",
    "print(f\"   üìã Validation: {len(val_dataset):,} samples\") \n",
    "print(f\"   üß™ Test: {len(test_dataset):,} samples\")\n",
    "\n",
    "# Create data loaders\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=32, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Test the data loader\n",
    "print(\"\\nüß™ Testing data loader...\")\n",
    "for batch_idx, (protein_batch, drug_batch, binding_batch, se_batch) in enumerate(train_loader):\n",
    "    print(f\"‚úÖ Batch {batch_idx} loaded successfully!\")\n",
    "    print(f\"   üß™ Protein batch: {protein_batch}\")\n",
    "    print(f\"   üíä Drug scalar: {drug_batch['scalar_feats'].shape}\")\n",
    "    print(f\"   üíä Drug vector: {drug_batch['vector_feats'].shape}\")\n",
    "    print(f\"   üéØ Binding labels: {binding_batch.shape}\")\n",
    "    print(f\"   ü©∫ Side effects: {se_batch.shape}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0ba5736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Simplified model created successfully!\n",
      "üìä Model parameters: 2,683,346\n",
      "\n",
      "üß™ Testing model forward pass...\n",
      "‚úÖ Model forward pass successful!\n",
      "   üéØ Binding prediction: torch.Size([32])\n",
      "   ü©∫ Side effect prediction: torch.Size([32, 4817])\n"
     ]
    }
   ],
   "source": [
    "# Initialize the simplified model\n",
    "model = SimpleDTIModel(\n",
    "    protein_feat_dim=protein_feat_dim,\n",
    "    drug_scalar_dim=drug_scalar_dim,\n",
    "    drug_vector_neighbor_dim=drug_vector_neighbor_dim,\n",
    "    hidden_dim=128,\n",
    "    num_side_effects=dataset.side_effect_mapping['num_side_effects']\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Simplified model created successfully!\")\n",
    "print(f\"üìä Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "# Test model with a batch\n",
    "print(\"\\nüß™ Testing model forward pass...\")\n",
    "with torch.no_grad():\n",
    "    binding_pred, side_effect_pred = model(protein_batch, drug_batch)\n",
    "\n",
    "print(f\"‚úÖ Model forward pass successful!\")\n",
    "print(f\"   üéØ Binding prediction: {binding_pred.shape}\")\n",
    "print(f\"   ü©∫ Side effect prediction: {side_effect_pred.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0e2eb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: LOSS FUNCTIONS AND METRICS\n",
    "# =============================================================================\n",
    "\n",
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, binding_weight=0.7, side_effect_weight=0.3):\n",
    "        super().__init__()\n",
    "        self.binding_weight = binding_weight\n",
    "        self.side_effect_weight = side_effect_weight\n",
    "        self.binding_loss = nn.BCELoss()\n",
    "        self.side_effect_loss = nn.BCELoss()\n",
    "    \n",
    "    def forward(self, binding_pred, side_effect_pred, binding_true, side_effect_true):\n",
    "        binding_loss = self.binding_loss(binding_pred, binding_true)\n",
    "        \n",
    "        # Only compute side effect loss if we have side effect labels\n",
    "        if side_effect_pred.numel() > 0 and side_effect_true.numel() > 0:\n",
    "            side_effect_loss = self.side_effect_loss(side_effect_pred, side_effect_true)\n",
    "        else:\n",
    "            side_effect_loss = torch.tensor(0.0, device=binding_pred.device)\n",
    "        \n",
    "        total_loss = (self.binding_weight * binding_loss + \n",
    "                     self.side_effect_weight * side_effect_loss)\n",
    "        \n",
    "        return total_loss, binding_loss, side_effect_loss\n",
    "\n",
    "def calculate_metrics(binding_pred, binding_true, side_effect_pred=None, side_effect_true=None):\n",
    "    \"\"\"Calculate evaluation metrics\"\"\"\n",
    "    import numpy as np\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "    \n",
    "    binding_pred_np = binding_pred.detach().cpu().numpy()\n",
    "    binding_true_np = binding_true.detach().cpu().numpy()\n",
    "    \n",
    "    metrics = {}\n",
    "    \n",
    "    # Binding metrics\n",
    "    try:\n",
    "        metrics['binding_auc'] = roc_auc_score(binding_true_np, binding_pred_np)\n",
    "        metrics['binding_ap'] = average_precision_score(binding_true_np, binding_pred_np)\n",
    "        metrics['binding_f1'] = f1_score(binding_true_np, binding_pred_np > 0.5)\n",
    "    except Exception as e:\n",
    "        metrics['binding_auc'] = 0.0\n",
    "        metrics['binding_ap'] = 0.0\n",
    "        metrics['binding_f1'] = 0.0\n",
    "    \n",
    "    # Side effect metrics (if available)\n",
    "    if side_effect_pred is not None and side_effect_true is not None:\n",
    "        se_pred_np = side_effect_pred.detach().cpu().numpy()\n",
    "        se_true_np = side_effect_true.detach().cpu().numpy()\n",
    "        \n",
    "        try:\n",
    "            # Micro-averaged metrics for multi-label classification\n",
    "            metrics['side_effect_auc'] = roc_auc_score(se_true_np.ravel(), se_pred_np.ravel())\n",
    "            metrics['side_effect_ap'] = average_precision_score(se_true_np.ravel(), se_pred_np.ravel())\n",
    "            metrics['side_effect_f1'] = f1_score(se_true_np.ravel(), se_pred_np.ravel() > 0.5)\n",
    "        except:\n",
    "            metrics['side_effect_auc'] = 0.0\n",
    "            metrics['side_effect_ap'] = 0.0\n",
    "            metrics['side_effect_f1'] = 0.0\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0a595d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: TRAINING LOOP\n",
    "# =============================================================================\n",
    "\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            'train_loss': [], 'val_loss': [],\n",
    "            'train_binding_auc': [], 'val_binding_auc': [],\n",
    "            'train_side_effect_auc': [], 'val_side_effect_auc': []\n",
    "        }\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        all_binding_preds = []\n",
    "        all_binding_targets = []\n",
    "        all_side_effect_preds = []\n",
    "        all_side_effect_targets = []\n",
    "        \n",
    "        pbar = tqdm(self.train_loader, desc=\"üöÇ Training\")\n",
    "        for batch_idx, (protein_data, drug_data, binding_labels, se_labels) in enumerate(pbar):\n",
    "            # Move data to device\n",
    "            protein_data = protein_data.to(self.device)\n",
    "            drug_data = {\n",
    "                'scalar_feats': drug_data['scalar_feats'].to(self.device),\n",
    "                'vector_feats': drug_data['vector_feats'].to(self.device),\n",
    "                'batch': drug_data['batch'].to(self.device)\n",
    "            }\n",
    "            binding_labels = binding_labels.to(self.device)\n",
    "            se_labels = se_labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            binding_pred, se_pred = self.model(protein_data, drug_data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, binding_loss, se_loss = self.criterion(binding_pred, se_pred, binding_labels, se_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # Collect predictions for metrics\n",
    "            all_binding_preds.append(binding_pred.detach())\n",
    "            all_binding_targets.append(binding_labels.detach())\n",
    "            all_side_effect_preds.append(se_pred.detach())\n",
    "            all_side_effect_targets.append(se_labels.detach())\n",
    "            \n",
    "            # Update progress bar\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'B_Loss': f'{binding_loss.item():.4f}',\n",
    "                'SE_Loss': f'{se_loss.item():.4f}'\n",
    "            })\n",
    "        \n",
    "        # Calculate epoch metrics\n",
    "        binding_preds = torch.cat(all_binding_preds)\n",
    "        binding_targets = torch.cat(all_binding_targets)\n",
    "        side_effect_preds = torch.cat(all_side_effect_preds)\n",
    "        side_effect_targets = torch.cat(all_side_effect_targets)\n",
    "        \n",
    "        metrics = calculate_metrics(binding_preds, binding_targets, side_effect_preds, side_effect_targets)\n",
    "        \n",
    "        return total_loss / len(self.train_loader), metrics\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        all_binding_preds = []\n",
    "        all_binding_targets = []\n",
    "        all_side_effect_preds = []\n",
    "        all_side_effect_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for protein_data, drug_data, binding_labels, se_labels in tqdm(self.val_loader, desc=\"üìã Validating\"):\n",
    "                # Move data to device\n",
    "                protein_data = protein_data.to(self.device)\n",
    "                drug_data = {\n",
    "                    'scalar_feats': drug_data['scalar_feats'].to(self.device),\n",
    "                    'vector_feats': drug_data['vector_feats'].to(self.device),\n",
    "                    'batch': drug_data['batch'].to(self.device)\n",
    "                }\n",
    "                binding_labels = binding_labels.to(self.device)\n",
    "                se_labels = se_labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                binding_pred, se_pred = self.model(protein_data, drug_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss, binding_loss, se_loss = self.criterion(binding_pred, se_pred, binding_labels, se_labels)\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Collect predictions\n",
    "                all_binding_preds.append(binding_pred)\n",
    "                all_binding_targets.append(binding_labels)\n",
    "                all_side_effect_preds.append(se_pred)\n",
    "                all_side_effect_targets.append(se_labels)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        binding_preds = torch.cat(all_binding_preds)\n",
    "        binding_targets = torch.cat(all_binding_targets)\n",
    "        side_effect_preds = torch.cat(all_side_effect_preds)\n",
    "        side_effect_targets = torch.cat(all_side_effect_targets)\n",
    "        \n",
    "        metrics = calculate_metrics(binding_preds, binding_targets, side_effect_preds, side_effect_targets)\n",
    "        \n",
    "        return total_loss / len(self.val_loader), metrics\n",
    "    \n",
    "    def train(self, num_epochs=50, save_path=\"best_model.pth\"):\n",
    "        print(f\"üéØ Starting training for {num_epochs} epochs...\")\n",
    "        \n",
    "        best_val_auc = 0\n",
    "        patience = 10\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"\\n‚è∞ Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(\"-\" * 50)\n",
    "            \n",
    "            # Train\n",
    "            train_loss, train_metrics = self.train_epoch()\n",
    "            \n",
    "            # Validate\n",
    "            val_loss, val_metrics = self.validate()\n",
    "            \n",
    "            # Update history\n",
    "            self.history['train_loss'].append(train_loss)\n",
    "            self.history['val_loss'].append(val_loss)\n",
    "            self.history['train_binding_auc'].append(train_metrics['binding_auc'])\n",
    "            self.history['val_binding_auc'].append(val_metrics['binding_auc'])\n",
    "            self.history['train_side_effect_auc'].append(train_metrics.get('side_effect_auc', 0))\n",
    "            self.history['val_side_effect_auc'].append(val_metrics.get('side_effect_auc', 0))\n",
    "            \n",
    "            # Print results\n",
    "            print(f\"üìä Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "            print(f\"üéØ Train Binding AUC: {train_metrics['binding_auc']:.4f} | Val Binding AUC: {val_metrics['binding_auc']:.4f}\")\n",
    "            if 'side_effect_auc' in val_metrics:\n",
    "                print(f\"ü©∫ Train SE AUC: {train_metrics['side_effect_auc']:.4f} | Val SE AUC: {val_metrics['side_effect_auc']:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_metrics['binding_auc'] > best_val_auc:\n",
    "                best_val_auc = val_metrics['binding_auc']\n",
    "                patience_counter = 0\n",
    "                torch.save(self.model.state_dict(), save_path)\n",
    "                print(f\"üíæ New best model saved with Val AUC: {best_val_auc:.4f}\")\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"üõë Early stopping after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        print(f\"\\n‚úÖ Training completed! Best Val AUC: {best_val_auc:.4f}\")\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "02556236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è Using device: cuda\n",
      "üéØ Starting training for 5 epochs...\n",
      "\n",
      "‚è∞ Epoch 1/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÇ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [01:43<00:00,  8.43it/s, Loss=0.4384, B_Loss=0.6017, SE_Loss=0.0575]\n",
      "üìã Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:09<00:00, 11.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train Loss: 0.4347 | Val Loss: 0.3858\n",
      "üéØ Train Binding AUC: 0.7062 | Val Binding AUC: 0.7845\n",
      "ü©∫ Train SE AUC: 0.8776 | Val SE AUC: 0.9144\n",
      "üíæ New best model saved with Val AUC: 0.7845\n",
      "\n",
      "‚è∞ Epoch 2/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÇ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [01:20<00:00, 10.86it/s, Loss=0.3896, B_Loss=0.5322, SE_Loss=0.0570]\n",
      "üìã Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:08<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train Loss: 0.3851 | Val Loss: 0.3740\n",
      "üéØ Train Binding AUC: 0.7853 | Val Binding AUC: 0.8007\n",
      "ü©∫ Train SE AUC: 0.9000 | Val SE AUC: 0.9194\n",
      "üíæ New best model saved with Val AUC: 0.8007\n",
      "\n",
      "‚è∞ Epoch 3/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÇ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [01:20<00:00, 10.82it/s, Loss=0.2534, B_Loss=0.3387, SE_Loss=0.0545]\n",
      "üìã Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:07<00:00, 13.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train Loss: 0.3759 | Val Loss: 0.3647\n",
      "üéØ Train Binding AUC: 0.7984 | Val Binding AUC: 0.8126\n",
      "ü©∫ Train SE AUC: 0.9004 | Val SE AUC: 0.9204\n",
      "üíæ New best model saved with Val AUC: 0.8126\n",
      "\n",
      "‚è∞ Epoch 4/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÇ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [01:16<00:00, 11.38it/s, Loss=0.3526, B_Loss=0.4779, SE_Loss=0.0603]\n",
      "üìã Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:08<00:00, 13.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train Loss: 0.3663 | Val Loss: 0.3756\n",
      "üéØ Train Binding AUC: 0.8125 | Val Binding AUC: 0.8215\n",
      "ü©∫ Train SE AUC: 0.9009 | Val SE AUC: 0.9195\n",
      "üíæ New best model saved with Val AUC: 0.8215\n",
      "\n",
      "‚è∞ Epoch 5/5\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "üöÇ Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 869/869 [01:16<00:00, 11.43it/s, Loss=0.3790, B_Loss=0.5147, SE_Loss=0.0624]\n",
      "üìã Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 109/109 [00:07<00:00, 14.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Train Loss: 0.3632 | Val Loss: 0.3722\n",
      "üéØ Train Binding AUC: 0.8156 | Val Binding AUC: 0.8256\n",
      "ü©∫ Train SE AUC: 0.9020 | Val SE AUC: 0.9203\n",
      "üíæ New best model saved with Val AUC: 0.8256\n",
      "\n",
      "‚úÖ Training completed! Best Val AUC: 0.8256\n",
      "üéâ Training pipeline complete!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: START TRAINING\n",
    "# =============================================================================\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"üñ•Ô∏è Using device: {device}\")\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = MultiTaskLoss(binding_weight=0.7, side_effect_weight=0.3)\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "\n",
    "# Start training!\n",
    "history = trainer.train(num_epochs=5, save_path=\"best_simple_model.pth\")\n",
    "\n",
    "print(\"üéâ Training pipeline complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3b91fcb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading best model for predictions...\n",
      "\n",
      "üéÆ INTERACTIVE PREDICTION MODE\n",
      "Enter 'quit' to exit\n",
      "üß™ Predicting for:\n",
      "   üß¨ Protein: MSLSFCGNNISSYNINDGVLQNSCFVDALNLVPHVFLLFITFPILFIGWG...\n",
      "   üíä Drug: CC1(C)Oc2ccc(C#N)cc2[C@@H](N2CCCC2=O)[C@@H]1O\n",
      "\n",
      "============================================================\n",
      "üéØ PREDICTION RESULTS\n",
      "============================================================\n",
      "\n",
      "üíä DRUG-TARGET BINDING PREDICTION:\n",
      "   Probability: 0.5851\n",
      "   üü¢ PREDICTION: LIKELY BINDS (confidence: 58.5%)\n",
      "   üìä Binding Strength: Weak\n",
      "\n",
      "ü©∫ PREDICTED SIDE EFFECTS (top 10):\n",
      "    1. UTI                                      0.8925 (High confidence)\n",
      "    2. Gas                                      0.8771 (High confidence)\n",
      "    3. Nervous                                  0.8348 (High confidence)\n",
      "    4. Nausea                                   0.8026 (High confidence)\n",
      "    5. Gastrointestinal disorder                0.7564 (Medium confidence)\n",
      "    6. Vomiting                                 0.7101 (Medium confidence)\n",
      "    7. Rash                                     0.6810 (Medium confidence)\n",
      "    8. Headache                                 0.6630 (Medium confidence)\n",
      "    9. Diarrhea                                 0.6458 (Medium confidence)\n",
      "   10. Dizziness                                0.6353 (Medium confidence)\n",
      "\n",
      "üìä SIDE EFFECT SUMMARY:\n",
      "   Total predicted side effects: 18\n",
      "   Average confidence: 0.6716\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MANUAL PREDICTION PIPELINE\n",
    "# =============================================================================\n",
    "\n",
    "def predict_manual_input(protein_sequence, drug_smiles, model, protein_builder, drug_builder, \n",
    "                        side_effect_mapping, uniprot_id, device='cuda', threshold=0.5):\n",
    "    \"\"\"\n",
    "    Make predictions for manual input of protein sequence and drug SMILES\n",
    "    \"\"\"\n",
    "    print(f\"üß™ Predicting for:\")\n",
    "    print(f\"   üß¨ Protein: {protein_sequence[:50]}...\")\n",
    "    print(f\"   üíä Drug: {drug_smiles}\")\n",
    "    \n",
    "    # Build graphs for the new inputs\n",
    "    with torch.no_grad():\n",
    "        # Create protein graph\n",
    "        protein_graph = protein_builder.build_protein_graph(uniprot_id, protein_sequence)\n",
    "        protein_graph.batch = torch.zeros(protein_graph.x.size(0), dtype=torch.long)\n",
    "        \n",
    "        # Create drug data\n",
    "        drug_data = drug_builder.smiles_to_gvp_data(drug_smiles, \"manual_drug\")\n",
    "        if drug_data is None:\n",
    "            print(\"‚ùå Failed to process drug SMILES\")\n",
    "            return None\n",
    "        \n",
    "        drug_data['batch'] = torch.zeros(drug_data['scalar_feats'].size(0), dtype=torch.long)\n",
    "        \n",
    "        # Move to device\n",
    "        protein_graph = protein_graph.to(device)\n",
    "        drug_data = {\n",
    "            'scalar_feats': drug_data['scalar_feats'].to(device),\n",
    "            'vector_feats': drug_data['vector_feats'].to(device),\n",
    "            'batch': drug_data['batch'].to(device)\n",
    "        }\n",
    "        \n",
    "        # Make prediction\n",
    "        binding_score, side_effect_scores = model(protein_graph, drug_data)\n",
    "        \n",
    "        # Convert to probabilities\n",
    "        binding_prob = binding_score.item()\n",
    "        side_effect_probs = side_effect_scores.cpu().numpy().flatten()\n",
    "        \n",
    "        return binding_prob, side_effect_probs\n",
    "\n",
    "def interpret_predictions(binding_prob, side_effect_probs, side_effect_mapping, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Convert model outputs to interpretable results\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üéØ PREDICTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Binding prediction interpretation\n",
    "    print(f\"\\nüíä DRUG-TARGET BINDING PREDICTION:\")\n",
    "    print(f\"   Probability: {binding_prob:.4f}\")\n",
    "    \n",
    "    if binding_prob >= threshold:\n",
    "        print(f\"   üü¢ PREDICTION: LIKELY BINDS (confidence: {binding_prob:.1%})\")\n",
    "        binding_strength = \"Strong\" if binding_prob > 0.8 else \"Moderate\" if binding_prob > 0.6 else \"Weak\"\n",
    "        print(f\"   üìä Binding Strength: {binding_strength}\")\n",
    "    else:\n",
    "        print(f\"   üî¥ PREDICTION: UNLIKELY TO BIND (confidence: {(1-binding_prob):.1%})\")\n",
    "    \n",
    "    # Side effect predictions\n",
    "    print(f\"\\nü©∫ PREDICTED SIDE EFFECTS (top 10):\")\n",
    "    \n",
    "    # Get side effect IDs and names\n",
    "    se_id_to_idx = side_effect_mapping['se_id_to_idx']\n",
    "    idx_to_se_id = {v: k for k, v in se_id_to_idx.items()}\n",
    "    \n",
    "    # Get ADR data for side effect names\n",
    "    adr_names = {}\n",
    "    for _, row in adr_pd.iterrows():\n",
    "        adr_names[row['meddra_id']] = row['meddra_name']\n",
    "    \n",
    "    # Get top predicted side effects\n",
    "    top_indices = np.argsort(side_effect_probs)[-10:][::-1]  # Top 10 highest probability\n",
    "    predicted_side_effects = []\n",
    "    \n",
    "    for idx in top_indices:\n",
    "        prob = side_effect_probs[idx]\n",
    "        if prob >= threshold:\n",
    "            se_id = idx_to_se_id.get(idx)\n",
    "            se_name = adr_names.get(se_id, f\"Side Effect {se_id}\")\n",
    "            predicted_side_effects.append((se_name, prob))\n",
    "    \n",
    "    if predicted_side_effects:\n",
    "        for i, (se_name, prob) in enumerate(predicted_side_effects, 1):\n",
    "            confidence = \"High\" if prob > 0.8 else \"Medium\" if prob > 0.6 else \"Low\"\n",
    "            print(f\"   {i:2d}. {se_name:<40} {prob:.4f} ({confidence} confidence)\")\n",
    "    else:\n",
    "        print(\"   No significant side effects predicted above threshold\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    num_predicted_se = np.sum(side_effect_probs >= threshold)\n",
    "    avg_se_prob = np.mean(side_effect_probs[side_effect_probs >= threshold]) if num_predicted_se > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìä SIDE EFFECT SUMMARY:\")\n",
    "    print(f\"   Total predicted side effects: {num_predicted_se}\")\n",
    "    print(f\"   Average confidence: {avg_se_prob:.4f}\")\n",
    "    \n",
    "    return binding_prob, predicted_side_effects\n",
    "\n",
    "# Load the best model for predictions\n",
    "print(\"üîß Loading best model for predictions...\")\n",
    "model.load_state_dict(torch.load(\"best_simple_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Example usage with some test data\n",
    "def test_with_examples():\n",
    "    \"\"\"Test the prediction pipeline with some examples\"\"\"\n",
    "    \n",
    "    # Example 1: Get a real example from our dataset\n",
    "    print(\"1. Testing with dataset example:\")\n",
    "    sample = dataset[0]\n",
    "    protein_seq = sample['protein_data'].sequence\n",
    "    drug_smiles = sample['drug_data']['smiles']\n",
    "    \n",
    "    results = predict_manual_input(protein_seq, drug_smiles, model, protein_builder, \n",
    "                                 drug_builder, dataset.side_effect_mapping, device)\n",
    "    if results:\n",
    "        binding_prob, side_effect_probs = results\n",
    "        interpret_predictions(binding_prob, side_effect_probs, dataset.side_effect_mapping)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Example 2: Let user input their own\n",
    "    print(\"2. Now test with your own inputs:\")\n",
    "\n",
    "# Run the examples\n",
    "# test_with_examples()\n",
    "\n",
    "# Function for continuous manual input\n",
    "def interactive_prediction():\n",
    "    \"\"\"Interactive mode for manual predictions\"\"\"\n",
    "    print(\"\\nüéÆ INTERACTIVE PREDICTION MODE\")\n",
    "    print(\"Enter 'quit' to exit\")\n",
    "    \n",
    "    protein_seq = \"MSLSFCGNNISSYNINDGVLQNSCFVDALNLVPHVFLLFITFPILFIGWGSQSSKVQIHHNTWLHFPGHNLRWILTFALLFVHVCEIAEGIVSDSRRESRHLHLFMPAVMGFVATTTSIVYYHNIETSNFPKLLLALFLYWVMAFITKTIKLVKYCQSGLDISNLRFCITGMMVILNGLLMAVEINVIRVRRYVFFMNPQKVKPPEDLQDLGVRFLQPFVNLLSKATYWWMNTLIISAHKKPIDLKAIGKLPIAMRAVTNYVCLKDAYEEQKKKVADHPNRTPSIWLAMYRAFGRPILLSSTFRYLADLLGFAGPLCISGIVQRVNETQNGTNNTTGISETLSSKEFLENAYVLAVLLFLALILQRTFLQASYYVTIETGINLRGALLAMIYNKILRLSTSNLSMGEMTLGQINNLVAIETNQLMWFLFLCPNLWAMPVQIIMGVILLYNLLGSSALVGAAVIVLLAPIQYFIATKLAEAQKSTLDYSTERLKKTNEILKGIKLLKLYAWEHIFCKSVEETRMKELSSLKTFALYTSLSIFMNAAIPIAAVLATFVTHAYASGNNLKPAEAFASLSLFHILVTPLFLLSTVVRFAVKAIISVQKLNEFLLSDEIGDDSWRTGESSLPFESCKKHTGVQPKTINRKQPGRYHLDSYEQSTRRLRPAETEDIAIKVTNGYFSWGSGLATLSNIDIRIPTGQLTMIVGQVGCGKSSLLLAILGEMQTLEGKVHWSNVNESEPSFEATRSRNRYSVAYAAQKPWLLNATVEENITFGSPFNKQRYKAVTDACSLQPDIDLLPFGDQTEIGERGINLSGGQRQRICVARALYQNTNIVFLDDPFSALDIHLSDHLMQEGILKFLQDDKRTLVLVTHKLQYLTHADWIIAMKDGSVLREGTLKDIQTKDVELYEHWKTLMNRQDQELEKDMEADQTTLERKTLRRAMYSREAKAQMEDEDEEEEEEEDEDDNMSTVMRLRTKMPWKTCWRYLTSGGFFLLILMIFSKLLKHSVIVAIDYWLATWTSEYSINNTGKADQTYYVAGFSILCGAGIFLCLVTSLTVEWMGLTAAKNLHHNLLNKIILGPIRFFDTTPLGLILNRFSADTNIIDQHIPPTLESLTRSTLLCLSAIGMISYATPVFLVALLPLGVAFYFIQKYFRVASKDLQELDDSTQLPLLCHFSETAEGLTTIRAFRHETRFKQRMLELTDTNNIAYLFLSAANRWLEVRTDYLGACIVLTASIASISGSSNSGLVGLGLLYALTITNYLNWVVRNLADLEVQMGAVKKVNSFLTMESENYEGTMDPSQVPEHWPQEGEIKIHDLCVRYENNLKPVLKHVKAYIKPGQKVGICGRTGSGKSSLSLAFFRMVDIFDGKIVIDGIDISKLPLHTLRSRLSIILQDPILFSGSIRFNLDPECKCTDDRLWEALEIAQLKNMVKSLPGGLDAVVTEGGENFSVGQRQLFCLARAFVRKSSILIMDEATASIDMATENILQKVVMTAFADRTVVTIAHRVSSIMDAGLVLVFSEGILVECDTVPNLLAHKNGLFSTLVMTNK\"    \n",
    "    drug_smiles = \"CC1(C)Oc2ccc(C#N)cc2[C@@H](N2CCCC2=O)[C@@H]1O\"\n",
    "    uniprot_id = \"O60706\"\n",
    "    try:\n",
    "        results = predict_manual_input(protein_seq, drug_smiles, model, protein_builder, \n",
    "                                        drug_builder, dataset.side_effect_mapping, uniprot_id, device)\n",
    "        if results:\n",
    "            binding_prob, side_effect_probs = results\n",
    "            interpret_predictions(binding_prob, side_effect_probs, dataset.side_effect_mapping)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error during prediction: {e}\")\n",
    "\n",
    "# Start interactive mode\n",
    "interactive_prediction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc917d2",
   "metadata": {},
   "source": [
    "# This part is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "2178d1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[554, 22], edge_index=[2, 5418], uniprot_id='O15245', sequence='MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTPDHHCQSPGVAELSQRCGWSPAEELNYTVPGLGPAGEAFLGQCRRYEVDWNQSALSCVDPLASLATNRSHLPLGPCQDGWVYDTPGSSIVTEFNLVCADSWKLDLFQSCLNAGFLFGSLGVGYFADRFGRKLCLLGTVLVNAVSGVLMAFSPNYMSMLLFRLLQGLVSKGNWMAGYTLITEFVGSGSRRTVAIMYQMAFTVGLVALTGLAYALPHWRWLQLAVSLPTFLFLLYYWCVPESPRWLLSQKRNTEAIKIMDHIAQKNGKLPPADLKMLSLEEDVTEKLSPSFADLFRTPRLRKRTFILMYLWFTDSVLYQGLILHMGATSGNLYLDFLYSALVEIPGAFIALITIDRVGRIYPMAMSNLLAGAACLVMIFISPDLHWLNIIIMCVGRMGITIAIQMICLVNAELYPTFVRNLGVMVCSSLCDIGGIITPFIVFRLREVWQALPLILFAVLGLLAAGVTLLLPETKGVALPETMKDAENLGRKAKPKENTIYLKVQTSEPSGT', batch=[554])\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0]['protein_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2cb6e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVP(nn.Module):\n",
    "    \"\"\"Geometric Vector Perceptron - FIXED VERSION\"\"\"\n",
    "    def __init__(self, in_scalar, in_vector, out_scalar, out_vector):\n",
    "        super().__init__()\n",
    "        self.in_scalar = in_scalar\n",
    "        self.in_vector = in_vector\n",
    "        self.out_scalar = out_scalar\n",
    "        self.out_vector = out_vector\n",
    "        \n",
    "        # Scalar pathway (includes vector norms)\n",
    "        self.W_h = nn.Linear(in_scalar + in_vector, out_scalar)\n",
    "        \n",
    "        # Vector pathway - FIXED: input should be [..., 3, in_vector]\n",
    "        self.W_v = nn.Linear(in_vector, out_vector, bias=False)\n",
    "        \n",
    "    def forward(self, h, v):\n",
    "        # h: [num_nodes, in_scalar] \n",
    "        # v: [num_nodes, 3, in_vector]\n",
    "        \n",
    "        # Compute vector norms and incorporate into scalar features\n",
    "        v_norm = torch.norm(v, dim=1)  # [num_nodes, in_vector]\n",
    "        \n",
    "        # Enhanced scalar features\n",
    "        h_input = torch.cat([h, v_norm], dim=-1)\n",
    "        h_out = F.relu(self.W_h(h_input))\n",
    "        \n",
    "        # Transform vector features - FIXED\n",
    "        # v shape: [num_nodes, 3, in_vector] -> we want to transform the vector dimension\n",
    "        v_reshaped = v.reshape(-1, self.in_vector)  # [num_nodes * 3, in_vector]\n",
    "        v_transformed = self.W_v(v_reshaped)  # [num_nodes * 3, out_vector]\n",
    "        v_out = v_transformed.reshape(-1, 3, self.out_vector)  # [num_nodes, 3, out_vector]\n",
    "        \n",
    "        return h_out, v_out\n",
    "\n",
    "class GVPBlock(nn.Module):\n",
    "    def __init__(self, scalar_dim, vector_dim):\n",
    "        super().__init__()\n",
    "        self.gvp = GVP(scalar_dim, vector_dim, scalar_dim, vector_dim)\n",
    "        self.ln_scalar = nn.LayerNorm(scalar_dim)\n",
    "        self.ln_vector = nn.LayerNorm(vector_dim)\n",
    "        \n",
    "    def forward(self, h, v):\n",
    "        h_res, v_res = h, v\n",
    "        h, v = self.gvp(h, v)\n",
    "        h = self.ln_scalar(h + h_res)\n",
    "        v = self.ln_vector(v + v_res)\n",
    "        return h, v\n",
    "\n",
    "class DrugGVPEncoder(nn.Module):\n",
    "    def __init__(self, scalar_dim, vector_neighbor_dim, hidden_dim=128, num_layers=3):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vector_neighbor_dim = vector_neighbor_dim\n",
    "        \n",
    "        # Scalar feature projection\n",
    "        self.scalar_proj = nn.Linear(scalar_dim, hidden_dim)\n",
    "        \n",
    "        # Vector feature projection - FIXED: input should be neighbors * coordinates = 4 * 3 = 12\n",
    "        total_vector_features = vector_neighbor_dim * 3  # 4 neighbors * 3 coordinates = 12\n",
    "        self.vector_proj = nn.Linear(total_vector_features, hidden_dim)\n",
    "        \n",
    "        # GVP blocks\n",
    "        self.gvp_blocks = nn.ModuleList([\n",
    "            GVPBlock(hidden_dim, hidden_dim) for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Readout layer\n",
    "        self.readout = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        print(f\"DrugGVPEncoder initialized:\")\n",
    "        print(f\"  scalar_dim: {scalar_dim}\")\n",
    "        print(f\"  vector_neighbor_dim: {vector_neighbor_dim}\")\n",
    "        print(f\"  total_vector_features: {total_vector_features}\")\n",
    "        print(f\"  vector_proj: {self.vector_proj}\")\n",
    "        \n",
    "    def forward(self, scalar_feats, vector_feats, batch_mask=None):\n",
    "        # scalar_feats: [58, 14]\n",
    "        # vector_feats: [58, 4, 3]\n",
    "        \n",
    "        # Project scalar features\n",
    "        h = self.scalar_proj(scalar_feats)  # [58, 128]\n",
    "        \n",
    "        # Project vector features - CORRECT\n",
    "        v_flat = vector_feats.reshape(vector_feats.size(0), -1)  # [58, 4*3] = [58, 12]\n",
    "        v_projected = self.vector_proj(v_flat)  # [58, 128]\n",
    "        \n",
    "        # Create vector representation for GVP\n",
    "        v = v_projected.unsqueeze(1).repeat(1, 3, 1)  # [58, 3, 128]\n",
    "        \n",
    "        # Apply GVP blocks\n",
    "        for gvp_block in self.gvp_blocks:\n",
    "            h, v = gvp_block(h, v)\n",
    "        \n",
    "        # Readout\n",
    "        v_norm = torch.norm(v, dim=1)  # [58, 128]\n",
    "        combined = torch.cat([h, v_norm], dim=-1)  # [58, 256]\n",
    "        \n",
    "        # Global pooling\n",
    "        if batch_mask is not None:\n",
    "            drug_embedding = torch.zeros(batch_mask.max() + 1, combined.size(-1), device=combined.device)\n",
    "            for i in range(batch_mask.max() + 1):\n",
    "                mask = (batch_mask == i)\n",
    "                if mask.any():\n",
    "                    drug_embedding[i] = combined[mask].mean(dim=0)\n",
    "        else:\n",
    "            drug_embedding = combined.mean(dim=0, keepdim=True)  # [1, 256]\n",
    "        \n",
    "        drug_embedding = self.readout(drug_embedding)  # [1, 128]\n",
    "        return drug_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "433d8cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration updated from data:\n",
      "  protein_feat_dim: 22\n",
      "  drug_scalar_dim: 14\n",
      "  drug_vector_neighbor_dim: 4\n",
      "  drug_vector_coord_dim: 3\n"
     ]
    }
   ],
   "source": [
    "cfg.update_from_data(\n",
    "    protein_feat_dim=22,           # From protein graph: [931, 22]\n",
    "    drug_scalar_dim=14,            # From drug scalar: [58, 14]  \n",
    "    drug_vector_neighbor_dim=4,    # From drug vector: [58, 4, 3]\n",
    "    drug_vector_coord_dim=3        # From drug vector: [58, 4, 3]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ff52a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reinitializing model with fixed GVP...\n",
      "DrugGVPEncoder initialized:\n",
      "  scalar_dim: 14\n",
      "  vector_neighbor_dim: 4\n",
      "  total_vector_features: 12\n",
      "  vector_proj: Linear(in_features=12, out_features=128, bias=True)\n",
      "Model initialized with:\n",
      "  - Protein feature dim: 22\n",
      "  - Drug scalar dim: 14\n",
      "  - Drug vector dim: 4\n",
      "  - Hidden dim: 128\n",
      "  - Number of side effects: 4817\n",
      "\n",
      "Testing fixed model forward pass...\n",
      "Input shapes:\n",
      "  Protein features: torch.Size([931, 22])\n",
      "  Drug scalar features: torch.Size([58, 14])\n",
      "  Drug vector features: torch.Size([58, 4, 3])\n",
      "\n",
      "Output shapes:\n",
      "Binding prediction: torch.Size([]) - 0.5125\n",
      "Side effect prediction: torch.Size([1, 4817])\n",
      "Actual binding label: 1.0\n",
      "Actual side effects sum: 77.0\n"
     ]
    }
   ],
   "source": [
    "class GCN_GVP_Model(nn.Module):\n",
    "    def __init__(self, protein_feat_dim, drug_scalar_dim, drug_vector_dim, \n",
    "                 hidden_dim=128, num_side_effects=4817):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoders\n",
    "        self.protein_encoder = ProteinGCNEncoder(protein_feat_dim, hidden_dim)\n",
    "        self.drug_encoder = DrugGVPEncoder(drug_scalar_dim, drug_vector_dim, hidden_dim)\n",
    "        \n",
    "        self.num_side_effects = num_side_effects\n",
    "        \n",
    "        # Interaction module\n",
    "        self.interaction_net = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 3, hidden_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        # Prediction heads\n",
    "        self.binding_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Simpler side effect head to avoid overfitting\n",
    "        self.side_effect_head = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, self.num_side_effects),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        print(f\"Model initialized with:\")\n",
    "        print(f\"  - Protein feature dim: {protein_feat_dim}\")\n",
    "        print(f\"  - Drug scalar dim: {drug_scalar_dim}\")\n",
    "        print(f\"  - Drug vector dim: {drug_vector_dim}\")\n",
    "        print(f\"  - Hidden dim: {hidden_dim}\")\n",
    "        print(f\"  - Number of side effects: {self.num_side_effects}\")\n",
    "        \n",
    "    def forward(self, protein_data, drug_data):\n",
    "        # Encode protein\n",
    "        protein_embedding = self.protein_encoder(\n",
    "            protein_data.x, \n",
    "            protein_data.edge_index,\n",
    "            protein_data.batch if hasattr(protein_data, 'batch') else None\n",
    "        )\n",
    "        \n",
    "        # Encode drug\n",
    "        drug_embedding = self.drug_encoder(\n",
    "            drug_data['scalar_feats'],\n",
    "            drug_data['vector_feats'],\n",
    "            drug_data['batch'] if 'batch' in drug_data else None\n",
    "        )\n",
    "        \n",
    "        # Make sure embeddings have the same batch size\n",
    "        if protein_embedding.dim() == 1:\n",
    "            protein_embedding = protein_embedding.unsqueeze(0)\n",
    "        if drug_embedding.dim() == 1:\n",
    "            drug_embedding = drug_embedding.unsqueeze(0)\n",
    "            \n",
    "        # Interaction features\n",
    "        interaction_features = torch.cat([\n",
    "            protein_embedding,\n",
    "            drug_embedding,\n",
    "            protein_embedding * drug_embedding  # Element-wise interaction\n",
    "        ], dim=1)\n",
    "        \n",
    "        # Combined representation\n",
    "        combined_rep = self.interaction_net(interaction_features)\n",
    "        \n",
    "        # Predictions\n",
    "        binding_score = self.binding_head(combined_rep).squeeze()\n",
    "        side_effects = self.side_effect_head(combined_rep)\n",
    "        \n",
    "        return binding_score, side_effects\n",
    "\n",
    "# Reinitialize the model with fixed GVP\n",
    "print(\"Reinitializing model with fixed GVP...\")\n",
    "model = GCN_GVP_Model(\n",
    "    protein_feat_dim=cfg.protein_feat_dim,\n",
    "    drug_scalar_dim=cfg.drug_scalar_dim, \n",
    "    drug_vector_dim=cfg.drug_vector_neighbor_dim,  # This is actually the neighbor count (4)\n",
    "    hidden_dim=cfg.hidden_dim,\n",
    "    num_side_effects=4817\n",
    ")\n",
    "\n",
    "# Test the model again\n",
    "print(\"\\nTesting fixed model forward pass...\")\n",
    "sample_protein = sample['protein_data']\n",
    "sample_drug = sample['drug_data']\n",
    "\n",
    "# Add batch dimension\n",
    "sample_protein.batch = torch.zeros(sample_protein.x.size(0), dtype=torch.long)\n",
    "sample_drug['batch'] = torch.zeros(sample_drug['scalar_feats'].size(0), dtype=torch.long)\n",
    "\n",
    "print(f\"Input shapes:\")\n",
    "print(f\"  Protein features: {sample_protein.x.shape}\")\n",
    "print(f\"  Drug scalar features: {sample_drug['scalar_feats'].shape}\")\n",
    "print(f\"  Drug vector features: {sample_drug['vector_feats'].shape}\")\n",
    "\n",
    "# Forward pass\n",
    "with torch.no_grad():\n",
    "    binding_pred, side_effect_pred = model(sample_protein, sample_drug)\n",
    "\n",
    "print(f\"\\nOutput shapes:\")\n",
    "print(f\"Binding prediction: {binding_pred.shape} - {binding_pred.item():.4f}\")\n",
    "print(f\"Side effect prediction: {side_effect_pred.shape}\")\n",
    "print(f\"Actual binding label: {sample['binding_label'].item()}\")\n",
    "print(f\"Actual side effects sum: {sample['side_effects'].sum().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "43942d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import Batch as PyGBatch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"Custom collate function for our heterogeneous data\"\"\"\n",
    "    protein_data_list = []\n",
    "    drug_data_list = []\n",
    "    binding_labels = []\n",
    "    side_effects_list = []\n",
    "    \n",
    "    protein_batch = []\n",
    "    drug_batch = []\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        # Protein data\n",
    "        protein_data = sample['protein_data']\n",
    "        protein_data.batch = torch.full((protein_data.x.size(0),), i, dtype=torch.long)\n",
    "        protein_data_list.append(protein_data)\n",
    "        \n",
    "        # Drug data  \n",
    "        drug_data = sample['drug_data'].copy()\n",
    "        drug_data['batch'] = torch.full((drug_data['scalar_feats'].size(0),), i, dtype=torch.long)\n",
    "        drug_data_list.append(drug_data)\n",
    "        \n",
    "        # Labels\n",
    "        binding_labels.append(sample['binding_label'])\n",
    "        side_effects_list.append(sample['side_effects'])\n",
    "    \n",
    "    # Batch protein data using PyG's Batch\n",
    "    batched_protein = PyGBatch.from_data_list(protein_data_list)\n",
    "    \n",
    "    # Batch drug data manually\n",
    "    batched_drug = {\n",
    "        'scalar_feats': torch.cat([d['scalar_feats'] for d in drug_data_list], dim=0),\n",
    "        'vector_feats': torch.cat([d['vector_feats'] for d in drug_data_list], dim=0),\n",
    "        'batch': torch.cat([d['batch'] for d in drug_data_list], dim=0)\n",
    "    }\n",
    "    \n",
    "    batched_binding = torch.stack(binding_labels)\n",
    "    batched_side_effects = torch.stack(side_effects_list)\n",
    "    \n",
    "    return batched_protein, batched_drug, batched_binding, batched_side_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d04a8e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splits: Train 27792, Val 3474, Test 3475\n",
      "\n",
      "Testing data loader with batch...\n",
      "Batch 0:\n",
      "  Protein batch: DataBatch(x=[22348, 22], edge_index=[2, 190314], uniprot_id=[32], sequence=[32], batch=[22348], ptr=[33])\n",
      "  Drug scalar batch: torch.Size([1851, 14])\n",
      "  Drug vector batch: torch.Size([1851, 4, 3])\n",
      "  Binding labels: torch.Size([32])\n",
      "  Side effects: torch.Size([32, 4817])\n",
      "  Model outputs - Binding: torch.Size([32]), Side effects: torch.Size([32, 4817])\n"
     ]
    }
   ],
   "source": [
    "# Create data loaders with our working collate function\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Split dataset\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = random_split(\n",
    "    dataset, [train_size, val_size, test_size]\n",
    ")\n",
    "\n",
    "print(f\"Dataset splits: Train {len(train_dataset)}, Val {len(val_dataset)}, Test {len(test_dataset)}\")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=cfg.batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=cfg.batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=cfg.batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# Test the data loader with a batch\n",
    "print(\"\\nTesting data loader with batch...\")\n",
    "for batch_idx, (protein_batch, drug_batch, binding_batch, se_batch) in enumerate(train_loader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"  Protein batch: {protein_batch}\")\n",
    "    print(f\"  Drug scalar batch: {drug_batch['scalar_feats'].shape}\")\n",
    "    print(f\"  Drug vector batch: {drug_batch['vector_feats'].shape}\")\n",
    "    print(f\"  Binding labels: {binding_batch.shape}\")\n",
    "    print(f\"  Side effects: {se_batch.shape}\")\n",
    "    \n",
    "    # Test model with batch\n",
    "    with torch.no_grad():\n",
    "        batch_binding_pred, batch_se_pred = model(protein_batch, drug_batch)\n",
    "    \n",
    "    print(f\"  Model outputs - Binding: {batch_binding_pred.shape}, Side effects: {batch_se_pred.shape}\")\n",
    "    \n",
    "    if batch_idx == 0:  # Just test first batch\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a8a9a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiTaskLoss(nn.Module):\n",
    "    def __init__(self, binding_weight=0.7, side_effect_weight=0.3, pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.binding_weight = binding_weight\n",
    "        self.side_effect_weight = side_effect_weight\n",
    "        \n",
    "        # For binding classification\n",
    "        self.binding_bce = nn.BCELoss()\n",
    "        \n",
    "        # For multi-label side effects\n",
    "        if pos_weight is not None:\n",
    "            self.side_effect_bce = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "        else:\n",
    "            self.side_effect_bce = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    def forward(self, binding_pred, side_effect_pred, binding_true, side_effect_true):\n",
    "        binding_loss = self.binding_bce(binding_pred, binding_true)\n",
    "        \n",
    "        # Side effect loss - only compute if we have side effect labels\n",
    "        if side_effect_pred.numel() > 0 and side_effect_true.numel() > 0:\n",
    "            side_effect_loss = self.side_effect_bce(side_effect_pred, side_effect_true)\n",
    "        else:\n",
    "            side_effect_loss = torch.tensor(0.0, device=binding_pred.device)\n",
    "        \n",
    "        total_loss = (self.binding_weight * binding_loss + \n",
    "                     self.side_effect_weight * side_effect_loss)\n",
    "        \n",
    "        return total_loss, binding_loss, side_effect_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "199743b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Training setup complete!\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.device = device\n",
    "        self.model.to(device)\n",
    "        \n",
    "    def train_epoch(self):\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        binding_preds = []\n",
    "        binding_targets = []\n",
    "        \n",
    "        for batch_idx, (protein_data, drug_data, binding_labels, se_labels) in enumerate(self.train_loader):\n",
    "            # Move data to device\n",
    "            protein_data = protein_data.to(self.device)\n",
    "            drug_data = {\n",
    "                'scalar_feats': drug_data['scalar_feats'].to(self.device),\n",
    "                'vector_feats': drug_data['vector_feats'].to(self.device),\n",
    "                'batch': drug_data['batch'].to(self.device)\n",
    "            }\n",
    "            binding_labels = binding_labels.to(self.device)\n",
    "            se_labels = se_labels.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            binding_pred, se_pred = self.model(protein_data, drug_data)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss, binding_loss, se_loss = self.criterion(binding_pred, se_pred, binding_labels, se_labels)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            binding_preds.extend(binding_pred.detach().cpu().numpy())\n",
    "            binding_targets.extend(binding_labels.detach().cpu().numpy())\n",
    "            \n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'  Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "        \n",
    "        # Calculate metrics\n",
    "        binding_auc = roc_auc_score(binding_targets, binding_preds)\n",
    "        binding_ap = average_precision_score(binding_targets, binding_preds)\n",
    "        \n",
    "        return total_loss / len(self.train_loader), binding_auc, binding_ap\n",
    "    \n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        binding_preds = []\n",
    "        binding_targets = []\n",
    "        se_preds = []\n",
    "        se_targets = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for protein_data, drug_data, binding_labels, se_labels in self.val_loader:\n",
    "                # Move data to device\n",
    "                protein_data = protein_data.to(self.device)\n",
    "                drug_data = {\n",
    "                    'scalar_feats': drug_data['scalar_feats'].to(self.device),\n",
    "                    'vector_feats': drug_data['vector_feats'].to(self.device),\n",
    "                    'batch': drug_data['batch'].to(self.device)\n",
    "                }\n",
    "                binding_labels = binding_labels.to(self.device)\n",
    "                se_labels = se_labels.to(self.device)\n",
    "                \n",
    "                # Forward pass\n",
    "                binding_pred, se_pred = self.model(protein_data, drug_data)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss, binding_loss, se_loss = self.criterion(binding_pred, se_pred, binding_labels, se_labels)\n",
    "                \n",
    "                total_loss += loss.item()\n",
    "                binding_preds.extend(binding_pred.detach().cpu().numpy())\n",
    "                binding_targets.extend(binding_labels.detach().cpu().numpy())\n",
    "                se_preds.extend(se_pred.detach().cpu().numpy())\n",
    "                se_targets.extend(se_labels.detach().cpu().numpy())\n",
    "        \n",
    "        # Calculate metrics\n",
    "        binding_auc = roc_auc_score(binding_targets, binding_preds)\n",
    "        binding_ap = average_precision_score(binding_targets, binding_preds)\n",
    "        \n",
    "        # Side effect metrics (simplified)\n",
    "        se_preds = np.array(se_preds)\n",
    "        se_targets = np.array(se_targets)\n",
    "        se_auc = roc_auc_score(se_targets.ravel(), se_preds.ravel()) if len(se_targets) > 0 else 0.0\n",
    "        \n",
    "        return total_loss / len(self.val_loader), binding_auc, binding_ap, se_auc\n",
    "\n",
    "# Initialize training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=cfg.learning_rate, weight_decay=1e-4)\n",
    "criterion = MultiTaskLoss(binding_weight=0.7, side_effect_weight=0.3)\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(model, train_loader, val_loader, criterion, optimizer, device)\n",
    "\n",
    "print(\"Training setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "308b2a26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/1\n",
      "  Batch 0, Loss: 0.7785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:31:51] UFFTYPER: Unrecognized charge state for atom: 14\n",
      "[16:33:37] UFFTYPER: Unrecognized charge state for atom: 12\n",
      "[16:33:51] UFFTYPER: Unrecognized charge state for atom: 0\n",
      "[16:33:51] UFFTYPER: Unrecognized atom type: Zn+2 (0)\n",
      "[16:34:32] UFFTYPER: Unrecognized atom type: Ca+2 (0)\n",
      "[16:34:32] UFFTYPER: Unrecognized atom type: Ca+2 (0)\n",
      "[16:34:32] UFFTYPER: Unrecognized atom type: Ca+2 (0)\n",
      "[16:39:53] UFFTYPER: Unrecognized charge state for atom: 20\n",
      "[16:40:48] UFFTYPER: Unrecognized charge state for atom: 4\n",
      "[16:45:12] UFFTYPER: Unrecognized charge state for atom: 14\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting training...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[76]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(num_epochs)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m train_loss, train_binding_auc, train_binding_ap = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Validate\u001b[39;00m\n\u001b[32m     14\u001b[39m val_loss, val_binding_auc, val_binding_ap, val_se_auc = trainer.validate()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[75]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mTrainer.train_epoch\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     18\u001b[39m binding_preds = []\n\u001b[32m     19\u001b[39m binding_targets = []\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinding_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mse_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Move data to device\u001b[39;49;00m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprotein_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotein_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdrug_data\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscalar_feats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscalar_feats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvector_feats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvector_feats\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrug_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mbatch\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m    \u001b[49m\u001b[43m}\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakar\\miniconda3\\envs\\dti-adr\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    698\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    699\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    700\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m701\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    702\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    703\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    704\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    705\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    706\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    707\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakar\\miniconda3\\envs\\dti-adr\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    755\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    756\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m757\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    758\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    759\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakar\\miniconda3\\envs\\dti-adr\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakar\\miniconda3\\envs\\dti-adr\\Lib\\site-packages\\torch\\utils\\data\\dataset.py:420\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    419\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m420\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 103\u001b[39m, in \u001b[36mDTIDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    100\u001b[39m row = \u001b[38;5;28mself\u001b[39m.dti_df.iloc[idx]\n\u001b[32m    102\u001b[39m \u001b[38;5;66;03m# Get protein graph\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m protein_graph = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_protein_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget_uniprot_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msequence\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[38;5;66;03m# Get drug data\u001b[39;00m\n\u001b[32m    106\u001b[39m drug_data = \u001b[38;5;28mself\u001b[39m.get_drug_data(row[\u001b[33m'\u001b[39m\u001b[33mdrug_chembl_id\u001b[39m\u001b[33m'\u001b[39m], row[\u001b[33m'\u001b[39m\u001b[33msmiles\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mDTIDataset.get_protein_graph\u001b[39m\u001b[34m(self, uniprot_id, sequence)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get or create protein graph\"\"\"\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m uniprot_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.protein_cache:\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28mself\u001b[39m.protein_cache[uniprot_id] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprotein_builder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild_protein_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43muniprot_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.protein_cache[uniprot_id]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mProteinGraphBuilder.build_protein_graph\u001b[39m\u001b[34m(self, uniprot_id, sequence)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# Build edges\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m coords \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     edge_index = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild_spatial_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# Fallback: sequence-based edges\u001b[39;00m\n\u001b[32m    122\u001b[39m     n_residues = \u001b[38;5;28mlen\u001b[39m(sequence)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[61]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mProteinGraphBuilder.build_spatial_edges\u001b[39m\u001b[34m(self, coords, cutoff)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_nodes):\n\u001b[32m     57\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(i + \u001b[32m1\u001b[39m, n_nodes):\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m         distance = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinalg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoords\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m distance < cutoff:\n\u001b[32m     60\u001b[39m             edges.append([i, j])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\zakar\\miniconda3\\envs\\dti-adr\\Lib\\site-packages\\numpy\\linalg\\_linalg.py:2792\u001b[39m, in \u001b[36mnorm\u001b[39m\u001b[34m(x, ord, axis, keepdims)\u001b[39m\n\u001b[32m   2790\u001b[39m     sqnorm = x_real.dot(x_real) + x_imag.dot(x_imag)\n\u001b[32m   2791\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2792\u001b[39m     sqnorm = \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2793\u001b[39m ret = sqrt(sqnorm)\n\u001b[32m   2794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m keepdims:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "def train_model(num_epochs=50):\n",
    "    best_val_auc = 0\n",
    "    patience = 10\n",
    "    patience_counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_binding_auc, train_binding_ap = trainer.train_epoch()\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_binding_auc, val_binding_ap, val_se_auc = trainer.validate()\n",
    "        \n",
    "        print(f'Train Loss: {train_loss:.4f}, Train AUC: {train_binding_auc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f}, Val AUC: {val_binding_auc:.4f}, Val SE AUC: {val_se_auc:.4f}')\n",
    "        \n",
    "        # Early stopping\n",
    "        if val_binding_auc > best_val_auc:\n",
    "            best_val_auc = val_binding_auc\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f'New best model saved with AUC: {best_val_auc:.4f}')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping after {epoch+1} epochs')\n",
    "                break\n",
    "\n",
    "# Start training\n",
    "print(\"Starting training...\")\n",
    "train_model(num_epochs=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti-adr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
