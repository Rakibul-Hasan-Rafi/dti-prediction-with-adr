{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2a46ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92994f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main dataset shape: (34741, 7)\n",
      "Columns: ['drug_chembl_id', 'target_uniprot_id', 'label', 'smiles', 'sequence', 'molfile_3d', 'rxcui']\n",
      "\n",
      "First few rows:\n",
      "  drug_chembl_id target_uniprot_id  label  \\\n",
      "0     CHEMBL1000            O15245      0   \n",
      "1     CHEMBL1000            P08183      1   \n",
      "2     CHEMBL1000            P35367      1   \n",
      "3     CHEMBL1000            Q02763      0   \n",
      "4     CHEMBL1000            Q12809      0   \n",
      "\n",
      "                                        smiles  \\\n",
      "0  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "1  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "2  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "3  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "4  O=C(O)COCCN1CCN(C(c2ccccc2)c2ccc(Cl)cc2)CC1   \n",
      "\n",
      "                                            sequence  \\\n",
      "0  MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...   \n",
      "1  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...   \n",
      "2  MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...   \n",
      "3  MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...   \n",
      "4  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...   \n",
      "\n",
      "                                          molfile_3d  rxcui  \n",
      "0  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "1  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "2  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "3  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "4  \\n     RDKit          3D\\n\\n 52 54  0  0  0  0...  20610  \n",
      "\n",
      "Unique drugs: 1028\n",
      "Unique proteins: 2385\n",
      "Total interactions: 34741\n"
     ]
    }
   ],
   "source": [
    "# Load the main dataset\n",
    "data_path = \"scope_onside_common_v3.parquet\"\n",
    "main_df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Main dataset shape: {main_df.shape}\")\n",
    "print(f\"Columns: {main_df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(main_df.head())\n",
    "\n",
    "# Get unique counts\n",
    "n_unique_drugs = main_df['drug_id'].nunique() if 'drug_id' in main_df.columns else main_df.iloc[:, 0].nunique()\n",
    "n_unique_proteins = main_df['protein_id'].nunique() if 'protein_id' in main_df.columns else main_df.iloc[:, 1].nunique()\n",
    "\n",
    "print(f\"\\nUnique drugs: {n_unique_drugs}\")\n",
    "print(f\"Unique proteins: {n_unique_proteins}\")\n",
    "print(f\"Total interactions: {len(main_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17af320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ EXPERIMENT CONFIGURATION:\n",
      "   Drug encoding: chemberta\n",
      "   Protein encoding: esm\n",
      "   3D features: Enabled\n",
      "   ADR encoding: TF-IDF (fixed)\n",
      "\n",
      "üìÅ Loading embeddings...\n",
      "Drug chemberta embeddings loaded: (1028, 5)\n",
      "Protein esm embeddings loaded: (2385, 4)\n",
      "EGNN Drug 3D embeddings loaded: (1028, 3)\n",
      "GVP-GNN Protein 3D embeddings loaded: (2385, 8)\n",
      "\n",
      "üíä Loading ADR TF-IDF data...\n",
      "EGNN Drug 3D embeddings loaded: (1028, 3)\n",
      "GVP-GNN Protein 3D embeddings loaded: (2385, 8)\n",
      "\n",
      "üíä Loading ADR TF-IDF data...\n",
      "ADR TF-IDF train: (719, 4049)\n",
      "ADR TF-IDF val: (154, 4049)\n",
      "ADR TF-IDF test: (155, 4049)\n",
      "ADR dimensions: 4048 (from 4817 original)\n",
      "Combined ADR data: (1028, 4049)\n",
      "\n",
      "üìè Calculating embedding dimensions...\n",
      "\n",
      "üìä EMBEDDING DIMENSIONS:\n",
      "Drug (chemberta): 384 + 256 (3D) = 640\n",
      "Protein (esm): 1280 + 1024 (3D) = 2304\n",
      "ADR (TF-IDF): 4048\n",
      "Shared space: 512\n",
      "ADR TF-IDF train: (719, 4049)\n",
      "ADR TF-IDF val: (154, 4049)\n",
      "ADR TF-IDF test: (155, 4049)\n",
      "ADR dimensions: 4048 (from 4817 original)\n",
      "Combined ADR data: (1028, 4049)\n",
      "\n",
      "üìè Calculating embedding dimensions...\n",
      "\n",
      "üìä EMBEDDING DIMENSIONS:\n",
      "Drug (chemberta): 384 + 256 (3D) = 640\n",
      "Protein (esm): 1280 + 1024 (3D) = 2304\n",
      "ADR (TF-IDF): 4048\n",
      "Shared space: 512\n"
     ]
    }
   ],
   "source": [
    "# CONFIGURATION - Select which embeddings to use\n",
    "DRUG_ENCODING = 'chemberta'    # Options: 'smiles2vec', 'chemberta'\n",
    "PROTEIN_ENCODING = 'esm'        # Options: 'esm' (add more options here when available)\n",
    "USE_3D_FEATURES = True          # Whether to use 3D features (EGNN for drugs, GVP-GNN for proteins)\n",
    "\n",
    "print(f\"üî¨ EXPERIMENT CONFIGURATION:\")\n",
    "print(f\"   Drug encoding: {DRUG_ENCODING}\")\n",
    "print(f\"   Protein encoding: {PROTEIN_ENCODING}\")\n",
    "print(f\"   3D features: {'Enabled' if USE_3D_FEATURES else 'Disabled'}\")\n",
    "print(f\"   ADR encoding: TF-IDF (fixed)\")\n",
    "\n",
    "# Available embedding files\n",
    "EMBEDDING_PATHS = {\n",
    "    'drug': {\n",
    "        'smiles2vec': \"2. Drug_embeddings/smiles_embeddings_smiles2vec.parquet\",\n",
    "        'chemberta': \"2. Drug_embeddings/smiles_embeddings_chemberta.parquet\",\n",
    "    },\n",
    "    'protein': {\n",
    "        'esm': \"3. Protein_enbeddings/ESM_embeddings_(t33_650m model).parquet\",\n",
    "    },\n",
    "    '3d_drug': \"2. Drug_embeddings/EGNN_drug_embeddings_v2.parquet\",\n",
    "    '3d_protein': \"3. Protein_enbeddings/GVP-GNN_protein_embeddings.parquet\",\n",
    "    'adr': \"1. Adr_embeddings/TFIDF_ADR_vectors/\"\n",
    "}\n",
    "\n",
    "# Load selected embeddings\n",
    "print(f\"\\nüìÅ Loading embeddings...\")\n",
    "\n",
    "# 1. Load selected drug embeddings\n",
    "drug_embeddings_df = pd.read_parquet(EMBEDDING_PATHS['drug'][DRUG_ENCODING])\n",
    "print(f\"Drug {DRUG_ENCODING} embeddings loaded: {drug_embeddings_df.shape}\")\n",
    "\n",
    "# 2. Load selected protein embeddings  \n",
    "protein_embeddings_df = pd.read_parquet(EMBEDDING_PATHS['protein'][PROTEIN_ENCODING])\n",
    "print(f\"Protein {PROTEIN_ENCODING} embeddings loaded: {protein_embeddings_df.shape}\")\n",
    "\n",
    "# 3. Load 3D embeddings if enabled\n",
    "if USE_3D_FEATURES:\n",
    "    egnn_drug_df = pd.read_parquet(EMBEDDING_PATHS['3d_drug'])\n",
    "    gvp_protein_df = pd.read_parquet(EMBEDDING_PATHS['3d_protein'])\n",
    "    print(f\"EGNN Drug 3D embeddings loaded: {egnn_drug_df.shape}\")\n",
    "    print(f\"GVP-GNN Protein 3D embeddings loaded: {gvp_protein_df.shape}\")\n",
    "else:\n",
    "    egnn_drug_df = None\n",
    "    gvp_protein_df = None\n",
    "    print(\"3D features disabled\")\n",
    "\n",
    "# 4. Load ADR TF-IDF data\n",
    "print(\"\\nüíä Loading ADR TF-IDF data...\")\n",
    "\n",
    "# Load all three splits \n",
    "adr_train_df = pd.read_parquet(f\"{EMBEDDING_PATHS['adr']}train/tfidf_wide.parquet\")\n",
    "adr_val_df = pd.read_parquet(f\"{EMBEDDING_PATHS['adr']}val/tfidf_wide.parquet\") \n",
    "adr_test_df = pd.read_parquet(f\"{EMBEDDING_PATHS['adr']}test/tfidf_wide.parquet\")\n",
    "\n",
    "print(f\"ADR TF-IDF train: {adr_train_df.shape}\")\n",
    "print(f\"ADR TF-IDF val: {adr_val_df.shape}\")\n",
    "print(f\"ADR TF-IDF test: {adr_test_df.shape}\")\n",
    "\n",
    "# Load global stats\n",
    "import json\n",
    "with open(f\"{EMBEDDING_PATHS['adr']}global_stats.json\", 'r') as f:\n",
    "    adr_stats = json.load(f)\n",
    "\n",
    "print(f\"ADR dimensions: {adr_stats['n_adrs_kept']} (from {adr_stats['n_adrs_original']} original)\")\n",
    "\n",
    "# Combine all splits \n",
    "adr_embeddings_df = pd.concat([adr_train_df, adr_val_df, adr_test_df], ignore_index=True)\n",
    "print(f\"Combined ADR data: {adr_embeddings_df.shape}\")\n",
    "\n",
    "# 5. Calculate embedding dimensions\n",
    "print(f\"\\nüìè Calculating embedding dimensions...\")\n",
    "\n",
    "# Get base dimensions\n",
    "sample_drug_emb = drug_embeddings_df['embedding'].iloc[0]\n",
    "sample_protein_emb = protein_embeddings_df['embedding'].iloc[0]\n",
    "\n",
    "BASE_DRUG_DIM = len(sample_drug_emb)\n",
    "BASE_PROTEIN_DIM = len(sample_protein_emb)\n",
    "ADR_EMBEDDING_DIM = adr_stats['n_adrs_kept']\n",
    "\n",
    "# 3D dimensions\n",
    "if USE_3D_FEATURES:\n",
    "    sample_egnn_drug = egnn_drug_df['embedding'].iloc[0]\n",
    "    sample_gvp_protein = gvp_protein_df['embedding'].iloc[0]\n",
    "    EGNN_DRUG_3D_DIM = len(sample_egnn_drug)\n",
    "    GVP_PROTEIN_3D_DIM = len(sample_gvp_protein)\n",
    "else:\n",
    "    EGNN_DRUG_3D_DIM = 0\n",
    "    GVP_PROTEIN_3D_DIM = 0\n",
    "\n",
    "# Total dimensions\n",
    "DRUG_EMBEDDING_DIM = BASE_DRUG_DIM + EGNN_DRUG_3D_DIM\n",
    "PROTEIN_EMBEDDING_DIM = BASE_PROTEIN_DIM + GVP_PROTEIN_3D_DIM\n",
    "SHARED_DIM = 512\n",
    "\n",
    "print(f\"\\nüìä EMBEDDING DIMENSIONS:\")\n",
    "print(f\"Drug ({DRUG_ENCODING}): {BASE_DRUG_DIM} + {EGNN_DRUG_3D_DIM} (3D) = {DRUG_EMBEDDING_DIM}\")\n",
    "print(f\"Protein ({PROTEIN_ENCODING}): {BASE_PROTEIN_DIM} + {GVP_PROTEIN_3D_DIM} (3D) = {PROTEIN_EMBEDDING_DIM}\")\n",
    "print(f\"ADR (TF-IDF): {ADR_EMBEDDING_DIM}\")\n",
    "print(f\"Shared space: {SHARED_DIM}\")\n",
    "\n",
    "# ID column mapping\n",
    "ID_COLUMNS = {\n",
    "    'drug': 'drug_chembl_id',\n",
    "    'protein': 'id' if PROTEIN_ENCODING == 'esm' else 'target_uniprot_id',\n",
    "    '3d_drug': 'drug_chembl_id',\n",
    "    '3d_protein': 'uniprot_id'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d455713b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Preparing data for training...\n",
      "Available embeddings: 1028 drugs, 2385 proteins\n",
      "Base embeddings extracted:\n",
      "  Drug: (1028, 384)\n",
      "  Protein: (2385, 1280)\n",
      "  ADR: (1028, 4048)\n",
      "\n",
      "üß¨ Adding 3D features...\n",
      "  3D Drug success: 1028/1028 (100.0%)\n",
      "  3D Protein success: 2385/2385 (100.0%)\n",
      "  Enhanced drug embeddings: (1028, 640)\n",
      "  Enhanced protein embeddings: (2385, 2304)\n",
      "\n",
      "Filtered dataset: (34741, 7)\n",
      "  3D Drug success: 1028/1028 (100.0%)\n",
      "  3D Protein success: 2385/2385 (100.0%)\n",
      "  Enhanced drug embeddings: (1028, 640)\n",
      "  Enhanced protein embeddings: (2385, 2304)\n",
      "\n",
      "Filtered dataset: (34741, 7)\n",
      "Final dataset after ADR filtering: 34,741 samples\n",
      "Sample embeddings:\n",
      "  Drug: (34741, 640)\n",
      "  Protein: (34741, 2304)\n",
      "  ADR: (34741, 4048)\n",
      "Final dataset after ADR filtering: 34,741 samples\n",
      "Sample embeddings:\n",
      "  Drug: (34741, 640)\n",
      "  Protein: (34741, 2304)\n",
      "  ADR: (34741, 4048)\n",
      "\n",
      "Label statistics:\n",
      "  DTI positive rate: 0.352\n",
      "  ADR threshold: 0.1245\n",
      "  Average ADR labels per sample: 17.49\n",
      "\n",
      "‚úÖ Data preparation complete!\n",
      "Ready for model training with chemberta + esm + 3D\n",
      "\n",
      "Label statistics:\n",
      "  DTI positive rate: 0.352\n",
      "  ADR threshold: 0.1245\n",
      "  Average ADR labels per sample: 17.49\n",
      "\n",
      "‚úÖ Data preparation complete!\n",
      "Ready for model training with chemberta + esm + 3D\n"
     ]
    }
   ],
   "source": [
    "# DATA PREPARATION AND COMBINATION\n",
    "print(f\"\\nüîÑ Preparing data for training...\")\n",
    "\n",
    "# Create ID mappings\n",
    "drug_ids = drug_embeddings_df[ID_COLUMNS['drug']].values\n",
    "protein_ids = protein_embeddings_df[ID_COLUMNS['protein']].values\n",
    "\n",
    "drug_id_to_idx = {drug_id: idx for idx, drug_id in enumerate(drug_ids)}\n",
    "protein_id_to_idx = {protein_id: idx for idx, protein_id in enumerate(protein_ids)}\n",
    "\n",
    "print(f\"Available embeddings: {len(drug_ids)} drugs, {len(protein_ids)} proteins\")\n",
    "\n",
    "# Extract embedding matrices\n",
    "drug_embedding_matrix = np.vstack(drug_embeddings_df['embedding'].values).astype(np.float32)\n",
    "protein_embedding_matrix = np.vstack(protein_embeddings_df['embedding'].values).astype(np.float32)\n",
    "adr_embedding_matrix = adr_embeddings_df.iloc[:, 1:].values.astype(np.float32)\n",
    "\n",
    "print(f\"Base embeddings extracted:\")\n",
    "print(f\"  Drug: {drug_embedding_matrix.shape}\")\n",
    "print(f\"  Protein: {protein_embedding_matrix.shape}\")\n",
    "print(f\"  ADR: {adr_embedding_matrix.shape}\")\n",
    "\n",
    "# Add 3D features if enabled\n",
    "if USE_3D_FEATURES:\n",
    "    print(\"\\nüß¨ Adding 3D features...\")\n",
    "    \n",
    "    # Drug 3D features (EGNN)\n",
    "    drug_3d_dict = {}\n",
    "    for _, row in egnn_drug_df.iterrows():\n",
    "        drug_id = row[ID_COLUMNS['3d_drug']]\n",
    "        embedding = np.array(row['embedding'], dtype=np.float32)\n",
    "        drug_3d_dict[drug_id] = embedding\n",
    "    \n",
    "    drug_3d_matrix = []\n",
    "    drug_3d_success = 0\n",
    "    for drug_id in drug_ids:\n",
    "        if drug_id in drug_3d_dict:\n",
    "            drug_3d_matrix.append(drug_3d_dict[drug_id])\n",
    "            drug_3d_success += 1\n",
    "        else:\n",
    "            drug_3d_matrix.append(np.zeros(EGNN_DRUG_3D_DIM, dtype=np.float32))\n",
    "    drug_3d_matrix = np.array(drug_3d_matrix)\n",
    "    \n",
    "    # Protein 3D features (GVP-GNN)\n",
    "    protein_3d_dict = {}\n",
    "    for _, row in gvp_protein_df.iterrows():\n",
    "        protein_id = row[ID_COLUMNS['3d_protein']]\n",
    "        embedding = np.array(row['embedding'], dtype=np.float32)\n",
    "        protein_3d_dict[protein_id] = embedding\n",
    "    \n",
    "    protein_3d_matrix = []\n",
    "    protein_3d_success = 0\n",
    "    for protein_id in protein_ids:\n",
    "        if protein_id in protein_3d_dict:\n",
    "            protein_3d_matrix.append(protein_3d_dict[protein_id])\n",
    "            protein_3d_success += 1\n",
    "        else:\n",
    "            protein_3d_matrix.append(np.zeros(GVP_PROTEIN_3D_DIM, dtype=np.float32))\n",
    "    protein_3d_matrix = np.array(protein_3d_matrix)\n",
    "    \n",
    "    # Combine with base embeddings\n",
    "    drug_embedding_matrix = np.concatenate([drug_embedding_matrix, drug_3d_matrix], axis=1)\n",
    "    protein_embedding_matrix = np.concatenate([protein_embedding_matrix, protein_3d_matrix], axis=1)\n",
    "    \n",
    "    print(f\"  3D Drug success: {drug_3d_success}/{len(drug_ids)} ({100*drug_3d_success/len(drug_ids):.1f}%)\")\n",
    "    print(f\"  3D Protein success: {protein_3d_success}/{len(protein_ids)} ({100*protein_3d_success/len(protein_ids):.1f}%)\")\n",
    "    print(f\"  Enhanced drug embeddings: {drug_embedding_matrix.shape}\")\n",
    "    print(f\"  Enhanced protein embeddings: {protein_embedding_matrix.shape}\")\n",
    "\n",
    "# Filter main dataset to only include available embeddings\n",
    "main_df_filtered = main_df[\n",
    "    (main_df['drug_chembl_id'].isin(drug_ids)) & \n",
    "    (main_df['target_uniprot_id'].isin(protein_ids))\n",
    "].copy()\n",
    "\n",
    "print(f\"\\nFiltered dataset: {main_df_filtered.shape}\")\n",
    "\n",
    "# Map IDs to indices\n",
    "main_df_filtered['drug_idx'] = main_df_filtered['drug_chembl_id'].map(drug_id_to_idx)\n",
    "main_df_filtered['protein_idx'] = main_df_filtered['target_uniprot_id'].map(protein_id_to_idx)\n",
    "\n",
    "# Get corresponding embeddings for each sample\n",
    "sample_drug_embeddings = drug_embedding_matrix[main_df_filtered['drug_idx'].values]\n",
    "sample_protein_embeddings = protein_embedding_matrix[main_df_filtered['protein_idx'].values]\n",
    "\n",
    "# Map to ADR embeddings using rxcui\n",
    "adr_drug_ids = adr_embeddings_df['rxcui'].values\n",
    "adr_drug_to_idx = {drug_id: idx for idx, drug_id in enumerate(adr_drug_ids)}\n",
    "main_df_filtered['adr_idx'] = main_df_filtered['rxcui'].map(adr_drug_to_idx)\n",
    "\n",
    "# Filter out samples without ADR mapping\n",
    "valid_mask = main_df_filtered['adr_idx'].notna()\n",
    "main_df_filtered = main_df_filtered[valid_mask].copy()\n",
    "sample_drug_embeddings = sample_drug_embeddings[valid_mask]\n",
    "sample_protein_embeddings = sample_protein_embeddings[valid_mask]\n",
    "\n",
    "sample_adr_embeddings = adr_embedding_matrix[main_df_filtered['adr_idx'].values.astype(int)]\n",
    "\n",
    "print(f\"Final dataset after ADR filtering: {len(main_df_filtered):,} samples\")\n",
    "print(f\"Sample embeddings:\")\n",
    "print(f\"  Drug: {sample_drug_embeddings.shape}\")\n",
    "print(f\"  Protein: {sample_protein_embeddings.shape}\")\n",
    "print(f\"  ADR: {sample_adr_embeddings.shape}\")\n",
    "\n",
    "# Prepare labels\n",
    "dti_labels = main_df_filtered['label'].values.astype(np.float32)\n",
    "\n",
    "# ADR labels with adaptive threshold\n",
    "adr_values = sample_adr_embeddings.flatten()\n",
    "adr_nonzero = adr_values[adr_values > 0]\n",
    "adr_threshold = np.percentile(adr_nonzero, 80) if len(adr_nonzero) > 0 else 0.1\n",
    "adr_labels = (sample_adr_embeddings > adr_threshold).astype(np.float32)\n",
    "\n",
    "dti_positive_rate = dti_labels.mean()\n",
    "adr_avg_labels = adr_labels.sum(axis=1).mean()\n",
    "\n",
    "print(f\"\\nLabel statistics:\")\n",
    "print(f\"  DTI positive rate: {dti_positive_rate:.3f}\")\n",
    "print(f\"  ADR threshold: {adr_threshold:.4f}\")\n",
    "print(f\"  Average ADR labels per sample: {adr_avg_labels:.2f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data preparation complete!\")\n",
    "print(f\"Ready for model training with {DRUG_ENCODING} + {PROTEIN_ENCODING} + {'3D' if USE_3D_FEATURES else 'no3D'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e51c44cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìù TO ADD YOUR NEW ENCODINGS:\n",
      "==================================================\n",
      "1. Add your new encoding files to the available_embeddings dictionary:\n",
      "   embedding_config.available_embeddings['drug']['your_new_encoding'] = 'path/to/your/file.parquet'\n",
      "   embedding_config.available_embeddings['protein']['your_new_encoding'] = 'path/to/your/file.parquet'\n",
      "\n",
      "2. Example of adding new encodings:\n",
      "# Add new drug encodings\n",
      "embedding_config.available_embeddings['drug']['morgan_fp'] = 'path/to/morgan_fingerprints.parquet'\n",
      "embedding_config.available_embeddings['drug']['desc_2d'] = 'path/to/2d_descriptors.parquet'\n",
      "\n",
      "# Add new protein encodings\n",
      "embedding_config.available_embeddings['protein']['unirep'] = 'path/to/unirep_embeddings.parquet'\n",
      "embedding_config.available_embeddings['protein']['protbert'] = 'path/to/protbert_embeddings.parquet'\n",
      "\n",
      "üöÄ READY TO RUN EXPERIMENTS!\n",
      "==================================================\n",
      "Now you can run experiments with any combination:\n",
      "\n",
      "# Example 1: Run current best combination\n",
      "result1 = run_complete_experiment('smiles2vec', 'esm', use_3d=True, num_epochs=20)\n",
      "\n",
      "# Example 2: Test without 3D features\n",
      "result2 = run_complete_experiment('smiles2vec', 'esm', use_3d=False, num_epochs=20)\n",
      "\n",
      "# Example 3: When you add new encodings, test them\n",
      "# result3 = run_complete_experiment('morgan_fp', 'esm', use_3d=True, num_epochs=20)\n",
      "# result4 = run_complete_experiment('smiles2vec', 'protbert', use_3d=True, num_epochs=20)\n",
      "\n",
      "üîç COMPARE RESULTS:\n",
      "==================================================\n",
      "# After running multiple experiments, compare them:\n",
      "results_files = ['results_file1.json', 'results_file2.json', 'results_file3.json']\n",
      "comparison = create_comparison_report(results_files)\n",
      "\n",
      "üìä JSON RESULTS STRUCTURE:\n",
      "==================================================\n",
      "Each experiment saves a JSON file with:\n",
      "- config: experiment configuration\n",
      "- train_history: metrics for each training epoch\n",
      "- val_history: validation metrics for each epoch\n",
      "- test_results: final test set performance\n",
      "- best_epoch: epoch with best validation performance\n",
      "- best_val_accuracy: best validation accuracy achieved\n",
      "\n",
      "‚ú® KEY BENEFITS:\n",
      "==================================================\n",
      "‚úÖ Modular: Easy to swap any encoding combination\n",
      "‚úÖ Automatic tracking: All metrics saved automatically\n",
      "‚úÖ Detailed epochs: Accuracy, AUC, F1, Precision, Recall per epoch\n",
      "‚úÖ Easy comparison: JSON files can be loaded and compared\n",
      "‚úÖ Reproducible: All configurations saved with results\n",
      "‚úÖ Extensible: Easy to add new encodings and metrics\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "==================================================\n",
      "1. Add paths to your new encoding files in the available_embeddings dictionary\n",
      "2. Run experiments: result = run_complete_experiment('encoding1', 'encoding2', use_3d=True)\n",
      "3. Compare results using the generated JSON files\n",
      "4. Identify the best encoding combination for your task\n",
      "5. Use the best model for deployment!\n",
      "\n",
      "üß™ TESTING MODULAR SYSTEM...\n",
      "‚ùå Error in system test: name 'embedding_config' is not defined\n",
      "Please check the setup and try again.\n"
     ]
    }
   ],
   "source": [
    "# HOW TO ADD YOUR NEW ENCODINGS AND RUN EXPERIMENTS\n",
    "\n",
    "print(\"üìù TO ADD YOUR NEW ENCODINGS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"1. Add your new encoding files to the available_embeddings dictionary:\")\n",
    "print(\"   embedding_config.available_embeddings['drug']['your_new_encoding'] = 'path/to/your/file.parquet'\")\n",
    "print(\"   embedding_config.available_embeddings['protein']['your_new_encoding'] = 'path/to/your/file.parquet'\")\n",
    "print()\n",
    "print(\"2. Example of adding new encodings:\")\n",
    "\n",
    "# Example: Add new encodings (uncomment and modify paths when you have them)\n",
    "# embedding_config.available_embeddings['drug']['morgan_fp'] = \"path/to/morgan_fingerprints.parquet\"\n",
    "# embedding_config.available_embeddings['drug']['desc_2d'] = \"path/to/2d_descriptors.parquet\"\n",
    "# embedding_config.available_embeddings['protein']['unirep'] = \"path/to/unirep_embeddings.parquet\"\n",
    "# embedding_config.available_embeddings['protein']['protbert'] = \"path/to/protbert_embeddings.parquet\"\n",
    "\n",
    "print(\"# Add new drug encodings\")\n",
    "print(\"embedding_config.available_embeddings['drug']['morgan_fp'] = 'path/to/morgan_fingerprints.parquet'\")\n",
    "print(\"embedding_config.available_embeddings['drug']['desc_2d'] = 'path/to/2d_descriptors.parquet'\")\n",
    "print()\n",
    "print(\"# Add new protein encodings\")  \n",
    "print(\"embedding_config.available_embeddings['protein']['unirep'] = 'path/to/unirep_embeddings.parquet'\")\n",
    "print(\"embedding_config.available_embeddings['protein']['protbert'] = 'path/to/protbert_embeddings.parquet'\")\n",
    "print()\n",
    "\n",
    "print(\"üöÄ READY TO RUN EXPERIMENTS!\")\n",
    "print(\"=\"*50)\n",
    "print(\"Now you can run experiments with any combination:\")\n",
    "print()\n",
    "\n",
    "print(\"# Example 1: Run current best combination\")\n",
    "print(\"result1 = run_complete_experiment('smiles2vec', 'esm', use_3d=True, num_epochs=20)\")\n",
    "print()\n",
    "\n",
    "print(\"# Example 2: Test without 3D features\")  \n",
    "print(\"result2 = run_complete_experiment('smiles2vec', 'esm', use_3d=False, num_epochs=20)\")\n",
    "print()\n",
    "\n",
    "print(\"# Example 3: When you add new encodings, test them\")\n",
    "print(\"# result3 = run_complete_experiment('morgan_fp', 'esm', use_3d=True, num_epochs=20)\")\n",
    "print(\"# result4 = run_complete_experiment('smiles2vec', 'protbert', use_3d=True, num_epochs=20)\")\n",
    "print()\n",
    "\n",
    "print(\"üîç COMPARE RESULTS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"# After running multiple experiments, compare them:\")\n",
    "print(\"results_files = ['results_file1.json', 'results_file2.json', 'results_file3.json']\")\n",
    "print(\"comparison = create_comparison_report(results_files)\")\n",
    "print()\n",
    "\n",
    "print(\"üìä JSON RESULTS STRUCTURE:\")\n",
    "print(\"=\"*50)\n",
    "print(\"Each experiment saves a JSON file with:\")\n",
    "print(\"- config: experiment configuration\")\n",
    "print(\"- train_history: metrics for each training epoch\")\n",
    "print(\"- val_history: validation metrics for each epoch\")  \n",
    "print(\"- test_results: final test set performance\")\n",
    "print(\"- best_epoch: epoch with best validation performance\")\n",
    "print(\"- best_val_accuracy: best validation accuracy achieved\")\n",
    "print()\n",
    "\n",
    "print(\"‚ú® KEY BENEFITS:\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚úÖ Modular: Easy to swap any encoding combination\")\n",
    "print(\"‚úÖ Automatic tracking: All metrics saved automatically\")\n",
    "print(\"‚úÖ Detailed epochs: Accuracy, AUC, F1, Precision, Recall per epoch\")\n",
    "print(\"‚úÖ Easy comparison: JSON files can be loaded and compared\")\n",
    "print(\"‚úÖ Reproducible: All configurations saved with results\")\n",
    "print(\"‚úÖ Extensible: Easy to add new encodings and metrics\")\n",
    "print()\n",
    "\n",
    "print(\"üéØ NEXT STEPS:\")\n",
    "print(\"=\"*50) \n",
    "print(\"1. Add paths to your new encoding files in the available_embeddings dictionary\")\n",
    "print(\"2. Run experiments: result = run_complete_experiment('encoding1', 'encoding2', use_3d=True)\")\n",
    "print(\"3. Compare results using the generated JSON files\")\n",
    "print(\"4. Identify the best encoding combination for your task\")\n",
    "print(\"5. Use the best model for deployment!\")\n",
    "\n",
    "# Run a quick test to make sure everything works\n",
    "print(\"\\nüß™ TESTING MODULAR SYSTEM...\")\n",
    "try:\n",
    "    # Test configuration creation\n",
    "    test_config = embedding_config.create_experiment_config('smiles2vec', 'esm', use_3d=True)\n",
    "    print(f\"‚úÖ Configuration system working: {test_config['experiment_name']}\")\n",
    "    \n",
    "    # Test results tracker\n",
    "    test_tracker = ResultsTracker(test_config)\n",
    "    print(\"‚úÖ Results tracking system working\")\n",
    "    \n",
    "    print(\"üéâ ALL SYSTEMS READY! You can now run experiments with different encoding combinations.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in system test: {e}\")\n",
    "    print(\"Please check the setup and try again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dff3746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING NEW 3D EMBEDDING FILES ===\n",
      "Error loading EGNN drug embeddings: [Errno 2] No such file or directory: 'EGNN_drug_embeddings_v2.parquet'\n",
      "\n",
      "==================================================\n",
      "Error loading GVP-GNN protein embeddings: [Errno 2] No such file or directory: 'GVP-GNN_protein_embeddings.parquet'\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Check the structure of new 3D embeddings - EGNN and GVP-GNN\n",
    "print(\"=== CHECKING NEW 3D EMBEDDING FILES ===\")\n",
    "\n",
    "# Check EGNN drug embeddings\n",
    "try:\n",
    "    egnn_drug_df = pd.read_parquet(\"EGNN_drug_embeddings_v2.parquet\")\n",
    "    print(f\"EGNN Drug embeddings shape: {egnn_drug_df.shape}\")\n",
    "    print(f\"EGNN Drug columns: {egnn_drug_df.columns.tolist()}\")\n",
    "    print(f\"EGNN Drug sample:\")\n",
    "    print(egnn_drug_df.head(2))\n",
    "    \n",
    "    # Check embedding dimension\n",
    "    if 'embedding' in egnn_drug_df.columns:\n",
    "        sample_egnn = egnn_drug_df['embedding'].iloc[0]\n",
    "        print(f\"EGNN embedding dimension: {len(sample_egnn) if hasattr(sample_egnn, '__len__') else 'scalar'}\")\n",
    "    elif len(egnn_drug_df.columns) > 1:\n",
    "        # If embeddings are in separate columns\n",
    "        embedding_cols = [col for col in egnn_drug_df.columns if col not in ['drug_chembl_id', 'drug_id']]\n",
    "        print(f\"EGNN embedding dimension (from columns): {len(embedding_cols)}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading EGNN drug embeddings: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Check GVP-GNN protein embeddings  \n",
    "try:\n",
    "    gvp_protein_df = pd.read_parquet(\"GVP-GNN_protein_embeddings.parquet\")\n",
    "    print(f\"GVP-GNN Protein embeddings shape: {gvp_protein_df.shape}\")\n",
    "    print(f\"GVP-GNN Protein columns: {gvp_protein_df.columns.tolist()}\")\n",
    "    print(f\"GVP-GNN Protein sample:\")\n",
    "    print(gvp_protein_df.head(2))\n",
    "    \n",
    "    # Check embedding dimension\n",
    "    if 'embedding' in gvp_protein_df.columns:\n",
    "        sample_gvp = gvp_protein_df['embedding'].iloc[0]\n",
    "        print(f\"GVP-GNN embedding dimension: {len(sample_gvp) if hasattr(sample_gvp, '__len__') else 'scalar'}\")\n",
    "    elif len(gvp_protein_df.columns) > 1:\n",
    "        # If embeddings are in separate columns\n",
    "        embedding_cols = [col for col in gvp_protein_df.columns if col not in ['target_uniprot_id', 'protein_id']]\n",
    "        print(f\"GVP-GNN embedding dimension (from columns): {len(embedding_cols)}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error loading GVP-GNN protein embeddings: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc80e633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üß† Creating model...\n",
      "Architecture:\n",
      "  Drug -> Shared: 640 -> 512\n",
      "  Protein -> Shared: 2304 -> 512\n",
      "  ADR -> Shared: 4048 -> 512\n",
      "  DTI Head: 1024 -> 1\n",
      "  ADR Head: 1024 -> 4048\n"
     ]
    }
   ],
   "source": [
    "# MODEL ARCHITECTURE\n",
    "class ProjectionHead(nn.Module):\n",
    "    \"\"\"Maps embeddings to shared latent space\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, output_dim, dropout=0.2):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Linear(input_dim, output_dim * 2),\n",
    "            nn.LayerNorm(output_dim * 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(output_dim * 2, output_dim),\n",
    "            nn.LayerNorm(output_dim)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.projection(x), dim=1)\n",
    "\n",
    "class DTIHead(nn.Module):\n",
    "    \"\"\"Drug-Target Interaction prediction (binary classification)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(DTIHead, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, drug_emb, protein_emb):\n",
    "        combined = torch.cat([drug_emb, protein_emb], dim=1)\n",
    "        return self.classifier(combined)\n",
    "\n",
    "class ADRHead(nn.Module):\n",
    "    \"\"\"Adverse Drug Reaction prediction (multi-label classification)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim, num_adr_labels, dropout=0.3):\n",
    "        super(ADRHead, self).__init__()\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(128, num_adr_labels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, drug_emb, protein_emb=None):\n",
    "        if protein_emb is not None:\n",
    "            input_features = torch.cat([drug_emb, protein_emb], dim=1)\n",
    "        else:\n",
    "            input_features = drug_emb\n",
    "        return self.classifier(input_features)\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"InfoNCE contrastive loss for cross-modal alignment\"\"\"\n",
    "    \n",
    "    def __init__(self, temperature=0.1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        \n",
    "    def forward(self, embeddings1, embeddings2):\n",
    "        batch_size = embeddings1.size(0)\n",
    "        sim_matrix = torch.matmul(embeddings1, embeddings2.T) / self.temperature\n",
    "        labels = torch.arange(batch_size).to(embeddings1.device)\n",
    "        return F.cross_entropy(sim_matrix, labels)\n",
    "\n",
    "class MultimodalDTIADRModel(nn.Module):\n",
    "    \"\"\"Complete multimodal DTI-ADR model\"\"\"\n",
    "    \n",
    "    def __init__(self, drug_dim, protein_dim, adr_dim, shared_dim, num_adr_labels):\n",
    "        super(MultimodalDTIADRModel, self).__init__()\n",
    "        \n",
    "        # Projection heads\n",
    "        self.drug_projection = ProjectionHead(drug_dim, shared_dim)\n",
    "        self.protein_projection = ProjectionHead(protein_dim, shared_dim)\n",
    "        self.adr_projection = ProjectionHead(adr_dim, shared_dim)\n",
    "        \n",
    "        # Task heads\n",
    "        self.dti_head = DTIHead(shared_dim * 2)\n",
    "        self.adr_head = ADRHead(shared_dim * 2, num_adr_labels)\n",
    "        \n",
    "        # Contrastive loss\n",
    "        self.contrastive_loss = ContrastiveLoss()\n",
    "        \n",
    "    def forward(self, drug_emb, protein_emb, adr_emb, mode='train'):\n",
    "        # Project to shared space\n",
    "        drug_shared = self.drug_projection(drug_emb)\n",
    "        protein_shared = self.protein_projection(protein_emb)\n",
    "        adr_shared = self.adr_projection(adr_emb)\n",
    "        \n",
    "        # Task predictions\n",
    "        dti_pred = self.dti_head(drug_shared, protein_shared)\n",
    "        adr_pred = self.adr_head(drug_shared, protein_shared)\n",
    "        \n",
    "        outputs = {\n",
    "            'dti_pred': dti_pred,\n",
    "            'adr_pred': adr_pred,\n",
    "            'drug_shared': drug_shared,\n",
    "            'protein_shared': protein_shared,\n",
    "            'adr_shared': adr_shared\n",
    "        }\n",
    "        \n",
    "        # Add contrastive losses during training\n",
    "        if mode == 'train':\n",
    "            outputs['contrastive_dp'] = self.contrastive_loss(drug_shared, protein_shared)\n",
    "            outputs['contrastive_da'] = self.contrastive_loss(drug_shared, adr_shared)\n",
    "            \n",
    "        return outputs\n",
    "\n",
    "print(f\"\\nüß† Creating model...\")\n",
    "print(f\"Architecture:\")\n",
    "print(f\"  Drug -> Shared: {DRUG_EMBEDDING_DIM} -> {SHARED_DIM}\")\n",
    "print(f\"  Protein -> Shared: {PROTEIN_EMBEDDING_DIM} -> {SHARED_DIM}\")\n",
    "print(f\"  ADR -> Shared: {ADR_EMBEDDING_DIM} -> {SHARED_DIM}\")\n",
    "print(f\"  DTI Head: {SHARED_DIM * 2} -> 1\")\n",
    "print(f\"  ADR Head: {SHARED_DIM * 2} -> {ADR_EMBEDDING_DIM}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e8e7e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Model created:\n",
      "  Total parameters: 9,860,945\n",
      "  Device: cuda\n",
      "  ‚úÖ Model architecture ready!\n",
      "\n",
      "‚öñÔ∏è Training setup:\n",
      "  DTI positive rate: 0.352\n",
      "  DTI positive weight: 1.840\n",
      "  Optimizer: AdamW (lr=5e-4, wd=1e-4)\n",
      "  Scheduler: ReduceLROnPlateau\n",
      "\n",
      "üöÄ Ready to train chemberta + esm + 3D!\n"
     ]
    }
   ],
   "source": [
    "# CREATE AND TEST MODEL\n",
    "model = MultimodalDTIADRModel(\n",
    "    drug_dim=DRUG_EMBEDDING_DIM,\n",
    "    protein_dim=PROTEIN_EMBEDDING_DIM,\n",
    "    adr_dim=ADR_EMBEDDING_DIM,\n",
    "    shared_dim=SHARED_DIM,\n",
    "    num_adr_labels=ADR_EMBEDDING_DIM\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nü§ñ Model created:\")\n",
    "print(f\"  Total parameters: {total_params:,}\")\n",
    "print(f\"  Device: {device}\")\n",
    "\n",
    "# Model will be tested after data loaders are created\n",
    "print(f\"  ‚úÖ Model architecture ready!\")\n",
    "\n",
    "# Setup optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "\n",
    "# Loss functions\n",
    "dti_criterion = nn.BCELoss()\n",
    "adr_criterion = nn.BCELoss()\n",
    "\n",
    "# Calculate class weights for DTI imbalance\n",
    "pos_weight = (1 - dti_positive_rate) / dti_positive_rate\n",
    "print(f\"\\n‚öñÔ∏è Training setup:\")\n",
    "print(f\"  DTI positive rate: {dti_positive_rate:.3f}\")\n",
    "print(f\"  DTI positive weight: {pos_weight:.3f}\")\n",
    "print(f\"  Optimizer: AdamW (lr=5e-4, wd=1e-4)\")\n",
    "print(f\"  Scheduler: ReduceLROnPlateau\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to train {DRUG_ENCODING} + {PROTEIN_ENCODING} + {'3D' if USE_3D_FEATURES else 'no3D'}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c698468f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Creating dataset splits...\n",
      "Split sizes - Train: 24,318, Val: 5,211, Test: 5,212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data loaders created!\n",
      "   Batch size: 64\n",
      "   Train batches: 380\n",
      "   Val batches: 82\n",
      "   Test batches: 82\n",
      "\n",
      "üß™ Testing model with sample batch...\n",
      "  DTI predictions shape: torch.Size([64, 1])\n",
      "  ADR predictions shape: torch.Size([64, 4048])\n",
      "  ‚úÖ Model working correctly!\n",
      "\n",
      "üöÄ Ready to train chemberta + esm + 3D!\n"
     ]
    }
   ],
   "source": [
    "# DATASET AND DATA LOADERS\n",
    "class DTIADRDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for DTI-ADR multimodal data\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dict):\n",
    "        self.drug_embeddings = torch.FloatTensor(data_dict['drug_embeddings'])\n",
    "        self.protein_embeddings = torch.FloatTensor(data_dict['protein_embeddings'])\n",
    "        self.adr_embeddings = torch.FloatTensor(data_dict['adr_embeddings'])\n",
    "        self.dti_labels = torch.FloatTensor(data_dict['dti_labels'])\n",
    "        self.adr_labels = torch.FloatTensor(data_dict['adr_labels'])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dti_labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'drug_embedding': self.drug_embeddings[idx],\n",
    "            'protein_embedding': self.protein_embeddings[idx],\n",
    "            'adr_embedding': self.adr_embeddings[idx],\n",
    "            'dti_label': self.dti_labels[idx],\n",
    "            'adr_label': self.adr_labels[idx]\n",
    "        }\n",
    "\n",
    "# Create train/validation/test splits\n",
    "print(\"üîÑ Creating dataset splits...\")\n",
    "\n",
    "# Create data dictionary from prepared arrays\n",
    "dataset_dict = {\n",
    "    'drug_embeddings': sample_drug_embeddings,\n",
    "    'protein_embeddings': sample_protein_embeddings, \n",
    "    'adr_embeddings': sample_adr_embeddings,\n",
    "    'dti_labels': dti_labels.reshape(-1, 1),\n",
    "    'adr_labels': adr_labels\n",
    "}\n",
    "\n",
    "# Split indices\n",
    "total_samples = len(dataset_dict['dti_labels'])\n",
    "train_size = int(0.7 * total_samples)\n",
    "val_size = int(0.15 * total_samples)\n",
    "test_size = total_samples - train_size - val_size\n",
    "\n",
    "indices = np.random.permutation(total_samples)\n",
    "train_indices = indices[:train_size]\n",
    "val_indices = indices[train_size:train_size + val_size]\n",
    "test_indices = indices[train_size + val_size:]\n",
    "\n",
    "print(f\"Split sizes - Train: {len(train_indices):,}, Val: {len(val_indices):,}, Test: {len(test_indices):,}\")\n",
    "\n",
    "# Create data dictionaries for each split\n",
    "def create_split_data(indices, data_dict):\n",
    "    return {\n",
    "        'drug_embeddings': data_dict['drug_embeddings'][indices],\n",
    "        'protein_embeddings': data_dict['protein_embeddings'][indices],\n",
    "        'adr_embeddings': data_dict['adr_embeddings'][indices],\n",
    "        'dti_labels': data_dict['dti_labels'][indices],\n",
    "        'adr_labels': data_dict['adr_labels'][indices]\n",
    "    }\n",
    "\n",
    "train_data = create_split_data(train_indices, dataset_dict)\n",
    "val_data = create_split_data(val_indices, dataset_dict)\n",
    "test_data = create_split_data(test_indices, dataset_dict)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = DTIADRDataset(train_data)\n",
    "val_dataset = DTIADRDataset(val_data) \n",
    "test_dataset = DTIADRDataset(test_data)\n",
    "\n",
    "# Create data loaders\n",
    "BATCH_SIZE = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"‚úÖ Data loaders created!\")\n",
    "print(f\"   Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   Train batches: {len(train_loader)}\")\n",
    "print(f\"   Val batches: {len(val_loader)}\")\n",
    "print(f\"   Test batches: {len(test_loader)}\")\n",
    "\n",
    "# Test model with a sample batch\n",
    "print(\"\\nüß™ Testing model with sample batch...\")\n",
    "with torch.no_grad():\n",
    "    test_batch = next(iter(train_loader))\n",
    "    outputs = model(\n",
    "        test_batch['drug_embedding'].to(device),\n",
    "        test_batch['protein_embedding'].to(device),\n",
    "        test_batch['adr_embedding'].to(device)\n",
    "    )\n",
    "    print(f\"  DTI predictions shape: {outputs['dti_pred'].shape}\")\n",
    "    print(f\"  ADR predictions shape: {outputs['adr_pred'].shape}\")\n",
    "    print(f\"  ‚úÖ Model working correctly!\")\n",
    "\n",
    "print(f\"\\nüöÄ Ready to train {DRUG_ENCODING} + {PROTEIN_ENCODING} + {'3D' if USE_3D_FEATURES else 'no3D'}!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "decd14b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Training function ready!\n",
      "Usage: history, results_file = train_model(model, train_loader, val_loader, num_epochs=50)\n"
     ]
    }
   ],
   "source": [
    "# TRAINING FUNCTION WITH DETAILED TRACKING\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, save_results=True):\n",
    "    \"\"\"Train model with detailed metrics tracking and automatic result saving\"\"\"\n",
    "    \n",
    "    # Setup optimizer and loss functions\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n",
    "    dti_criterion = nn.BCELoss()\n",
    "    adr_criterion = nn.BCELoss()\n",
    "    \n",
    "    # Create experiment name and results directory\n",
    "    experiment_name = f\"{DRUG_ENCODING}_{PROTEIN_ENCODING}_{'3D' if USE_3D_FEATURES else 'no3D'}\"\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    results_dir = f\"results_{experiment_name}_{timestamp}\"\n",
    "    os.makedirs(results_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"üöÄ Starting training: {experiment_name}\")\n",
    "    print(f\"üìÅ Results will be saved to: {results_dir}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # Initialize tracking\n",
    "    training_history = {\n",
    "        'experiment_name': experiment_name,\n",
    "        'config': {\n",
    "            'drug_encoding': DRUG_ENCODING,\n",
    "            'protein_encoding': PROTEIN_ENCODING,\n",
    "            'use_3d': USE_3D_FEATURES,\n",
    "            'drug_dim': DRUG_EMBEDDING_DIM,\n",
    "            'protein_dim': PROTEIN_EMBEDDING_DIM,\n",
    "            'adr_dim': ADR_EMBEDDING_DIM,\n",
    "            'shared_dim': SHARED_DIM,\n",
    "            'batch_size': BATCH_SIZE,\n",
    "            'num_epochs': num_epochs\n",
    "        },\n",
    "        'train_history': [],\n",
    "        'val_history': [],\n",
    "        'best_epoch': 0,\n",
    "        'best_val_accuracy': 0.0\n",
    "    }\n",
    "    \n",
    "    best_val_accuracy = 0.0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_losses = {'dti': 0, 'adr': 0, 'contrastive': 0, 'total': 0}\n",
    "        train_dti_preds, train_dti_labels = [], []\n",
    "        \n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\", leave=False):\n",
    "            drug_emb = batch['drug_embedding'].to(device)\n",
    "            protein_emb = batch['protein_embedding'].to(device)\n",
    "            adr_emb = batch['adr_embedding'].to(device)\n",
    "            dti_labels_batch = batch['dti_label'].to(device)\n",
    "            adr_labels_batch = batch['adr_label'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(drug_emb, protein_emb, adr_emb, mode='train')\n",
    "            \n",
    "            # Compute losses\n",
    "            dti_loss = dti_criterion(outputs['dti_pred'], dti_labels_batch)\n",
    "            adr_loss = adr_criterion(outputs['adr_pred'], adr_labels_batch)\n",
    "            contrastive_loss = outputs.get('contrastive_dp', 0) + outputs.get('contrastive_da', 0)\n",
    "            \n",
    "            total_loss = dti_loss + 0.5 * adr_loss + 0.5 * contrastive_loss\n",
    "            \n",
    "            total_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track losses\n",
    "            train_losses['dti'] += dti_loss.item()\n",
    "            train_losses['adr'] += adr_loss.item()\n",
    "            train_losses['contrastive'] += contrastive_loss.item() if isinstance(contrastive_loss, torch.Tensor) else contrastive_loss\n",
    "            train_losses['total'] += total_loss.item()\n",
    "            \n",
    "            # Collect predictions for metrics\n",
    "            train_dti_preds.extend(outputs['dti_pred'].detach().cpu().numpy())\n",
    "            train_dti_labels.extend(dti_labels_batch.detach().cpu().numpy())\n",
    "        \n",
    "        # Calculate training metrics\n",
    "        train_dti_preds = np.array(train_dti_preds).flatten()\n",
    "        train_dti_labels = np.array(train_dti_labels).flatten()\n",
    "        train_dti_pred_binary = (train_dti_preds > 0.5).astype(int)\n",
    "        \n",
    "        train_accuracy = accuracy_score(train_dti_labels, train_dti_pred_binary)\n",
    "        train_auc = roc_auc_score(train_dti_labels, train_dti_preds) if len(np.unique(train_dti_labels)) > 1 else 0.0\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            train_dti_labels, train_dti_pred_binary, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_losses = {'dti': 0, 'adr': 0, 'total': 0}\n",
    "        val_dti_preds, val_dti_labels = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]  \", leave=False):\n",
    "                drug_emb = batch['drug_embedding'].to(device)\n",
    "                protein_emb = batch['protein_embedding'].to(device)\n",
    "                adr_emb = batch['adr_embedding'].to(device)\n",
    "                dti_labels_batch = batch['dti_label'].to(device)\n",
    "                adr_labels_batch = batch['adr_label'].to(device)\n",
    "                \n",
    "                outputs = model(drug_emb, protein_emb, adr_emb, mode='eval')\n",
    "                \n",
    "                dti_loss = dti_criterion(outputs['dti_pred'], dti_labels_batch)\n",
    "                adr_loss = adr_criterion(outputs['adr_pred'], adr_labels_batch)\n",
    "                total_loss = dti_loss + 0.5 * adr_loss\n",
    "                \n",
    "                val_losses['dti'] += dti_loss.item()\n",
    "                val_losses['adr'] += adr_loss.item()\n",
    "                val_losses['total'] += total_loss.item()\n",
    "                \n",
    "                val_dti_preds.extend(outputs['dti_pred'].cpu().numpy())\n",
    "                val_dti_labels.extend(dti_labels_batch.cpu().numpy())\n",
    "        \n",
    "        # Calculate validation metrics\n",
    "        val_dti_preds = np.array(val_dti_preds).flatten()\n",
    "        val_dti_labels = np.array(val_dti_labels).flatten()\n",
    "        val_dti_pred_binary = (val_dti_preds > 0.5).astype(int)\n",
    "        \n",
    "        val_accuracy = accuracy_score(val_dti_labels, val_dti_pred_binary)\n",
    "        val_auc = roc_auc_score(val_dti_labels, val_dti_preds) if len(np.unique(val_dti_labels)) > 1 else 0.0\n",
    "        val_precision, val_recall, val_f1, _ = precision_recall_fscore_support(\n",
    "            val_dti_labels, val_dti_pred_binary, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # Average losses\n",
    "        train_avg_losses = {k: v/len(train_loader) for k, v in train_losses.items()}\n",
    "        val_avg_losses = {k: v/len(val_loader) for k, v in val_losses.items()}\n",
    "        \n",
    "        # Log metrics\n",
    "        train_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'dti_accuracy': train_accuracy,\n",
    "            'dti_auc': train_auc,\n",
    "            'dti_precision': train_precision,\n",
    "            'dti_recall': train_recall,\n",
    "            'dti_f1': train_f1,\n",
    "            **train_avg_losses\n",
    "        }\n",
    "        \n",
    "        val_metrics = {\n",
    "            'epoch': epoch + 1,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'dti_accuracy': val_accuracy,\n",
    "            'dti_auc': val_auc,\n",
    "            'dti_precision': val_precision,\n",
    "            'dti_recall': val_recall,\n",
    "            'dti_f1': val_f1,\n",
    "            **val_avg_losses\n",
    "        }\n",
    "        \n",
    "        training_history['train_history'].append(train_metrics)\n",
    "        training_history['val_history'].append(val_metrics)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  üéØ DTI Accuracy:  Train {train_accuracy:.4f} | Val {val_accuracy:.4f}\")\n",
    "        print(f\"  üìà AUC-ROC:       Train {train_auc:.4f} | Val {val_auc:.4f}\")\n",
    "        print(f\"  üéõÔ∏è  Precision:     Train {train_precision:.4f} | Val {val_precision:.4f}\")\n",
    "        print(f\"  üîç Recall:        Train {train_recall:.4f} | Val {val_recall:.4f}\")\n",
    "        print(f\"  ‚öñÔ∏è  F1-Score:      Train {train_f1:.4f} | Val {val_f1:.4f}\")\n",
    "        print(f\"  üí• Loss:          Train {train_avg_losses['total']:.4f} | Val {val_avg_losses['total']:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_accuracy)\n",
    "        \n",
    "        # Track best model\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            training_history['best_val_accuracy'] = best_val_accuracy\n",
    "            training_history['best_epoch'] = epoch + 1\n",
    "            print(f\"  üåü NEW BEST VAL ACCURACY: {val_accuracy:.4f} ({val_accuracy*100:.1f}%)\")\n",
    "            \n",
    "            # Save best model\n",
    "            if save_results:\n",
    "                torch.save(model.state_dict(), f\"{results_dir}/best_model.pth\")\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    print(f\"\\nüéâ Training completed!\")\n",
    "    print(f\"   Best validation accuracy: {best_val_accuracy:.4f} ({best_val_accuracy*100:.1f}%)\")\n",
    "    \n",
    "    # Save training history\n",
    "    if save_results:\n",
    "        results_file = f\"{results_dir}/training_results.json\"\n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(training_history, f, indent=4)\n",
    "        print(f\"   üìä Results saved to: {results_file}\")\n",
    "        \n",
    "        return training_history, results_file\n",
    "    else:\n",
    "        return training_history, None\n",
    "\n",
    "print(\"üöÄ Training function ready!\")\n",
    "print(\"Usage: history, results_file = train_model(model, train_loader, val_loader, num_epochs=50)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6745d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ Evaluation functions ready!\n",
      "\n",
      "üí° EXAMPLE USAGE:\n",
      "==================================================\n",
      "# 1. Train model\n",
      "history, results_file = train_model(model, train_loader, val_loader, num_epochs=30)\n",
      "\n",
      "# 2. Evaluate on test set\n",
      "test_results = evaluate_model(model, test_loader)\n",
      "\n",
      "# 3. Compare multiple experiments\n",
      "compare_results(['results_file1.json', 'results_file2.json'])\n",
      "\n",
      "# 4. Change configuration and run again\n",
      "# Just change DRUG_ENCODING or PROTEIN_ENCODING at the top and re-run!\n",
      "\n",
      "üéØ CURRENT EXPERIMENT:\n",
      "   chemberta + esm + 3D\n",
      "   Ready to train!\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION FUNCTION\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    print(f\"üî¨ Evaluating model on test set...\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_dti_preds, all_dti_labels = [], []\n",
    "    all_adr_preds, all_adr_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            drug_emb = batch['drug_embedding'].to(device)\n",
    "            protein_emb = batch['protein_embedding'].to(device)\n",
    "            adr_emb = batch['adr_embedding'].to(device)\n",
    "            dti_labels_batch = batch['dti_label'].to(device)\n",
    "            adr_labels_batch = batch['adr_label'].to(device)\n",
    "            \n",
    "            outputs = model(drug_emb, protein_emb, adr_emb, mode='eval')\n",
    "            \n",
    "            all_dti_preds.extend(outputs['dti_pred'].cpu().numpy())\n",
    "            all_dti_labels.extend(dti_labels_batch.cpu().numpy())\n",
    "            all_adr_preds.extend(outputs['adr_pred'].cpu().numpy())\n",
    "            all_adr_labels.extend(adr_labels_batch.cpu().numpy())\n",
    "    \n",
    "    # DTI Metrics\n",
    "    all_dti_preds = np.array(all_dti_preds).flatten()\n",
    "    all_dti_labels = np.array(all_dti_labels).flatten()\n",
    "    dti_pred_binary = (all_dti_preds > 0.5).astype(int)\n",
    "    \n",
    "    dti_accuracy = accuracy_score(all_dti_labels, dti_pred_binary)\n",
    "    dti_auc = roc_auc_score(all_dti_labels, all_dti_preds) if len(np.unique(all_dti_labels)) > 1 else 0.0\n",
    "    dti_precision, dti_recall, dti_f1, _ = precision_recall_fscore_support(\n",
    "        all_dti_labels, dti_pred_binary, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # ADR Metrics\n",
    "    all_adr_preds = np.array(all_adr_preds)\n",
    "    all_adr_labels = np.array(all_adr_labels)\n",
    "    adr_pred_binary = (all_adr_preds > 0.5).astype(int)\n",
    "    adr_accuracy = accuracy_score(all_adr_labels.flatten(), adr_pred_binary.flatten())\n",
    "    \n",
    "    test_results = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'dti_accuracy': dti_accuracy,\n",
    "        'dti_auc': dti_auc,\n",
    "        'dti_precision': dti_precision,\n",
    "        'dti_recall': dti_recall,\n",
    "        'dti_f1': dti_f1,\n",
    "        'adr_accuracy': adr_accuracy,\n",
    "        'n_test_samples': len(all_dti_labels)\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nüéØ TEST RESULTS:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Drug-Target Interaction (DTI):\")\n",
    "    print(f\"  üéØ Accuracy:  {dti_accuracy:.4f} ({dti_accuracy*100:.1f}%)\")\n",
    "    print(f\"  üìà AUC-ROC:   {dti_auc:.4f}\")\n",
    "    print(f\"  üéõÔ∏è  Precision: {dti_precision:.4f}\")\n",
    "    print(f\"  üîç Recall:    {dti_recall:.4f}\")\n",
    "    print(f\"  ‚öñÔ∏è  F1-Score:  {dti_f1:.4f}\")\n",
    "    print(f\"\\nAdverse Drug Reaction (ADR):\")\n",
    "    print(f\"  üéØ Accuracy:  {adr_accuracy:.4f} ({adr_accuracy*100:.1f}%)\")\n",
    "    print(f\"\\nüìä Dataset: {len(all_dti_labels):,} test samples\")\n",
    "    \n",
    "    return test_results\n",
    "\n",
    "# EXAMPLE USAGE AND COMPARISON FUNCTION\n",
    "def compare_results(results_files):\n",
    "    \"\"\"Compare results from multiple experiments\"\"\"\n",
    "    if not results_files:\n",
    "        print(\"No results files provided\")\n",
    "        return\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for results_file in results_files:\n",
    "        try:\n",
    "            with open(results_file, 'r') as f:\n",
    "                results = json.load(f)\n",
    "            \n",
    "            comparison_data.append({\n",
    "                'experiment': results['experiment_name'],\n",
    "                'drug_encoding': results['config']['drug_encoding'],\n",
    "                'protein_encoding': results['config']['protein_encoding'],\n",
    "                'use_3d': results['config']['use_3d'],\n",
    "                'best_val_accuracy': results['best_val_accuracy'],\n",
    "                'best_epoch': results['best_epoch'],\n",
    "                'file': results_file\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {results_file}: {e}\")\n",
    "    \n",
    "    if comparison_data:\n",
    "        comparison_data.sort(key=lambda x: x['best_val_accuracy'], reverse=True)\n",
    "        \n",
    "        print(\"\\nüèÜ EXPERIMENT COMPARISON:\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Rank':<4} {'Experiment':<30} {'Drug':<12} {'Protein':<8} {'3D':<3} {'Best Acc':<9} {'Epoch':<5}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for i, exp in enumerate(comparison_data, 1):\n",
    "            use_3d = \"‚úì\" if exp['use_3d'] else \"‚úó\"\n",
    "            print(f\"{i:<4} {exp['experiment']:<30} {exp['drug_encoding']:<12} {exp['protein_encoding']:<8} {use_3d:<3} {exp['best_val_accuracy']:.4f}    {exp['best_epoch']:<5}\")\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "print(\"üî¨ Evaluation functions ready!\")\n",
    "print(\"\\nüí° EXAMPLE USAGE:\")\n",
    "print(\"=\"*50)\n",
    "print(\"# 1. Train model\")\n",
    "print(\"history, results_file = train_model(model, train_loader, val_loader, num_epochs=30)\")\n",
    "print()\n",
    "print(\"# 2. Evaluate on test set\")\n",
    "print(\"test_results = evaluate_model(model, test_loader)\")\n",
    "print()\n",
    "print(\"# 3. Compare multiple experiments\")\n",
    "print(\"compare_results(['results_file1.json', 'results_file2.json'])\")\n",
    "print()\n",
    "print(\"# 4. Change configuration and run again\")\n",
    "print(\"# Just change DRUG_ENCODING or PROTEIN_ENCODING at the top and re-run!\")\n",
    "\n",
    "print(f\"\\nüéØ CURRENT EXPERIMENT:\")\n",
    "print(f\"   {DRUG_ENCODING} + {PROTEIN_ENCODING} + {'3D' if USE_3D_FEATURES else 'no3D'}\")\n",
    "print(f\"   Ready to train!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a295db28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training process...\n",
      "Configuration: chemberta + esm + 3D\n",
      "Device: cuda\n",
      "Training samples: 24,318\n",
      "Validation samples: 5,211\n",
      "Test samples: 5,212\n",
      "üöÄ Starting training: chemberta_esm_3D\n",
      "üìÅ Results will be saved to: results_chemberta_esm_3D_20251007_195822\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20 [Train]:   0%|          | 0/380 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20:\n",
      "  üéØ DTI Accuracy:  Train 0.7826 | Val 0.8096\n",
      "  üìà AUC-ROC:       Train 0.8412 | Val 0.8869\n",
      "  üéõÔ∏è  Precision:     Train 0.6824 | Val 0.6965\n",
      "  üîç Recall:        Train 0.7114 | Val 0.8209\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.6966 | Val 0.7536\n",
      "  üí• Loss:          Train 2.7348 | Val 0.4258\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8096 (81.0%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8161 | Val 0.8309\n",
      "  üìà AUC-ROC:       Train 0.8773 | Val 0.9006\n",
      "  üéõÔ∏è  Precision:     Train 0.7345 | Val 0.7876\n",
      "  üîç Recall:        Train 0.7449 | Val 0.7165\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7397 | Val 0.7504\n",
      "  üí• Loss:          Train 2.4072 | Val 0.4001\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8309 (83.1%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8269 | Val 0.8411\n",
      "  üìà AUC-ROC:       Train 0.8879 | Val 0.9088\n",
      "  üéõÔ∏è  Precision:     Train 0.7493 | Val 0.7640\n",
      "  üîç Recall:        Train 0.7611 | Val 0.7987\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7552 | Val 0.7810\n",
      "  üí• Loss:          Train 2.3401 | Val 0.3785\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8411 (84.1%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8320 | Val 0.8449\n",
      "  üìà AUC-ROC:       Train 0.8972 | Val 0.9134\n",
      "  üéõÔ∏è  Precision:     Train 0.7617 | Val 0.7672\n",
      "  üîç Recall:        Train 0.7584 | Val 0.8079\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7601 | Val 0.7870\n",
      "  üí• Loss:          Train 2.2911 | Val 0.3840\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8449 (84.5%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8332 | Val 0.8411\n",
      "  üìà AUC-ROC:       Train 0.9021 | Val 0.9159\n",
      "  üéõÔ∏è  Precision:     Train 0.7666 | Val 0.7632\n",
      "  üîç Recall:        Train 0.7541 | Val 0.8003\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7603 | Val 0.7813\n",
      "  üí• Loss:          Train 2.2590 | Val 0.3604\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8385 | Val 0.8478\n",
      "  üìà AUC-ROC:       Train 0.9070 | Val 0.9192\n",
      "  üéõÔ∏è  Precision:     Train 0.7742 | Val 0.7720\n",
      "  üîç Recall:        Train 0.7618 | Val 0.8101\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7680 | Val 0.7906\n",
      "  üí• Loss:          Train 2.2313 | Val 0.3519\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8478 (84.8%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8407 | Val 0.8380\n",
      "  üìà AUC-ROC:       Train 0.9112 | Val 0.9187\n",
      "  üéõÔ∏è  Precision:     Train 0.7825 | Val 0.7377\n",
      "  üîç Recall:        Train 0.7559 | Val 0.8431\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7690 | Val 0.7869\n",
      "  üí• Loss:          Train 2.2047 | Val 0.3683\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8420 | Val 0.8542\n",
      "  üìà AUC-ROC:       Train 0.9138 | Val 0.9238\n",
      "  üéõÔ∏è  Precision:     Train 0.7833 | Val 0.7922\n",
      "  üîç Recall:        Train 0.7599 | Val 0.7982\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7714 | Val 0.7951\n",
      "  üí• Loss:          Train 2.1876 | Val 0.3395\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8542 (85.4%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8446 | Val 0.8551\n",
      "  üìà AUC-ROC:       Train 0.9169 | Val 0.9255\n",
      "  üéõÔ∏è  Precision:     Train 0.7912 | Val 0.8021\n",
      "  üîç Recall:        Train 0.7569 | Val 0.7852\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7737 | Val 0.7935\n",
      "  üí• Loss:          Train 2.1645 | Val 0.3372\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8551 (85.5%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8481 | Val 0.8553\n",
      "  üìà AUC-ROC:       Train 0.9202 | Val 0.9283\n",
      "  üéõÔ∏è  Precision:     Train 0.7972 | Val 0.7774\n",
      "  üîç Recall:        Train 0.7605 | Val 0.8295\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7785 | Val 0.8026\n",
      "  üí• Loss:          Train 2.1460 | Val 0.3319\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8553 (85.5%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8494 | Val 0.8576\n",
      "  üìà AUC-ROC:       Train 0.9206 | Val 0.9288\n",
      "  üéõÔ∏è  Precision:     Train 0.7994 | Val 0.8319\n",
      "  üîç Recall:        Train 0.7618 | Val 0.7500\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7801 | Val 0.7888\n",
      "  üí• Loss:          Train 2.1288 | Val 0.3304\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8576 (85.8%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8537 | Val 0.8536\n",
      "  üìà AUC-ROC:       Train 0.9236 | Val 0.9296\n",
      "  üéõÔ∏è  Precision:     Train 0.8109 | Val 0.8006\n",
      "  üîç Recall:        Train 0.7604 | Val 0.7819\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7848 | Val 0.7911\n",
      "  üí• Loss:          Train 2.1181 | Val 0.3288\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8556 | Val 0.8611\n",
      "  üìà AUC-ROC:       Train 0.9263 | Val 0.9308\n",
      "  üéõÔ∏è  Precision:     Train 0.8159 | Val 0.8283\n",
      "  üîç Recall:        Train 0.7599 | Val 0.7673\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7869 | Val 0.7966\n",
      "  üí• Loss:          Train 2.1049 | Val 0.3227\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8611 (86.1%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8565 | Val 0.8584\n",
      "  üìà AUC-ROC:       Train 0.9272 | Val 0.9303\n",
      "  üéõÔ∏è  Precision:     Train 0.8166 | Val 0.7921\n",
      "  üîç Recall:        Train 0.7622 | Val 0.8144\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7885 | Val 0.8031\n",
      "  üí• Loss:          Train 2.0929 | Val 0.3255\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8585 | Val 0.8586\n",
      "  üìà AUC-ROC:       Train 0.9294 | Val 0.9304\n",
      "  üéõÔ∏è  Precision:     Train 0.8218 | Val 0.7907\n",
      "  üîç Recall:        Train 0.7619 | Val 0.8176\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7908 | Val 0.8039\n",
      "  üí• Loss:          Train 2.0778 | Val 0.3262\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8593 | Val 0.8637\n",
      "  üìà AUC-ROC:       Train 0.9313 | Val 0.9309\n",
      "  üéõÔ∏è  Precision:     Train 0.8242 | Val 0.8391\n",
      "  üîç Recall:        Train 0.7615 | Val 0.7619\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7916 | Val 0.7986\n",
      "  üí• Loss:          Train 2.0715 | Val 0.3298\n",
      "  üåü NEW BEST VAL ACCURACY: 0.8637 (86.4%)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8617 | Val 0.8542\n",
      "  üìà AUC-ROC:       Train 0.9314 | Val 0.9320\n",
      "  üéõÔ∏è  Precision:     Train 0.8251 | Val 0.7603\n",
      "  üîç Recall:        Train 0.7686 | Val 0.8598\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7958 | Val 0.8070\n",
      "  üí• Loss:          Train 2.0640 | Val 0.3302\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8631 | Val 0.8542\n",
      "  üìà AUC-ROC:       Train 0.9328 | Val 0.9322\n",
      "  üéõÔ∏è  Precision:     Train 0.8270 | Val 0.8666\n",
      "  üîç Recall:        Train 0.7711 | Val 0.6959\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.7981 | Val 0.7719\n",
      "  üí• Loss:          Train 2.0525 | Val 0.3342\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8657 | Val 0.8605\n",
      "  üìà AUC-ROC:       Train 0.9349 | Val 0.9340\n",
      "  üéõÔ∏è  Precision:     Train 0.8333 | Val 0.7876\n",
      "  üîç Recall:        Train 0.7715 | Val 0.8306\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.8012 | Val 0.8085\n",
      "  üí• Loss:          Train 2.0411 | Val 0.3232\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20:\n",
      "  üéØ DTI Accuracy:  Train 0.8665 | Val 0.8593\n",
      "  üìà AUC-ROC:       Train 0.9365 | Val 0.9351\n",
      "  üéõÔ∏è  Precision:     Train 0.8322 | Val 0.8821\n",
      "  üîç Recall:        Train 0.7760 | Val 0.6964\n",
      "  ‚öñÔ∏è  F1-Score:      Train 0.8031 | Val 0.7783\n",
      "  üí• Loss:          Train 2.0355 | Val 0.3197\n",
      "------------------------------------------------------------\n",
      "\n",
      "üéâ Training completed!\n",
      "   Best validation accuracy: 0.8637 (86.4%)\n",
      "   üìä Results saved to: results_chemberta_esm_3D_20251007_195822/training_results.json\n",
      "\n",
      "‚úÖ Training completed! Results saved to: results_chemberta_esm_3D_20251007_195822/training_results.json\n",
      "üî¨ Evaluating model on test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 82/82 [00:01<00:00, 75.63it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéØ TEST RESULTS:\n",
      "==================================================\n",
      "Drug-Target Interaction (DTI):\n",
      "  üéØ Accuracy:  0.8576 (85.8%)\n",
      "  üìà AUC-ROC:   0.9285\n",
      "  üéõÔ∏è  Precision: 0.8778\n",
      "  üîç Recall:    0.6970\n",
      "  ‚öñÔ∏è  F1-Score:  0.7770\n",
      "\n",
      "Adverse Drug Reaction (ADR):\n",
      "  üéØ Accuracy:  0.9991 (99.9%)\n",
      "\n",
      "üìä Dataset: 5,212 test samples\n",
      "\n",
      "üéâ Experiment completed successfully!\n",
      "üìä Best validation accuracy: 0.8637\n",
      "üìÅ Results saved and ready for comparison\n"
     ]
    }
   ],
   "source": [
    "# START TRAINING\n",
    "print(\"üöÄ Starting training process...\")\n",
    "print(f\"Configuration: {DRUG_ENCODING} + {PROTEIN_ENCODING} + {'3D' if USE_3D_FEATURES else 'no3D'}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Training samples: {len(train_dataset):,}\")\n",
    "print(f\"Validation samples: {len(val_dataset):,}\")\n",
    "print(f\"Test samples: {len(test_dataset):,}\")\n",
    "\n",
    "# Start training with 20 epochs for faster initial results\n",
    "history, results_file = train_model(model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "print(f\"\\n‚úÖ Training completed! Results saved to: {results_file}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_results = evaluate_model(model, test_loader)\n",
    "\n",
    "print(f\"\\nüéâ Experiment completed successfully!\")\n",
    "print(f\"üìä Best validation accuracy: {history['best_val_accuracy']:.4f}\")\n",
    "print(f\"üìÅ Results saved and ready for comparison\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti-adr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
