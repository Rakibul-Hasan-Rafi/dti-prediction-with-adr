{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "694b0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bdcad51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loading adr data\n",
    "'''\n",
    "\n",
    "adrData = pd.read_parquet(\"../Data/1. Adr_embeddings/TFIDF_ADR_vectors/train/tfidf_long.parquet\")\n",
    "testAdrData = pd.read_parquet(\"../Data/1. Adr_embeddings/TFIDF_ADR_vectors/test/tfidf_long.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "386a490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADRVectorizer:\n",
    "    '''\n",
    "    This right here my friend is a vectorizer.\n",
    "    U give him all your Unique id list and he will create a vector.\n",
    "    Consistancy is the name of the game.\n",
    "    Heck you can even check the meddra ids from the vector back.\n",
    "    Emojis for radito... ðŸ˜ºðŸ™Š\n",
    "    '''\n",
    "    def __init__(self, uniqueAdrs):\n",
    "        self.uniqueAdrs = sorted(uniqueAdrs)\n",
    "        self.adrToIdx = {adr: idx for idx, adr in enumerate(self.uniqueAdrs)}\n",
    "        self.idxToAdr = {idx: adr for idx, adr in enumerate(self.uniqueAdrs)}\n",
    "        self.numAdrs = len(self.uniqueAdrs)\n",
    "    \n",
    "    def getVector(self, drugAdrs):\n",
    "        vector = np.zeros(self.numAdrs, dtype=np.float32)\n",
    "        for adr in drugAdrs:\n",
    "            if adr in self.adrToIdx:\n",
    "                vector[self.adrToIdx[adr]] = 1.0\n",
    "\n",
    "        return vector\n",
    "    \n",
    "    def getAdrsFromVector(self, vector):\n",
    "        presentIndices = np.where(vector > 0)[0]\n",
    "        adrs = [self.idxToAdr[idx] for idx in presentIndices if idx in self.idxToAdr]\n",
    "        \n",
    "        return adrs\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bd3c967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "creating the vectors for the drugs..\n",
    "emojis section for radito.. ðŸ™€ðŸ™‰ðŸ˜ºðŸ™Š\n",
    "'''\n",
    "uniqueAdrs = adrData['meddra_id'].unique()\n",
    "adrVectorizer = ADRVectorizer(uniqueAdrs= uniqueAdrs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "00f9b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     rxcui                                                adr\n",
      "0  1000082  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1    10109  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "     rxcui                                                adr\n",
      "0  1037042  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n",
      "1    10432  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "syke, last cell was not creating the vector\n",
    "more emojis.. ðŸ™€ðŸ™‰ðŸ˜ºðŸ™Š\n",
    "'''\n",
    "def adrVectorPd(adrData, vectorizer):\n",
    "    adrVectorsData = []\n",
    "\n",
    "    for rxcui, group in adrData.groupby('rxcui'):\n",
    "        drugAdrs = group['meddra_id'].tolist()\n",
    "\n",
    "        adrVector = vectorizer.getVector( drugAdrs)\n",
    "        adrVectorsData.append({\n",
    "        'rxcui': rxcui,\n",
    "        'adr'  : adrVector\n",
    "    })\n",
    "\n",
    "\n",
    "    adrVectorized = pd.DataFrame(adrVectorsData)\n",
    "    return adrVectorized\n",
    "\n",
    "adrVectorized = adrVectorPd(adrData, adrVectorizer)\n",
    "testAdrVectorized = adrVectorPd(testAdrData, adrVectorizer)\n",
    "\n",
    "print(adrVectorized.head(2))\n",
    "print(testAdrVectorized.head(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1ab6f6ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34741 entries, 0 to 34740\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   drug_chembl_id     34741 non-null  object\n",
      " 1   target_uniprot_id  34741 non-null  object\n",
      " 2   label              34741 non-null  int64 \n",
      " 3   smiles             34741 non-null  object\n",
      " 4   sequence           34741 non-null  object\n",
      " 5   molfile_3d         34741 non-null  object\n",
      " 6   rxcui              34741 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Loading in dti data\n",
    "'''\n",
    "dtiPd = pd.read_parquet('../Data/scope_onside_common_v3.parquet')\n",
    "dtiPd.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d8df3a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "loading drug and protein embedding dataset\n",
    "'''\n",
    "\n",
    "proteinEmbed = pd.read_parquet('../Data/3. Protein_enbeddings/GVP-GNN_protein_embeddings.parquet')\n",
    "drugEmbed = pd.read_parquet('../Data/2. Drug_embeddings/EGNN_drug_embeddings_v2.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "db4df80a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2382 entries, 0 to 2381\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   uniprot_id       2382 non-null   object \n",
      " 1   length           2382 non-null   int64  \n",
      " 2   mean_pLDDT       2382 non-null   float64\n",
      " 3   embedding_dim    2382 non-null   int64  \n",
      " 4   encoder_version  2382 non-null   object \n",
      " 5   pdb_md5          2382 non-null   object \n",
      " 6   embedding        2382 non-null   object \n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 130.4+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1028 entries, 0 to 1027\n",
      "Data columns (total 3 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   drug_chembl_id  1028 non-null   object\n",
      " 1   rxcui           1028 non-null   object\n",
      " 2   embedding       1028 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 24.2+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(proteinEmbed.info())\n",
    "print(drugEmbed.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "bf725c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Class to load dataset for some basic checking\n",
    "and to get access to dimension check fuctions\n",
    "and retrieving dimension values for easy access\n",
    "Insert emoji for radito..\n",
    "'''\n",
    "class Dataset:  \n",
    "    def __init__(self, data=None):\n",
    "\n",
    "        self.data = data\n",
    "        self.proteinDim = None\n",
    "        self.drugDim = None\n",
    "        \n",
    "        if data is not None:\n",
    "            self._calculateDimensions()\n",
    "\n",
    "    def _calculateDimensions(self):\n",
    "        if len(self.data) > 0:\n",
    "            sample_prot_embed = self.data.iloc[0]['embedding_prot']\n",
    "            if hasattr(sample_prot_embed, 'shape'):\n",
    "                self.proteinDim = sample_prot_embed.shape[0]\n",
    "            else:\n",
    "                self.proteinDim = len(sample_prot_embed)\n",
    "            \n",
    "            sample_drug_embed = self.data.iloc[0]['embedding_drug']\n",
    "            if hasattr(sample_drug_embed, 'shape'):\n",
    "                self.drugDim = sample_drug_embed.shape[0]\n",
    "            else:\n",
    "                self.drugDim = len(sample_drug_embed)\n",
    "\n",
    "    def getProteinDimension(self):\n",
    "        return self.proteinDim\n",
    "    \n",
    "    def getDrugDimension(self):\n",
    "        return self.drugDim\n",
    "    \n",
    "    def getData(self):\n",
    "        return self.data\n",
    "    \n",
    "    def checkDimension(self, fieldName):\n",
    "        if fieldName not in self.data.columns:\n",
    "            raise ValueError(f\"Field '{fieldName}' not found in dataset. Available fields: {list(self.data.columns)}\")\n",
    "        \n",
    "        # Get the first entry to determine expected dimension\n",
    "        firstEntry = self.data[fieldName].iloc[0]\n",
    "        \n",
    "        if hasattr(firstEntry, 'shape'):\n",
    "            expectedDim = firstEntry.shape[0] if len(firstEntry.shape) > 0 else 1\n",
    "\n",
    "        else:\n",
    "            expectedDim = len(firstEntry)\n",
    "        \n",
    "        # Check consistency across all entries\n",
    "        inconsistentIndices = []\n",
    "        for idx, value in enumerate(self.data[fieldName]):\n",
    "            if hasattr(value, 'shape'):\n",
    "                currentDim = value.shape[0] if len(value.shape) > 0 else 1\n",
    "\n",
    "            else:\n",
    "                currentDim = len(value)\n",
    "            \n",
    "            if currentDim != expectedDim:\n",
    "                inconsistentIndices.append(idx)\n",
    "        \n",
    "        if inconsistentIndices:\n",
    "            message = f\"Dimension inconsistency in '{fieldName}': Expected {expectedDim}, but found different dimensions at indices {inconsistentIndices}\"\n",
    "            return False, expectedDim, message\n",
    "        else:\n",
    "            message = f\"All entries in '{fieldName}' have consistent dimension: {expectedDim}\"\n",
    "            return True, expectedDim, message\n",
    "    \n",
    "    def getFeaturesAndLabels(self):\n",
    "        proteinEmbeddings = np.stack(self.data['embedding_prot'].values)\n",
    "        drugEmbeddings = np.stack(self.data['embedding_drug'].values)\n",
    "        dtiLabels = self.data['label'].values\n",
    "        adrVectors = np.stack(self.data['adr'].values)\n",
    "        \n",
    "        return proteinEmbeddings, drugEmbeddings, dtiLabels, adrVectors\n",
    "    \n",
    "    def info(self):\n",
    "        if self.data is None:\n",
    "            print(\"No data loaded.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"Dataset shape: {self.data.shape}\")\n",
    "        print(f\"Protein embedding dimension: {self.getProteinDimension()}\")\n",
    "        print(f\"Drug embedding dimension: {self.getDrugDimension()}\")\n",
    "        print(f\"Number of samples: {len(self.data)}\")\n",
    "        print(f\"Number of unique drugs: {self.data['drug_chembl_id'].nunique()}\")\n",
    "        print(f\"Number of unique proteins: {self.data['target_uniprot_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6a043f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This funciton will make sure to merge all the datas\n",
    "to one single pd which we will feed the dataset class.\n",
    "\n",
    "Note!\n",
    "Do whatever in the function or the params but make sure\n",
    "your return pd contains the following field\n",
    "'drug_chembl_id', 'target_uniprot_id', 'label', \n",
    "'smiles', 'sequence', 'embedding_prot', 'embedding_drug', 'adr'\n",
    "\n",
    "!important Insert emoji for radito..\n",
    "'''\n",
    "\n",
    "def prepareDataset(adrVectorized, dtiPd, proteinEmbed, drugEmbed):\n",
    "    \"\"\"\n",
    "    Pre-process and merge all dataframes to create the final dataset\n",
    "    \"\"\"\n",
    "    # Merge drug embeddings with DTI data\n",
    "    drugData = pd.merge(dtiPd, drugEmbed, \n",
    "                        on=['drug_chembl_id', 'rxcui'], \n",
    "                        how='inner')\n",
    "    \n",
    "    # Merge with protein embeddings\n",
    "    proteinDrugData = pd.merge(drugData, proteinEmbed,\n",
    "                                left_on='target_uniprot_id',\n",
    "                                right_on='uniprot_id',\n",
    "                                how='inner')\n",
    "    \n",
    "    # Merge with ADR vectors\n",
    "    finalData = pd.merge(proteinDrugData, adrVectorized,\n",
    "                         on='rxcui',\n",
    "                         how='inner')\n",
    "    \n",
    "    # Select only the required columns and rename embeddings\n",
    "    finalData = finalData[['drug_chembl_id', 'target_uniprot_id', 'label', \n",
    "                            'smiles', 'sequence', 'embedding_x', 'embedding_y', 'adr']]\n",
    "    \n",
    "    finalData = finalData.rename(columns={\n",
    "        'embedding_x': 'embedding_prot',\n",
    "        'embedding_y': 'embedding_drug'\n",
    "    })\n",
    "    \n",
    "    return finalData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "910cfa27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (22228, 8)\n",
      "Protein embedding dimension: 256\n",
      "Drug embedding dimension: 1024\n",
      "Number of samples: 22228\n",
      "Number of unique drugs: 721\n",
      "Number of unique proteins: 2063\n"
     ]
    }
   ],
   "source": [
    "processedData = prepareDataset(adrVectorized, dtiPd, proteinEmbed, drugEmbed)\n",
    "dataset = Dataset(processedData)\n",
    "\n",
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "60109257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 256, \"All entries in 'embedding_prot' have consistent dimension: 256\")\n",
      "(True, 1024, \"All entries in 'embedding_drug' have consistent dimension: 1024\")\n"
     ]
    }
   ],
   "source": [
    "print(dataset.checkDimension('embedding_prot'))\n",
    "print(dataset.checkDimension('embedding_drug'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "91954715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (7595, 8)\n",
      "Protein embedding dimension: 256\n",
      "Drug embedding dimension: 1024\n",
      "Number of samples: 7595\n",
      "Number of unique drugs: 156\n",
      "Number of unique proteins: 1209\n"
     ]
    }
   ],
   "source": [
    "testProcessedData = prepareDataset(testAdrVectorized, dtiPd, proteinEmbed, drugEmbed)\n",
    "\n",
    "test_dataset = Dataset(testProcessedData)\n",
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "52cbd8c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(True, 256, \"All entries in 'embedding_prot' have consistent dimension: 256\")\n",
      "(True, 1024, \"All entries in 'embedding_drug' have consistent dimension: 1024\")\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset.checkDimension('embedding_prot'))\n",
    "print(test_dataset.checkDimension('embedding_drug'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "54974bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiModalDTIADRPrediction(nn.Module):\n",
    "    def __init__(self, protDim, drugDim, adrDim):\n",
    "        super().__init__()\n",
    "        \n",
    "        inputDim = protDim + drugDim\n",
    "        hiddenDim = inputDim // 2\n",
    "        subHiddenDim = hiddenDim // 2\n",
    "\n",
    "        self.drugProtFuse = nn.Sequential(\n",
    "            nn.Linear(inputDim, hiddenDim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hiddenDim, hiddenDim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hiddenDim, hiddenDim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.dtiHead = nn.Sequential(\n",
    "            nn.Linear(hiddenDim, subHiddenDim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(subHiddenDim, subHiddenDim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(subHiddenDim//2, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.adrHead = nn.Sequential(\n",
    "            nn.Linear(hiddenDim, hiddenDim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddenDim*2, hiddenDim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hiddenDim*3, adrDim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward (self, proteinEmbed, drugEmbed):\n",
    "        combined = torch.cat([proteinEmbed, drugEmbed], dim=1)\n",
    "\n",
    "        fused = self.drugProtFuse(combined)\n",
    "\n",
    "        dtiPred = self.dtiHead(fused)\n",
    "        adrPred = self.adrHead(fused)\n",
    "\n",
    "        return dtiPred, adrPred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "053e8444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Protein dimension:  256 \n",
      "Drug dimension   :  1024 \n",
      "ADR dimension    :  4048\n"
     ]
    }
   ],
   "source": [
    "proteinDim = dataset.getProteinDimension()\n",
    "drugDim = dataset.getDrugDimension()\n",
    "adrDim = adrVectorizer.numAdrs\n",
    "\n",
    "print(\n",
    "    \"Protein dimension: \", proteinDim, \n",
    "    \"\\nDrug dimension   : \", drugDim,\n",
    "    \"\\nADR dimension    : \", adrDim\n",
    ")\n",
    "\n",
    "model = MultiModalDTIADRPrediction(\n",
    "    protDim=proteinDim,\n",
    "    drugDim=drugDim,\n",
    "    adrDim= adrDim\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "33953296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "7dd330cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(model, testDataset, batch_size=32):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    protEmbed, drugEmbed, dtiLabels, adrVectors = testDataset.getFeaturesAndLabels()\n",
    "    \n",
    "    protEmbed = torch.FloatTensor(protEmbed)\n",
    "    drugEmbed = torch.FloatTensor(drugEmbed)\n",
    "    dtiLabels = torch.FloatTensor(dtiLabels)\n",
    "    adrVectors = torch.FloatTensor(adrVectors)\n",
    "    \n",
    "    dataset = TensorDataset(protEmbed, drugEmbed, dtiLabels, adrVectors)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_dti_preds = []\n",
    "    all_adr_preds = []\n",
    "    all_dti_labels = []\n",
    "    all_adr_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_prot, batch_drug, batch_dti, batch_adr in tqdm(dataloader, desc='Testing'):\n",
    "            batch_prot = batch_prot.to(device)\n",
    "            batch_drug = batch_drug.to(device)\n",
    "            \n",
    "            dtiPred, adrPred = model(batch_prot, batch_drug)\n",
    "            \n",
    "            all_dti_preds.append(dtiPred.cpu())\n",
    "            all_adr_preds.append(adrPred.cpu())\n",
    "            all_dti_labels.append(batch_dti.cpu())\n",
    "            all_adr_labels.append(batch_adr.cpu())\n",
    "    \n",
    "    dti_preds = torch.cat(all_dti_preds)\n",
    "    adr_preds = torch.cat(all_adr_preds)\n",
    "    dti_labels = torch.cat(all_dti_labels)\n",
    "    adr_labels = torch.cat(all_adr_labels)\n",
    "    \n",
    "    # DTI Scores\n",
    "    dti_binary = (dti_preds > 0.5).float()\n",
    "    dti_accuracy = (dti_binary.squeeze() == dti_labels).float().mean()\n",
    "    dti_f1 = f1_score(dti_labels, dti_binary)\n",
    "    dti_auc = roc_auc_score(dti_labels, dti_preds)\n",
    "    \n",
    "    # ADR Scores\n",
    "    adr_binary = (adr_preds > 0.5).float()\n",
    "    adr_accuracy = (adr_binary == adr_labels).float().mean()\n",
    "    adr_f1 = f1_score(adr_labels.flatten(), adr_binary.flatten(), average='macro')\n",
    "    adr_auc = roc_auc_score(adr_labels.flatten(), adr_preds.flatten())\n",
    "    \n",
    "    print(\"=\" * 50)\n",
    "    print(\"MODEL EVALUATION SCORES\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"\\nDTI PREDICTION SCORES:\")\n",
    "    print(f\"Accuracy: {dti_accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {dti_f1:.4f}\")\n",
    "    print(f\"ROC-AUC:  {dti_auc:.4f}\")\n",
    "    \n",
    "    print(\"\\nADR PREDICTION SCORES:\")\n",
    "    print(f\"Accuracy: {adr_accuracy:.4f}\")\n",
    "    print(f\"F1-Score: {adr_f1:.4f}\")\n",
    "    print(f\"ROC-AUC:  {adr_auc:.4f}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    return {\n",
    "        'dti_accuracy': dti_accuracy,\n",
    "        'dti_f1': dti_f1,\n",
    "        'dti_auc': dti_auc,\n",
    "        'adr_accuracy': adr_accuracy,\n",
    "        'adr_f1': adr_f1,\n",
    "        'adr_auc': adr_auc,\n",
    "        'dti_predictions': dti_preds,\n",
    "        'adr_predictions': adr_preds,\n",
    "        'dti_labels': dti_labels,\n",
    "        'adr_labels': adr_labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "1dd53569",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def trainModel(model, trainDataset, testDataset, epochs, batch_size=32, lr=0.0001, \n",
    "               save_dir='models', continue_training=False, checkpoint_path=None):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    protEmbed, drugEmbed, dtiLabels, adrVectors = trainDataset.getFeaturesAndLabels()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=True)\n",
    "    dti_criterion = nn.BCELoss()\n",
    "    adr_criterion = nn.BCELoss()\n",
    "    \n",
    "    start_epoch = 0\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Continue from checkpoint if requested\n",
    "    if continue_training and checkpoint_path and os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        start_epoch = checkpoint['epoch']\n",
    "        best_loss = checkpoint['best_loss']\n",
    "        print(f\"Resumed training from epoch {start_epoch}, best loss: {best_loss:.4f}\")\n",
    "    \n",
    "    protEmbed = torch.FloatTensor(protEmbed)\n",
    "    drugEmbed = torch.FloatTensor(drugEmbed)\n",
    "    dtiLabels = torch.FloatTensor(dtiLabels)\n",
    "    adrVectors = torch.FloatTensor(adrVectors)\n",
    "    \n",
    "    dataset = TensorDataset(protEmbed, drugEmbed, dtiLabels, adrVectors)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        pbar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{epochs}')\n",
    "        for batch_prot, batch_drug, batch_dti, batch_adr in pbar:\n",
    "            batch_prot = batch_prot.to(device)\n",
    "            batch_drug = batch_drug.to(device)\n",
    "            batch_dti = batch_dti.to(device)\n",
    "            batch_adr = batch_adr.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            dtiPred, adrPred = model(batch_prot, batch_drug)\n",
    "            \n",
    "            dtiLoss = dti_criterion(dtiPred.squeeze(), batch_dti)\n",
    "            adrLoss = adr_criterion(adrPred, batch_adr)\n",
    "            totalLoss = dtiLoss + adrLoss\n",
    "            \n",
    "            totalLoss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += totalLoss.item()\n",
    "            pbar.set_postfix({'Loss': f'{totalLoss.item():.4f}'})\n",
    "        \n",
    "        avg_loss = epoch_loss / len(dataloader)\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}, Avg Loss: {avg_loss:.4f}, LR: {optimizer.param_groups[0][\"lr\"]:.2e}')\n",
    "        \n",
    "        # Save best model\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), f'{save_dir}/best_model.pth')\n",
    "            print(f'New best model saved with loss: {best_loss:.4f}')\n",
    "        \n",
    "        # Save checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'scheduler_state_dict': scheduler.state_dict(),\n",
    "                'best_loss': best_loss,\n",
    "                'loss': avg_loss\n",
    "            }\n",
    "            torch.save(checkpoint, f'{save_dir}/checkpoint_epoch_{epoch+1}.pth')\n",
    "            torch.save(model.state_dict(), f'{save_dir}/model_epoch_{epoch+1}.pth')\n",
    "            print(f'Checkpoint saved at epoch {epoch+1}')\n",
    "            \n",
    "            # Evaluate on test dataset\n",
    "            test_results = testModel(model, testDataset, batch_size=batch_size)\n",
    "            print(f'Test Scores at Epoch {epoch+1}:')\n",
    "            print(f'  DTI - Acc: {test_results[\"dti_accuracy\"]:.4f}, F1: {test_results[\"dti_f1\"]:.4f}, AUC: {test_results[\"dti_auc\"]:.4f}')\n",
    "            print(f'  ADR - Acc: {test_results[\"adr_accuracy\"]:.4f}, F1: {test_results[\"adr_f1\"]:.4f}, AUC: {test_results[\"adr_auc\"]:.4f}')\n",
    "    \n",
    "    return avg_loss, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db100300",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss, best_loss = trainModel(\n",
    "    model, \n",
    "    dataset, \n",
    "    test_dataset, \n",
    "    save_dir='models_1',\n",
    "    continue_training=False, \n",
    "    epochs=100 \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti-adr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
