{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "67435db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Part 0: setup & config loader (no auto-fix) =====\n",
    "\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, Optional, List, Tuple\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# If pyyaml isn't installed, run:  !pip install pyyaml\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02fafb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Config summary ===\n",
      "DTI parquet:         F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\scope_onside_common_v3.parquet\n",
      "TF-IDF ADR root:     F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\TFIDF_ADR_vectors\n",
      "  - idf_table:       F:\\Thesis Korbi na\\dti-prediction-with-adr\\Data\\TFIDF_ADR_vectors\\idf_table.parquet\n",
      "  - splits:          ['train', 'val', 'test']\n",
      "PDB directory:       F:\\Thesis Korbi na\\dti-prediction-with-adr\\AlphaFoldData\n",
      "Split lists:         None (will infer splits from TF-IDF folders)\n",
      "ADRs kept (columns): 4048\n",
      "======================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- User: set path to your config.yaml here ----\n",
    "CONFIG_PATH = Path(\"config.yaml\")  # or \"config.yaml\" if you saved at root\n",
    "\n",
    "\n",
    "# -------- helper: small DotDict for cfg access --------\n",
    "class DotDict(dict):\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "\n",
    "def load_config(path: Path) -> DotDict:\n",
    "    assert path.exists(), f\"Config file not found: {path}\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw = yaml.safe_load(f)\n",
    "    return DotDict(raw)\n",
    "\n",
    "\n",
    "def resolve_path(p: str | Path) -> Path:\n",
    "    \"\"\"\n",
    "    Resolve a path relative to current working directory.\n",
    "    If it's already absolute, returns as-is.\n",
    "    \"\"\"\n",
    "    p = Path(p)\n",
    "    return p if p.is_absolute() else (Path.cwd() / p).resolve()\n",
    "\n",
    "\n",
    "def require_exists(path: Path, kind: str):\n",
    "    assert path.exists(), f\"Expected {kind} at: {path}\"\n",
    "\n",
    "\n",
    "def validate_config(cfg: DotDict) -> Dict[str, Path]:\n",
    "    # Resolve key paths\n",
    "    dti_path = resolve_path(cfg[\"data\"][\"dti_path\"])\n",
    "    adr_root = resolve_path(cfg[\"data\"][\"adr_tfidf_root\"])\n",
    "    pdb_dir  = resolve_path(cfg[\"data\"][\"pdb_dir\"])\n",
    "\n",
    "    # Validate existence\n",
    "    require_exists(dti_path, \"DTI parquet\")\n",
    "    require_exists(adr_root, \"TF-IDF ADR root directory\")\n",
    "    require_exists(adr_root / \"idf_table.parquet\", \"idf_table.parquet in TF-IDF root\")\n",
    "    for split in (\"train\", \"val\", \"test\"):\n",
    "        split_dir = adr_root / split\n",
    "        require_exists(split_dir, f\"{split} split directory in TF-IDF root\")\n",
    "        require_exists(split_dir / \"tfidf_wide.parquet\", f\"{split}/tfidf_wide.parquet\")\n",
    "\n",
    "    require_exists(pdb_dir, \"AlphaFold PDB directory\")\n",
    "\n",
    "    # Optional split files\n",
    "    splits = cfg[\"data\"][\"splits\"]\n",
    "    train_list = resolve_path(splits[\"train_rxcui_path\"]) if splits[\"train_rxcui_path\"] else None\n",
    "    val_list   = resolve_path(splits[\"val_rxcui_path\"]) if splits[\"val_rxcui_path\"] else None\n",
    "    test_list  = resolve_path(splits[\"test_rxcui_path\"]) if splits[\"test_rxcui_path\"] else None\n",
    "    for p in (train_list, val_list, test_list):\n",
    "        if p is not None:\n",
    "            require_exists(p, \"split list file\")\n",
    "\n",
    "    # Quick schema check on DTI (column names only; no heavy read)\n",
    "    # We'll fully load in Part 1.\n",
    "    sample_cols = pd.read_parquet(dti_path, engine=\"pyarrow\", columns=[]).columns if False else None\n",
    "    # (above line is a placeholder: pyarrow can't read just headers easily; we'll check later in Part 1)\n",
    "\n",
    "    return {\n",
    "        \"dti_path\": dti_path,\n",
    "        \"adr_root\": adr_root,\n",
    "        \"pdb_dir\": pdb_dir,\n",
    "        \"train_list\": train_list,\n",
    "        \"val_list\": val_list,\n",
    "        \"test_list\": test_list,\n",
    "    }\n",
    "\n",
    "\n",
    "def print_summary(cfg: DotDict, paths: Dict[str, Path]):\n",
    "    print(\"=== Config summary ===\")\n",
    "    print(f\"DTI parquet:         {paths['dti_path']}\")\n",
    "    print(f\"TF-IDF ADR root:     {paths['adr_root']}\")\n",
    "    print(f\"  - idf_table:       {paths['adr_root'] / 'idf_table.parquet'}\")\n",
    "    print(f\"  - splits:          {[p.name for p in (paths['adr_root'] / 'train', paths['adr_root'] / 'val', paths['adr_root'] / 'test')]}\")\n",
    "    print(f\"PDB directory:       {paths['pdb_dir']}\")\n",
    "    if paths['train_list'] or paths['val_list'] or paths['test_list']:\n",
    "        print(\"Split lists provided:\")\n",
    "        if paths['train_list']: print(f\"  - train_rxcui_path: {paths['train_list']}\")\n",
    "        if paths['val_list']:   print(f\"  - val_rxcui_path:   {paths['val_list']}\")\n",
    "        if paths['test_list']:  print(f\"  - test_rxcui_path:  {paths['test_list']}\")\n",
    "    else:\n",
    "        print(\"Split lists:         None (will infer splits from TF-IDF folders)\")\n",
    "\n",
    "    # Peek TF-IDF ADR dimension from idf_table\n",
    "    idf_path = paths['adr_root'] / \"idf_table.parquet\"\n",
    "    try:\n",
    "        idf_table = pd.read_parquet(idf_path)\n",
    "        n_adrs = len(idf_table)\n",
    "        print(f\"ADRs kept (columns): {n_adrs}\")\n",
    "    except Exception as e:\n",
    "        print(f\"(Could not read idf_table now; will load in Part 1) -> {e}\")\n",
    "\n",
    "    print(\"======================\\n\")\n",
    "\n",
    "\n",
    "# ---- Load & validate ----\n",
    "cfg = load_config(CONFIG_PATH)\n",
    "paths = validate_config(cfg)\n",
    "print_summary(cfg, paths)\n",
    "\n",
    "# We'll reuse `cfg` and `paths` in Part 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c82e8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split overlaps (should be 0): {('train', 'val'): 0, ('train', 'test'): 0, ('val', 'test'): 0}\n",
      "DTI total rows: 34,741\n",
      "train:  22,310 rows |   719 drugs | ADR matrix shape = (719, 4048)\n",
      "  val:   4,835 rows |   154 drugs | ADR matrix shape = (154, 4048)\n",
      " test:   7,596 rows |   155 drugs | ADR matrix shape = (155, 4048)\n",
      "\n",
      "Example train rxcui: 6676\n",
      "  Weighted ADR vector (len=4048), nnz=4\n",
      "  Binary ADR vector   (len=4048), nnz=4\n"
     ]
    }
   ],
   "source": [
    "# -- expects `cfg` and `paths` from Part 0 --\n",
    "\n",
    "# ---------- ADR metadata container ----------\n",
    "@dataclass\n",
    "class ADRInfo:\n",
    "    idf_table: pd.DataFrame          # columns: meddra_id, df, idf (ordered)\n",
    "    meddra_ids: np.ndarray           # shape [M], dtype=int64\n",
    "    adr_cols: List[str]              # [\"meddra_<id>\", ...] aligned to idf_table order\n",
    "    n_adrs: int\n",
    "\n",
    "def _adr_cols_from_idf(idf_table: pd.DataFrame) -> List[str]:\n",
    "    ids = idf_table[\"meddra_id\"].astype(np.int64).tolist()\n",
    "    return [f\"meddra_{i}\" for i in ids]\n",
    "\n",
    "def load_adr_info(adr_root: Path) -> ADRInfo:\n",
    "    idf_table = pd.read_parquet(adr_root / \"idf_table.parquet\")\n",
    "    idf_table = idf_table.sort_values(\"meddra_id\").reset_index(drop=True)\n",
    "    meddra_ids = idf_table[\"meddra_id\"].astype(np.int64).to_numpy()\n",
    "    adr_cols = _adr_cols_from_idf(idf_table)\n",
    "    return ADRInfo(\n",
    "        idf_table=idf_table,\n",
    "        meddra_ids=meddra_ids,\n",
    "        adr_cols=adr_cols,\n",
    "        n_adrs=len(adr_cols),\n",
    "    )\n",
    "\n",
    "def _ensure_adr_column_order(df: pd.DataFrame, adr_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reorder TF-IDF wide dataframe columns to match adr_cols.\n",
    "    Add missing columns as zeros. Drop unexpected extras.\n",
    "    \"\"\"\n",
    "    have = set(df.columns)\n",
    "    need = set(adr_cols)\n",
    "    missing = list(need - have)\n",
    "    extras  = list(have - (need | {\"rxcui\"}))\n",
    "    if extras:\n",
    "        df = df.drop(columns=extras)\n",
    "    if missing:\n",
    "        # add zero cols with correct float dtype\n",
    "        zeros = pd.DataFrame(\n",
    "            np.zeros((len(df), len(missing)), dtype=np.float32),\n",
    "            index=df.index, columns=missing\n",
    "        )\n",
    "        df = pd.concat([df, zeros], axis=1)\n",
    "    # reorder\n",
    "    return df[[\"rxcui\"] + adr_cols]\n",
    "\n",
    "def load_tfidf_split(adr_root: Path, split: str, adr_cols: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load <split>/tfidf_wide.parquet, ensure correct ADR column order, set index to rxcui.\n",
    "    Returns a DF with index=rxcui and columns=adr_cols (float32).\n",
    "    \"\"\"\n",
    "    path = adr_root / split / \"tfidf_wide.parquet\"\n",
    "    df = pd.read_parquet(path)\n",
    "    assert \"rxcui\" in df.columns, f\"'rxcui' column missing in {path}\"\n",
    "    df = _ensure_adr_column_order(df, adr_cols)\n",
    "    df[adr_cols] = df[adr_cols].astype(np.float32, copy=False)\n",
    "    df = df.set_index(\"rxcui\")\n",
    "    # normalize index to string/stripped\n",
    "    df.index = df.index.astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "def discover_split_drugs(tfidf_by_split: Dict[str, pd.DataFrame]) -> Dict[str, set]:\n",
    "    return {k: set(v.index.tolist()) for k, v in tfidf_by_split.items()}\n",
    "\n",
    "def load_dti_table(dti_path: Path) -> pd.DataFrame:\n",
    "    # Load only once; enforce schemas\n",
    "    dti = pd.read_parquet(dti_path)\n",
    "    required = [\"drug_chembl_id\",\"target_uniprot_id\",\"label\",\"smiles\",\"sequence\",\"molfile_3d\",\"rxcui\"]\n",
    "    missing = [c for c in required if c not in dti.columns]\n",
    "    assert not missing, f\"DTI parquet missing columns: {missing}\"\n",
    "\n",
    "    # tidy types\n",
    "    dti[\"rxcui\"] = dti[\"rxcui\"].astype(str).str.strip()\n",
    "    dti[\"target_uniprot_id\"] = dti[\"target_uniprot_id\"].astype(str).str.strip()\n",
    "    dti[\"label\"] = pd.to_numeric(dti[\"label\"], downcast=\"integer\")\n",
    "    # keep others as string-ish (object OK)\n",
    "    return dti\n",
    "\n",
    "def make_per_split_dti(dti_df: pd.DataFrame, split_drugs: Dict[str, set]) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Filter DTI rows by rxcui membership in each split.\n",
    "    Note: A drug can appear with many proteins; we split by drug only to avoid leakage.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    for split, drugs in split_drugs.items():\n",
    "        mask = dti_df[\"rxcui\"].isin(drugs)\n",
    "        out[split] = dti_df.loc[mask].reset_index(drop=True)\n",
    "    return out\n",
    "\n",
    "# ---------- ADR accessors ----------\n",
    "class ADRAccessor:\n",
    "    \"\"\"\n",
    "    Convenient accessor for ADR vectors aligned to idf_table order.\n",
    "    Provides weighted and binary vectors; returns zeros for unknown rxcui.\n",
    "    \"\"\"\n",
    "    def __init__(self, tfidf_by_split: Dict[str, pd.DataFrame], adr_cols: List[str]):\n",
    "        self.tfidf = tfidf_by_split   # each is DF indexed by rxcui\n",
    "        self.adr_cols = adr_cols\n",
    "        self.n_adrs = len(adr_cols)\n",
    "        # Precompute numpy views for speed\n",
    "        self._np = {k: v[self.adr_cols].to_numpy(dtype=np.float32, copy=False) for k, v in tfidf_by_split.items()}\n",
    "        self._idx = {k: {rx:i for i, rx in enumerate(df.index)} for k, df in tfidf_by_split.items()}\n",
    "\n",
    "    def get_weighted(self, split: str, rxcui: str) -> np.ndarray:\n",
    "        idx = self._idx[split].get(str(rxcui).strip(), None)\n",
    "        if idx is None:\n",
    "            return np.zeros(self.n_adrs, dtype=np.float32)\n",
    "        return self._np[split][idx]\n",
    "\n",
    "    def get_binary(self, split: str, rxcui: str) -> np.ndarray:\n",
    "        w = self.get_weighted(split, rxcui)\n",
    "        # TF-IDF>0 -> 1 else 0\n",
    "        return (w > 0).astype(np.float32)\n",
    "\n",
    "    def df(self, split: str) -> pd.DataFrame:\n",
    "        return self.tfidf[split][self.adr_cols]\n",
    "\n",
    "# ---------------- Load everything ----------------\n",
    "adr_info = load_adr_info(paths[\"adr_root\"])\n",
    "tfidf_by_split = {\n",
    "    \"train\": load_tfidf_split(paths[\"adr_root\"], \"train\", adr_info.adr_cols),\n",
    "    \"val\":   load_tfidf_split(paths[\"adr_root\"], \"val\",   adr_info.adr_cols),\n",
    "    \"test\":  load_tfidf_split(paths[\"adr_root\"], \"test\",  adr_info.adr_cols),\n",
    "}\n",
    "split_drugs = discover_split_drugs(tfidf_by_split)\n",
    "\n",
    "# sanity on split disjointness\n",
    "overlaps = {\n",
    "    (\"train\",\"val\"): len(split_drugs[\"train\"] & split_drugs[\"val\"]),\n",
    "    (\"train\",\"test\"): len(split_drugs[\"train\"] & split_drugs[\"test\"]),\n",
    "    (\"val\",\"test\"): len(split_drugs[\"val\"] & split_drugs[\"test\"]),\n",
    "}\n",
    "print(\"Split overlaps (should be 0):\", overlaps)\n",
    "\n",
    "# Load DTI table and make per-split views by drug\n",
    "dti_df = load_dti_table(paths[\"dti_path\"])\n",
    "dti_by_split = make_per_split_dti(dti_df, split_drugs)\n",
    "\n",
    "# Report basic shapes\n",
    "print(f\"DTI total rows: {len(dti_df):,}\")\n",
    "for s in (\"train\",\"val\",\"test\"):\n",
    "    n_rows = len(dti_by_split[s])\n",
    "    n_drugs = len(split_drugs[s])\n",
    "    print(f\"{s:>5}: {n_rows:>7,} rows | {n_drugs:>5,} drugs | ADR matrix shape = {tfidf_by_split[s].shape}\")\n",
    "\n",
    "# Build ADR accessor\n",
    "adr_accessor = ADRAccessor(tfidf_by_split, adr_info.adr_cols)\n",
    "\n",
    "# Quick peek: fetch ADR vector for first train drug\n",
    "if len(split_drugs[\"train\"]):\n",
    "    example_rx = next(iter(split_drugs[\"train\"]))\n",
    "    w = adr_accessor.get_weighted(\"train\", example_rx)\n",
    "    b = adr_accessor.get_binary(\"train\", example_rx)\n",
    "    print(f\"\\nExample train rxcui: {example_rx}\")\n",
    "    print(f\"  Weighted ADR vector (len={len(w)}), nnz={int((w>0).sum())}\")\n",
    "    print(f\"  Binary ADR vector   (len={len(b)}), nnz={int(b.sum())}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
