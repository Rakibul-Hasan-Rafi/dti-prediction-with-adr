{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e3171f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "            MACHINE LEARNING ENVIRONMENT REPORT (Windows)             \n",
      "======================================================================\n",
      "\n",
      "[System]\n",
      "  OS: Windows-11-10.0.26100-SP0\n",
      "  Python: 3.12.11 | packaged by conda-forge | (main, Jun  4 2025, 14:29:09) [MSC v.1943 64 bit (AMD64)]\n",
      "  Conda env: F:\\AI-Ml_conda_env\\research | venv: None\n",
      "\n",
      "[CPU/RAM/Disk]\n",
      "  CPU: Intel64 Family 6 Model 151 Stepping 2, GenuineIntel | phys/logical: 12/20\n",
      "  RAM: total 31.76 GiB | available 15.95 GiB\n",
      "  Disk C: total 399.13 GiB | free 48.05 GiB (88.0% used)\n",
      "\n",
      "[GPU / Drivers / CUDA]\n",
      "  nvidia-smi: found\n",
      "  NVIDIA driver: 576.88\n",
      "  GPU0: NVIDIA GeForce RTX 5070 Ti | None GiB VRAM | PCIe x16\n",
      "  nvcc (CUDA toolkit): not found\n",
      "\n",
      "[PyTorch]\n",
      "  Installed: True\n",
      "  Version: 2.7.0+cu128\n",
      "  CUDA available: True | devices: 1\n",
      "    GPU0: NVIDIA GeForce RTX 5070 Ti | CC 12.0 | 15.92 GiB\n",
      "  torch CUDA (build): 12.8\n",
      "  cuDNN available: True | version: 90701\n",
      "\n",
      "[TensorFlow]\n",
      "  Installed: False\n",
      "\n",
      "[Key Python Packages]\n",
      "  numpy: 2.3.3\n",
      "  pandas: 2.3.3\n",
      "  scikit-learn: not installed\n",
      "  xgboost: not installed\n",
      "  lightgbm: not installed\n",
      "  torch: 2.7.0+cu128\n",
      "  torchvision: not installed\n",
      "  torchaudio: 2.7.0+cu128\n",
      "  tensorflow: not installed\n",
      "\n",
      "Report saved to: f:\\Thesis Korbi na\\dti-prediction-with-adr\\Model_v1\\ml_env_report.json\n",
      "\n",
      "Tip: If PyTorch says CUDA is unavailable but you have an NVIDIA GPU, check matching CUDA Toolkit/driver and install the proper PyTorch build for your CUDA version (or use the CPU build).\n"
     ]
    }
   ],
   "source": [
    "# ml_env_check.py\n",
    "# Windows-focused ML environment audit\n",
    "# Produces a readable report + a JSON file: ml_env_report.json\n",
    "\n",
    "import json, os, platform, shutil, subprocess, sys, time\n",
    "from datetime import datetime\n",
    "\n",
    "def try_import(module_name):\n",
    "    try:\n",
    "        return __import__(module_name)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def run(cmd):\n",
    "    try:\n",
    "        completed = subprocess.run(\n",
    "            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n",
    "            text=True, shell=True, check=False\n",
    "        )\n",
    "        return completed.stdout.strip()\n",
    "    except Exception as e:\n",
    "        return f\"ERROR: {e}\"\n",
    "\n",
    "def bytes_to_gib(n):\n",
    "    return round(n / (1024**3), 2)\n",
    "\n",
    "def gather_os_python():\n",
    "    return {\n",
    "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "        \"os\": {\n",
    "            \"system\": platform.system(),\n",
    "            \"release\": platform.release(),\n",
    "            \"version\": platform.version(),\n",
    "            \"platform\": platform.platform(),\n",
    "            \"machine\": platform.machine(),\n",
    "        },\n",
    "        \"python\": {\n",
    "            \"version\": sys.version.replace(\"\\n\", \" \"),\n",
    "            \"executable\": sys.executable,\n",
    "        },\n",
    "        \"env_manager\": {\n",
    "            \"conda_env\": os.environ.get(\"CONDA_DEFAULT_ENV\"),\n",
    "            \"conda_prefix\": os.environ.get(\"CONDA_PREFIX\"),\n",
    "            \"venv\": os.environ.get(\"VIRTUAL_ENV\"),\n",
    "        }\n",
    "    }\n",
    "\n",
    "def gather_cpu_ram_disk():\n",
    "    report = {}\n",
    "    psutil = try_import(\"psutil\")\n",
    "    if psutil:\n",
    "        vm = psutil.virtual_memory()\n",
    "        report[\"cpu\"] = {\n",
    "            \"physical_cores\": psutil.cpu_count(logical=False),\n",
    "            \"logical_cores\": psutil.cpu_count(logical=True),\n",
    "            \"frequency_mhz\": getattr(psutil.cpu_freq(), \"current\", None)\n",
    "        }\n",
    "        report[\"ram\"] = {\n",
    "            \"total_gib\": bytes_to_gib(vm.total),\n",
    "            \"available_gib\": bytes_to_gib(vm.available),\n",
    "        }\n",
    "        # Disk (system drive C:)\n",
    "        try:\n",
    "            usage = psutil.disk_usage(\"C:\\\\\")\n",
    "            report[\"disk_C\"] = {\n",
    "                \"total_gib\": bytes_to_gib(usage.total),\n",
    "                \"used_gib\": bytes_to_gib(usage.used),\n",
    "                \"free_gib\": bytes_to_gib(usage.free),\n",
    "                \"percent_used\": usage.percent\n",
    "            }\n",
    "        except Exception as e:\n",
    "            report[\"disk_C\"] = {\"error\": str(e)}\n",
    "    else:\n",
    "        report[\"note\"] = \"Install psutil for detailed CPU/RAM/disk info: pip install psutil\"\n",
    "    # CPU name (Windows often via PROCESSOR_IDENTIFIER)\n",
    "    report.setdefault(\"cpu\", {})\n",
    "    report[\"cpu\"][\"name\"] = os.environ.get(\"PROCESSOR_IDENTIFIER\") or platform.processor()\n",
    "    return report\n",
    "\n",
    "def parse_nvidia_smi_text(txt):\n",
    "    # Pulls driver version and per-GPU model/memory info from nvidia-smi --query\n",
    "    info = {\"driver_version\": None, \"gpus\": []}\n",
    "    if not txt or \"ERROR\" in txt:\n",
    "        return info\n",
    "    lines = [x.strip() for x in txt.splitlines() if x.strip()]\n",
    "    # Expect CSV header then rows\n",
    "    # header: name, driver_version, memory.total, pcie.link.width.max\n",
    "    if len(lines) >= 2:\n",
    "        header = [h.strip() for h in lines[0].split(\",\")]\n",
    "        for row in lines[1:]:\n",
    "            cols = [c.strip() for c in row.split(\",\")]\n",
    "            item = dict(zip(header, cols))\n",
    "            if not info[\"driver_version\"]:\n",
    "                info[\"driver_version\"] = item.get(\"driver_version\")\n",
    "            # Normalize memory to GiB\n",
    "            mem = item.get(\"memory.total\")\n",
    "            mem_gib = None\n",
    "            if mem:\n",
    "                # nvidia-smi usually returns \"xxxx MiB\"\n",
    "                try:\n",
    "                    mem_gib = round(float(mem.split()[0]) / 1024.0, 2)\n",
    "                except Exception:\n",
    "                    mem_gib = mem\n",
    "            info[\"gpus\"].append({\n",
    "                \"name\": item.get(\"name\"),\n",
    "                \"memory_gib\": mem_gib,\n",
    "                \"pcie_width_max\": item.get(\"pcie.link.width.max\")\n",
    "            })\n",
    "    return info\n",
    "\n",
    "def gather_cuda_stack():\n",
    "    # Check presence of NVIDIA stack and CUDA toolkit\n",
    "    report = {\"nvidia_smi_available\": shutil.which(\"nvidia-smi\") is not None}\n",
    "    if report[\"nvidia_smi_available\"]:\n",
    "        q = \"nvidia-smi --query-gpu=name,driver_version,memory.total,pcie.link.width.max --format=csv\"\n",
    "        smi = run(q)\n",
    "        report.update(parse_nvidia_smi_text(smi))\n",
    "    else:\n",
    "        report[\"note\"] = \"nvidia-smi not found (no NVIDIA GPU or drivers not installed)\"\n",
    "\n",
    "    # CUDA toolkit (nvcc)\n",
    "    report[\"nvcc_available\"] = shutil.which(\"nvcc\") is not None\n",
    "    if report[\"nvcc_available\"]:\n",
    "        report[\"nvcc_version_raw\"] = run(\"nvcc --version\")\n",
    "\n",
    "    return report\n",
    "\n",
    "def gather_pytorch():\n",
    "    torch = try_import(\"torch\")\n",
    "    out = {\"installed\": bool(torch)}\n",
    "    if not torch:\n",
    "        return out\n",
    "    try:\n",
    "        out[\"version\"] = torch.__version__\n",
    "        out[\"cuda_available\"] = torch.cuda.is_available()\n",
    "        out[\"cuda_device_count\"] = torch.cuda.device_count() if out[\"cuda_available\"] else 0\n",
    "        gpus = []\n",
    "        if out[\"cuda_available\"]:\n",
    "            for i in range(torch.cuda.device_count()):\n",
    "                name = torch.cuda.get_device_name(i)\n",
    "                cc = torch.cuda.get_device_capability(i)\n",
    "                mem = torch.cuda.get_device_properties(i).total_memory\n",
    "                gpus.append({\n",
    "                    \"index\": i, \"name\": name,\n",
    "                    \"compute_capability\": f\"{cc[0]}.{cc[1]}\",\n",
    "                    \"total_mem_gib\": bytes_to_gib(mem)\n",
    "                })\n",
    "        out[\"gpus\"] = gpus\n",
    "        # CUDA/cuDNN versions as seen by PyTorch\n",
    "        out[\"torch_cuda_version\"] = getattr(torch.version, \"cuda\", None)\n",
    "        cudnn = getattr(torch.backends, \"cudnn\", None)\n",
    "        out[\"cudnn_available\"] = bool(cudnn and cudnn.is_available())\n",
    "        out[\"cudnn_version\"] = int(cudnn.version()) if out[\"cudnn_available\"] else None\n",
    "    except Exception as e:\n",
    "        out[\"error\"] = str(e)\n",
    "    return out\n",
    "\n",
    "def gather_tensorflow():\n",
    "    tf = try_import(\"tensorflow\")\n",
    "    out = {\"installed\": bool(tf)}\n",
    "    if not tf:\n",
    "        return out\n",
    "    try:\n",
    "        out[\"version\"] = tf.__version__\n",
    "        # GPU logical devices\n",
    "        try:\n",
    "            devs = tf.config.list_physical_devices(\"GPU\")\n",
    "            out[\"gpu_count\"] = len(devs)\n",
    "            out[\"gpus\"] = [str(d) for d in devs]\n",
    "        except Exception as e:\n",
    "            out[\"gpu_list_error\"] = str(e)\n",
    "        # Build info (CUDA/cuDNN as compiled)\n",
    "        try:\n",
    "            bi = tf.sysconfig.get_build_info()\n",
    "            out[\"cuda_version_build\"] = bi.get(\"cuda_version\")\n",
    "            out[\"cudnn_version_build\"] = bi.get(\"cudnn_version\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    except Exception as e:\n",
    "        out[\"error\"] = str(e)\n",
    "    return out\n",
    "\n",
    "def gather_key_packages():\n",
    "    pkgs = [\"numpy\", \"pandas\", \"scikit-learn\", \"xgboost\", \"lightgbm\", \"torch\", \"torchvision\", \"torchaudio\", \"tensorflow\"]\n",
    "    versions = {}\n",
    "    for p in pkgs:\n",
    "        m = try_import(p.replace(\"-\", \"_\"))\n",
    "        if m:\n",
    "            ver = getattr(m, \"__version__\", None)\n",
    "            versions[p] = ver or \"installed (version unknown)\"\n",
    "        else:\n",
    "            versions[p] = None\n",
    "    return versions\n",
    "\n",
    "\n",
    "report = {}\n",
    "report[\"system\"] = gather_os_python()\n",
    "report[\"resources\"] = gather_cpu_ram_disk()\n",
    "report[\"nvidia_cuda\"] = gather_cuda_stack()\n",
    "report[\"pytorch\"] = gather_pytorch()\n",
    "report[\"tensorflow\"] = gather_tensorflow()\n",
    "report[\"packages\"] = gather_key_packages()\n",
    "\n",
    "# Print pretty\n",
    "print(\"=\"*70)\n",
    "print(\"MACHINE LEARNING ENVIRONMENT REPORT (Windows)\".center(70))\n",
    "print(\"=\"*70)\n",
    "s = report[\"system\"]\n",
    "print(f\"\\n[System]\")\n",
    "print(f\"  OS: {s['os']['platform']}\")\n",
    "print(f\"  Python: {s['python']['version']}\")\n",
    "env = s[\"env_manager\"]\n",
    "print(f\"  Conda env: {env.get('conda_env') or 'None'} | venv: {env.get('venv') or 'None'}\")\n",
    "\n",
    "r = report[\"resources\"]\n",
    "cpu = r.get(\"cpu\", {})\n",
    "print(f\"\\n[CPU/RAM/Disk]\")\n",
    "print(f\"  CPU: {cpu.get('name')} | phys/logical: {cpu.get('physical_cores')}/{cpu.get('logical_cores')}\")\n",
    "if \"ram\" in r:\n",
    "    print(f\"  RAM: total {r['ram']['total_gib']} GiB | available {r['ram']['available_gib']} GiB\")\n",
    "if \"disk_C\" in r:\n",
    "    d = r[\"disk_C\"]\n",
    "    if \"error\" in d:\n",
    "        print(f\"  Disk C: error: {d['error']}\")\n",
    "    else:\n",
    "        print(f\"  Disk C: total {d['total_gib']} GiB | free {d['free_gib']} GiB ({d['percent_used']}% used)\")\n",
    "\n",
    "print(f\"\\n[GPU / Drivers / CUDA]\")\n",
    "n = report[\"nvidia_cuda\"]\n",
    "print(f\"  nvidia-smi: {'found' if n.get('nvidia_smi_available') else 'not found'}\")\n",
    "if n.get(\"driver_version\"):\n",
    "    print(f\"  NVIDIA driver: {n['driver_version']}\")\n",
    "if n.get(\"gpus\"):\n",
    "    for i, g in enumerate(n[\"gpus\"]):\n",
    "        print(f\"  GPU{i}: {g['name']} | {g['memory_gib']} GiB VRAM | PCIe x{g.get('pcie_width_max')}\")\n",
    "print(f\"  nvcc (CUDA toolkit): {'found' if n.get('nvcc_available') else 'not found'}\")\n",
    "if n.get(\"nvcc_version_raw\"):\n",
    "    first = n[\"nvcc_version_raw\"].splitlines()[-1].strip()\n",
    "    print(f\"  nvcc version: {first}\")\n",
    "\n",
    "print(f\"\\n[PyTorch]\")\n",
    "t = report[\"pytorch\"]\n",
    "print(f\"  Installed: {t['installed']}\")\n",
    "if t[\"installed\"]:\n",
    "    print(f\"  Version: {t.get('version')}\")\n",
    "    print(f\"  CUDA available: {t.get('cuda_available')} | devices: {t.get('cuda_device_count')}\")\n",
    "    if t.get(\"gpus\"):\n",
    "        for g in t[\"gpus\"]:\n",
    "            print(f\"    GPU{g['index']}: {g['name']} | CC {g['compute_capability']} | {g['total_mem_gib']} GiB\")\n",
    "    print(f\"  torch CUDA (build): {t.get('torch_cuda_version')}\")\n",
    "    print(f\"  cuDNN available: {t.get('cudnn_available')} | version: {t.get('cudnn_version')}\")\n",
    "\n",
    "print(f\"\\n[TensorFlow]\")\n",
    "tf = report[\"tensorflow\"]\n",
    "print(f\"  Installed: {tf['installed']}\")\n",
    "if tf[\"installed\"]:\n",
    "    print(f\"  Version: {tf.get('version')}\")\n",
    "    print(f\"  GPUs seen: {tf.get('gpu_count')}\")\n",
    "    print(f\"  Build CUDA: {tf.get('cuda_version_build')} | Build cuDNN: {tf.get('cudnn_version_build')}\")\n",
    "\n",
    "print(f\"\\n[Key Python Packages]\")\n",
    "for k, v in report[\"packages\"].items():\n",
    "    print(f\"  {k}: {v or 'not installed'}\")\n",
    "\n",
    "# Save JSON report\n",
    "out_path = os.path.abspath(\"ml_env_report.json\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"\\nReport saved to: {out_path}\")\n",
    "print(\"\\nTip: If PyTorch says CUDA is unavailable but you have an NVIDIA GPU, check matching CUDA Toolkit/driver and install the proper PyTorch build for your CUDA version (or use the CPU build).\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94e8ff74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASIC ML ENVIRONMENT SUMMARY ===\n",
      "\n",
      "[CPU]\n",
      "Name: Intel64 Family 6 Model 151 Stepping 2, GenuineIntel\n",
      "Cores: 12 physical / 20 logical\n",
      "\n",
      "[RAM]\n",
      "Total: 31.76 GiB\n",
      "\n",
      "[GPU / CUDA]\n",
      "CUDA available: True\n",
      "GPU 0: NVIDIA GeForce RTX 5070 Ti\n",
      "  Memory: 15.92 GiB\n",
      "  Compute Capability: 12.0\n",
      "\n",
      "[PyTorch]\n",
      "Version: 2.7.0+cu128\n",
      "Built with CUDA: 12.8\n",
      "cuDNN: 90701\n",
      "\n",
      "[System]\n",
      "OS: Windows 11\n",
      "Python: 3.12.11\n",
      "\n",
      "====================================\n"
     ]
    }
   ],
   "source": [
    "# Basic ML Environment Summary (for Jupyter)\n",
    "# Shows CPU, RAM, GPU, CUDA, PyTorch, TensorFlow info\n",
    "\n",
    "import platform\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "def bytes_to_gib(n):\n",
    "    return round(n / (1024**3), 2)\n",
    "\n",
    "print(\"=== BASIC ML ENVIRONMENT SUMMARY ===\\n\")\n",
    "\n",
    "# ---- CPU ----\n",
    "print(\"[CPU]\")\n",
    "print(f\"Name: {platform.processor() or 'Unknown'}\")\n",
    "print(f\"Cores: {psutil.cpu_count(logical=False)} physical / {psutil.cpu_count(logical=True)} logical\")\n",
    "\n",
    "# ---- RAM ----\n",
    "vm = psutil.virtual_memory()\n",
    "print(f\"\\n[RAM]\")\n",
    "print(f\"Total: {bytes_to_gib(vm.total)} GiB\")\n",
    "# print(f\"Available: {bytes_to_gib(vm.available)} GiB\")\n",
    "\n",
    "# ---- GPU / CUDA ----\n",
    "print(\"\\n[GPU / CUDA]\")\n",
    "if torch.cuda.is_available():\n",
    "    device_count = torch.cuda.device_count()\n",
    "    print(f\"CUDA available: True\")\n",
    "    for i in range(device_count):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        props = torch.cuda.get_device_properties(i)\n",
    "        print(f\"GPU {i}: {name}\")\n",
    "        print(f\"  Memory: {bytes_to_gib(props.total_memory)} GiB\")\n",
    "        print(f\"  Compute Capability: {props.major}.{props.minor}\")\n",
    "else:\n",
    "    print(\"CUDA available: False\")\n",
    "\n",
    "# ---- PyTorch ----\n",
    "print(\"\\n[PyTorch]\")\n",
    "print(f\"Version: {torch.__version__}\")\n",
    "print(f\"Built with CUDA: {getattr(torch.version, 'cuda', 'None')}\")\n",
    "print(f\"cuDNN: {getattr(torch.backends.cudnn, 'version', lambda: None)()}\")\n",
    "\n",
    "# ---- System ----\n",
    "print(\"\\n[System]\")\n",
    "print(f\"OS: {platform.system()} {platform.release()}\")\n",
    "print(f\"Python: {platform.python_version()}\")\n",
    "\n",
    "print(\"\\n====================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
