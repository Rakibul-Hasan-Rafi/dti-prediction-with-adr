{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198129bf",
   "metadata": {},
   "source": [
    "# ESM Encoding for Unique Proteins in scope_onside_common_v3\n",
    "\n",
    "This notebook encodes amino acid sequences for unique proteins identified by `target_uniprot_id` using the Hugging Face ESM2 model and real pretrained weights. It follows the logic shown in `esm-hugginface.py`, using masked mean pooling over token embeddings, excluding special and padding tokens.\n",
    "\n",
    "Outline:\n",
    "- Set up environment and GPU\n",
    "- Load dataset (CSV/Parquet/JSON)\n",
    "- Deduplicate by `target_uniprot_id`\n",
    "- Configure and load the ESM model from Hugging Face\n",
    "- Clean and validate sequences\n",
    "- Batch tokenization and masked mean pooling\n",
    "- Batched inference with sliding-window chunking for long sequences\n",
    "- Persist embeddings with metadata (Parquet/NPZ)\n",
    "- Caching and resume logic\n",
    "- Quick sanity tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc8b19d",
   "metadata": {},
   "source": [
    "## 1) Set Up Environment and GPU\n",
    "\n",
    "- Installs are optional here; if missing, run the commands in the next cell.\n",
    "- Detect CUDA and choose dtype: float16 on GPU, float32 otherwise.\n",
    "- Print device info and CUDA memory if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596202df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6267aad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "CUDA device count: 1\n",
      "Current device: 0\n",
      "Device name: NVIDIA GeForce RTX 3050 Ti Laptop GPU\n",
      "CUDA memory (free/total GB): 0.0 / 4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# Device and dtype selection\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    dtype = torch.float16\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    dtype = torch.float32\n",
    "\n",
    "print('Device:', device)\n",
    "if device.type == 'cuda':\n",
    "    print('CUDA device count:', torch.cuda.device_count())\n",
    "    print('Current device:', torch.cuda.current_device())\n",
    "    print('Device name:', torch.cuda.get_device_name(0))\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    print('CUDA memory (free/total GB):', round(free/1024**3, 2), '/', round(total/1024**3, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8d283e",
   "metadata": {},
   "source": [
    "## 2) Load scope_onside_common_v3 Dataset\n",
    "\n",
    "- Set input path and optionally override column names\n",
    "- Auto-detect file format and load\n",
    "- Validate columns, drop missing, and log counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ecc00e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input path: d:\\BracU\\Thesis\\esm-encode\\scope_onside_common_v3.parquet\n",
      "ID column: target_uniprot_id\n",
      "Sequence column (preset): None\n",
      "Loaded rows: 34741\n",
      "Columns: ['drug_chembl_id', 'target_uniprot_id', 'label', 'smiles', 'sequence', 'molfile_3d', 'rxcui']\n",
      "Dropped 0 rows with null target_uniprot_id or sequence. Remaining: 34741\n"
     ]
    }
   ],
   "source": [
    "# Parameters\n",
    "INPUT_PATH = os.getenv('SCOPE_ONSIDE_PATH', r'd:\\BracU\\Thesis\\esm-encode\\scope_onside_common_v3.parquet')\n",
    "ID_COL = os.getenv('PROTEIN_ID_COL', 'target_uniprot_id')\n",
    "SEQ_COL = os.getenv('PROTEIN_SEQ_COL', None)  # try to auto-detect if None\n",
    "\n",
    "print('Input path:', INPUT_PATH)\n",
    "print('ID column:', ID_COL)\n",
    "print('Sequence column (preset):', SEQ_COL)\n",
    "\n",
    "# Auto-detect file extension and load\n",
    "path = Path(INPUT_PATH)\n",
    "if not path.exists():\n",
    "    raise FileNotFoundError(f'Input file not found: {path}')\n",
    "\n",
    "ext = path.suffix.lower()\n",
    "if ext in ['.parquet', '.pq']:\n",
    "    df = pd.read_parquet(path)\n",
    "elif ext in ['.csv', '.tsv']:\n",
    "    df = pd.read_csv(path) if ext == '.csv' else pd.read_csv(path, sep='\\t')\n",
    "elif ext in ['.json', '.jsonl']:\n",
    "    df = pd.read_json(path, lines=ext == '.jsonl')\n",
    "else:\n",
    "    raise ValueError(f'Unsupported file extension: {ext}')\n",
    "\n",
    "print('Loaded rows:', len(df))\n",
    "print('Columns:', list(df.columns))\n",
    "\n",
    "# Auto-detect sequence column if not provided\n",
    "if SEQ_COL is None:\n",
    "    candidate_cols = [\n",
    "        'amino_acid_sequence', 'sequence', 'seq', 'protein_sequence', 'aa_sequence', 'AA_sequence'\n",
    "    ]\n",
    "    for c in candidate_cols:\n",
    "        if c in df.columns:\n",
    "            SEQ_COL = c\n",
    "            break\n",
    "\n",
    "if SEQ_COL is None:\n",
    "    raise KeyError('Could not find a sequence column automatically. Set PROTEIN_SEQ_COL or edit SEQ_COL above.')\n",
    "\n",
    "missing_cols = [c for c in [ID_COL, SEQ_COL] if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise KeyError(f'Missing required columns: {missing_cols}')\n",
    "\n",
    "# Drop missing\n",
    "before = len(df)\n",
    "after_drop = df[[ID_COL, SEQ_COL]].dropna().shape[0]\n",
    "df = df.dropna(subset=[ID_COL, SEQ_COL])\n",
    "print(f'Dropped {before - len(df)} rows with null {ID_COL} or {SEQ_COL}. Remaining: {len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389456a7",
   "metadata": {},
   "source": [
    "## 3) Deduplicate Proteins by target_uniprot_id\n",
    "\n",
    "- Group by ID\n",
    "- Check for multiple sequences per ID and resolve conflicts deterministically\n",
    "- Prepare unique DataFrame with ID, sequence, and length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ec35a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique proteins: 2385\n",
      "  target_uniprot_id                                           sequence  count  \\\n",
      "0            O15245  MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...    150   \n",
      "1            P08183  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...    328   \n",
      "2            P35367  MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...     67   \n",
      "3            Q02763  MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...     31   \n",
      "4            Q12809  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...    161   \n",
      "\n",
      "   seq_len  \n",
      "0      554  \n",
      "1     1280  \n",
      "2      487  \n",
      "3     1124  \n",
      "4     1159  \n"
     ]
    }
   ],
   "source": [
    "# Resolve duplicates: keep the most frequent sequence per ID; if tie, keep first appearance\n",
    "\n",
    "def resolve_duplicates(df: pd.DataFrame, id_col: str, seq_col: str) -> pd.DataFrame:\n",
    "    # Count sequences per ID\n",
    "    grp = df.groupby([id_col, seq_col], as_index=False).size().rename(columns={'size': 'count'})\n",
    "    # For each ID, select sequence with max count; tie broken by first occurrence order in original df\n",
    "    grp['rank'] = grp.groupby(id_col)['count'].rank(method='first', ascending=False)\n",
    "    best = grp[grp['rank'] == 1].drop(columns=['rank']).copy()\n",
    "    # Merge back to ensure stable order based on first occurrence in df\n",
    "    first_order = df.drop_duplicates(subset=[id_col]).set_index(id_col).reset_index()[[id_col]]\n",
    "    uniq = first_order.merge(best, on=id_col, how='left')\n",
    "    uniq = uniq.rename(columns={seq_col: 'sequence'})\n",
    "    uniq['seq_len'] = uniq['sequence'].str.len()\n",
    "    return uniq\n",
    "\n",
    "uniq_df = resolve_duplicates(df, ID_COL, SEQ_COL)\n",
    "print('Unique proteins:', len(uniq_df))\n",
    "print(uniq_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0041429b",
   "metadata": {},
   "source": [
    "## 4) Configure and Load ESM Model from Hugging Face\n",
    "\n",
    "- Parameterize model name (default: facebook/esm2_t33_650M_UR50D)\n",
    "- Load tokenizer and model with pretrained weights\n",
    "- Move to device, set eval, and record max positions and hidden size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d027eb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: facebook/esm2_t33_650M_UR50D\n",
      "Batch size: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size: 1280\n",
      "Max positions: 1026\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = os.getenv('ESM_MODEL_NAME', 'facebook/esm2_t33_650M_UR50D')\n",
    "BATCH_SIZE = int(os.getenv('ESM_BATCH_SIZE', '4'))  # keep small for memory; you can tune\n",
    "CHUNK_OVERLAP = int(os.getenv('ESM_CHUNK_OVERLAP', '50'))  # tokens overlap for long seq chunking\n",
    "\n",
    "print('Model:', MODEL_NAME)\n",
    "print('Batch size:', BATCH_SIZE)\n",
    "\n",
    "# Load tokenizer and model from pretrained weights\n",
    "# Note: we use AutoModel to keep it generic and load true repo weights without hardcoding local files\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, do_lower_case=False)\n",
    "model = AutoModel.from_pretrained(MODEL_NAME)\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "hidden_size = getattr(model.config, 'hidden_size', None) or getattr(model.config, 'd_model', None)\n",
    "max_positions = getattr(model.config, 'max_position_embeddings', None)\n",
    "\n",
    "print('Hidden size:', hidden_size)\n",
    "print('Max positions:', max_positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dda3d7",
   "metadata": {},
   "source": [
    "## 5) Sequence Cleaning and Validation\n",
    "\n",
    "- Normalize: strip, uppercase, map non-standard to 'X'\n",
    "- Validate against tokenizer vocab where possible\n",
    "- Record a cleaned flag and track sequences exceeding model max length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617d457d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences needing chunking: 293\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target_uniprot_id",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "sequence",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "seq_len",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "cleaned_seq",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "was_cleaned",
         "rawType": "bool",
         "type": "boolean"
        },
        {
         "name": "orig_len",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "needs_chunk",
         "rawType": "bool",
         "type": "boolean"
        }
       ],
       "ref": "664fac3b-7f9f-435a-89c1-dd0d5b8e8890",
       "rows": [
        [
         "0",
         "O15245",
         "MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTPDHHCQSPGVAELSQRCGWSPAEELNYTVPGLGPAGEAFLGQCRRYEVDWNQSALSCVDPLASLATNRSHLPLGPCQDGWVYDTPGSSIVTEFNLVCADSWKLDLFQSCLNAGFLFGSLGVGYFADRFGRKLCLLGTVLVNAVSGVLMAFSPNYMSMLLFRLLQGLVSKGNWMAGYTLITEFVGSGSRRTVAIMYQMAFTVGLVALTGLAYALPHWRWLQLAVSLPTFLFLLYYWCVPESPRWLLSQKRNTEAIKIMDHIAQKNGKLPPADLKMLSLEEDVTEKLSPSFADLFRTPRLRKRTFILMYLWFTDSVLYQGLILHMGATSGNLYLDFLYSALVEIPGAFIALITIDRVGRIYPMAMSNLLAGAACLVMIFISPDLHWLNIIIMCVGRMGITIAIQMICLVNAELYPTFVRNLGVMVCSSLCDIGGIITPFIVFRLREVWQALPLILFAVLGLLAAGVTLLLPETKGVALPETMKDAENLGRKAKPKENTIYLKVQTSEPSGT",
         "150",
         "554",
         "MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTPDHHCQSPGVAELSQRCGWSPAEELNYTVPGLGPAGEAFLGQCRRYEVDWNQSALSCVDPLASLATNRSHLPLGPCQDGWVYDTPGSSIVTEFNLVCADSWKLDLFQSCLNAGFLFGSLGVGYFADRFGRKLCLLGTVLVNAVSGVLMAFSPNYMSMLLFRLLQGLVSKGNWMAGYTLITEFVGSGSRRTVAIMYQMAFTVGLVALTGLAYALPHWRWLQLAVSLPTFLFLLYYWCVPESPRWLLSQKRNTEAIKIMDHIAQKNGKLPPADLKMLSLEEDVTEKLSPSFADLFRTPRLRKRTFILMYLWFTDSVLYQGLILHMGATSGNLYLDFLYSALVEIPGAFIALITIDRVGRIYPMAMSNLLAGAACLVMIFISPDLHWLNIIIMCVGRMGITIAIQMICLVNAELYPTFVRNLGVMVCSSLCDIGGIITPFIVFRLREVWQALPLILFAVLGLLAAGVTLLLPETKGVALPETMKDAENLGRKAKPKENTIYLKVQTSEPSGT",
         "False",
         "554",
         "False"
        ],
        [
         "1",
         "P08183",
         "MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWLDKLYMVVGTLAAIIHGAGLPLMMLVFGEMTDIFANAGNLEDLMSNITNRSDINDTGFFMNLEEDMTRYAYYYSGIGAGVLVAAYIQVSFWCLAAGRQIHKIRKQFFHAIMRQEIGWFDVHDVGELNTRLTDDVSKINEGIGDKIGMFFQSMATFFTGFIVGFTRGWKLTLVILAISPVLGLSAAVWAKILSSFTDKELLAYAKAGAVAEEVLAAIRTVIAFGGQKKELERYNKNLEEAKRIGIKKAITANISIGAAFLLIYASYALAFWYGTTLVLSGEYSIGQVLTVFFSVLIGAFSVGQASPSIEAFANARGAAYEIFKIIDNKPSIDSYSKSGHKPDNIKGNLEFRNVHFSYPSRKEVKILKGLNLKVQSGQTVALVGNSGCGKSTTVQLMQRLYDPTEGMVSVDGQDIRTINVRFLREIIGVVSQEPVLFATTIAENIRYGRENVTMDEIEKAVKEANAYDFIMKLPHKFDTLVGERGAQLSGGQKQRIAIARALVRNPKILLLDEATSALDTESEAVVQVALDKARKGRTTIVIAHRLSTVRNADVIAGFDDGVIVEKGNHDELMKEKGIYFKLVTMQTAGNEVELENAADESKSEIDALEMSSNDSRSSLIRKRSTRRSVRGSQAQDRKLSTKEALDESIPPVSFWRIMKLNLTEWPYFVVGVFCAIINGGLQPAFAIIFSKIIGVFTRIDDPETKRQNSNLFSLLFLALGIISFITFFLQGFTFGKAGEILTKRLRYMVFRSMLRQDVSWFDDPKNTTGALTTRLANDAAQVKGAIGSRLAVITQNIANLGTGIIISFIYGWQLTLLLLAIVPIIAIAGVVEMKMLSGQALKDKKELEGSGKIATEAIENFRTVVSLTQEQKFEHMYAQSLQVPYRNSLRKAHIFGITFSFTQAMMYFSYAGCFRFGAYLVAHKLMSFEDVLLVFSAVVFGAMAVGQVSSFAPDYAKAKISAAHIIMIIEKTPLIDSYSTEGLMPNTLEGNVTFGEVVFNYPTRPDIPVLQGLSLEVKKGQTLALVGSSGCGKSTVVQLLERFYDPLAGKVLLDGKEIKRLNVQWLRAHLGIVSQEPILFDCSIAENIAYGDNSRVVSQEEIVRAAKEANIHAFIESLPNKYSTKVGDKGTQLSGGQKQRIAIARALVRQPHILLLDEATSALDTESEKVVQEALDKAREGRTCIVIAHRLSTIQNADLIVVFQNGRVKEHGTHQQLLAQKGIYFSMVSVQAGTKRQ",
         "328",
         "1280",
         "MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWLDKLYMVVGTLAAIIHGAGLPLMMLVFGEMTDIFANAGNLEDLMSNITNRSDINDTGFFMNLEEDMTRYAYYYSGIGAGVLVAAYIQVSFWCLAAGRQIHKIRKQFFHAIMRQEIGWFDVHDVGELNTRLTDDVSKINEGIGDKIGMFFQSMATFFTGFIVGFTRGWKLTLVILAISPVLGLSAAVWAKILSSFTDKELLAYAKAGAVAEEVLAAIRTVIAFGGQKKELERYNKNLEEAKRIGIKKAITANISIGAAFLLIYASYALAFWYGTTLVLSGEYSIGQVLTVFFSVLIGAFSVGQASPSIEAFANARGAAYEIFKIIDNKPSIDSYSKSGHKPDNIKGNLEFRNVHFSYPSRKEVKILKGLNLKVQSGQTVALVGNSGCGKSTTVQLMQRLYDPTEGMVSVDGQDIRTINVRFLREIIGVVSQEPVLFATTIAENIRYGRENVTMDEIEKAVKEANAYDFIMKLPHKFDTLVGERGAQLSGGQKQRIAIARALVRNPKILLLDEATSALDTESEAVVQVALDKARKGRTTIVIAHRLSTVRNADVIAGFDDGVIVEKGNHDELMKEKGIYFKLVTMQTAGNEVELENAADESKSEIDALEMSSNDSRSSLIRKRSTRRSVRGSQAQDRKLSTKEALDESIPPVSFWRIMKLNLTEWPYFVVGVFCAIINGGLQPAFAIIFSKIIGVFTRIDDPETKRQNSNLFSLLFLALGIISFITFFLQGFTFGKAGEILTKRLRYMVFRSMLRQDVSWFDDPKNTTGALTTRLANDAAQVKGAIGSRLAVITQNIANLGTGIIISFIYGWQLTLLLLAIVPIIAIAGVVEMKMLSGQALKDKKELEGSGKIATEAIENFRTVVSLTQEQKFEHMYAQSLQVPYRNSLRKAHIFGITFSFTQAMMYFSYAGCFRFGAYLVAHKLMSFEDVLLVFSAVVFGAMAVGQVSSFAPDYAKAKISAAHIIMIIEKTPLIDSYSTEGLMPNTLEGNVTFGEVVFNYPTRPDIPVLQGLSLEVKKGQTLALVGSSGCGKSTVVQLLERFYDPLAGKVLLDGKEIKRLNVQWLRAHLGIVSQEPILFDCSIAENIAYGDNSRVVSQEEIVRAAKEANIHAFIESLPNKYSTKVGDKGTQLSGGQKQRIAIARALVRQPHILLLDEATSALDTESEKVVQEALDKAREGRTCIVIAHRLSTIQNADLIVVFQNGRVKEHGTHQQLLAQKGIYFSMVSVQAGTKRQ",
         "False",
         "1280",
         "True"
        ],
        [
         "2",
         "P35367",
         "MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNLLVLYAVRSERKLHTVGNLYIVSLSVADLIVGAVVMPMNILYLLMSKWSLGRPLCLFWLSMDYVASTASIFSVFILCIDRYRSVQQPLRYLKYRTKTRASATILGAWFLSFLWVIPILGWNHFMQQTSVRREDKCETDFYDVTWFKVMTAIINFYLPTLLMLWFYAKIYKAVRQHCQHRELINRSLPSFSEIKLRPENPKGDAKKPGKESPWEVLKRKPKDAGGGSVLKSPSQTPKEMKSPVVFSQEDDREVDKLYCFPLDIVHMQAAAEGSSRDYVAVNRSHGQLKTDEQGLNTHGASEISEDQMLGDSQSFSRTDSDTTTETAPGKGKLRSGSNTGLDYIKFTWKRLRSHSRQYVSGLHMNRERKAAKQLGFIMAAFILCWIPYFIFFMVIAFCKNCCNEHLHMFTIWLGYINSTLNPLIYPLCNENFKKTFKRILHIRS",
         "67",
         "487",
         "MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNLLVLYAVRSERKLHTVGNLYIVSLSVADLIVGAVVMPMNILYLLMSKWSLGRPLCLFWLSMDYVASTASIFSVFILCIDRYRSVQQPLRYLKYRTKTRASATILGAWFLSFLWVIPILGWNHFMQQTSVRREDKCETDFYDVTWFKVMTAIINFYLPTLLMLWFYAKIYKAVRQHCQHRELINRSLPSFSEIKLRPENPKGDAKKPGKESPWEVLKRKPKDAGGGSVLKSPSQTPKEMKSPVVFSQEDDREVDKLYCFPLDIVHMQAAAEGSSRDYVAVNRSHGQLKTDEQGLNTHGASEISEDQMLGDSQSFSRTDSDTTTETAPGKGKLRSGSNTGLDYIKFTWKRLRSHSRQYVSGLHMNRERKAAKQLGFIMAAFILCWIPYFIFFMVIAFCKNCCNEHLHMFTIWLGYINSTLNPLIYPLCNENFKKTFKRILHIRS",
         "False",
         "487",
         "False"
        ],
        [
         "3",
         "Q02763",
         "MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIASGWRPHEPITIGRDFEALMNQHQDPLEVTQDVTREWAKKVVWKREKASKINGAYFCEGRVRGEAIRIRTMKMRQQASFLPATLTMTVDKGDNVNISFKKVLIKEEDAVIYKNGSFIHSVPRHEVPDILEVHLPHAQPQDAGVYSARYIGGNLFTSAFTRLIVRRCEAQKWGPECNHLCTACMNNGVCHEDTGECICPPGFMGRTCEKACELHTFGRTCKERCSGQEGCKSYVFCLPDPYGCSCATGWKGLQCNEACHPGFYGPDCKLRCSCNNGEMCDRFQGCLCSPGWQGLQCEREGIQRMTPKIVDLPDHIEVNSGKFNPICKASGWPLPTNEEMTLVKPDGTVLHPKDFNHTDHFSVAIFTIHRILPPDSGVWVCSVNTVAGMVEKPFNISVKVLPKPLNAPNVIDTGHNFAVINISSEPYFGDGPIKSKKLLYKPVNHYEAWQHIQVTNEIVTLNYLEPRTEYELCVQLVRRGEGGEGHPGPVRRFTTASIGLPPPRGLNLLPKSQTTLNLTWQPIFPSSEDDFYVEVERRSVQKSDQQNIKVPGNLTSVLLNNLHPREQYVVRARVNTKAQGEWSEDLTAWTLSDILPPQPENIKISNITHSSAVISWTILDGYSISSITIRYKVQGKNEDQHVDVKIKNATITQYQLKGLEPETAYQVDIFAENNIGSSNPAFSHELVTLPESQAPADLGGGKMLLIAILGSAGMTCLTVLLAFLIILQLKRANVQRRMAQAFQNVREEPAVQFNSGTLALNRKVKNNPDPTIYPVLDWNDIKFQDVIGEGNFGQVLKARIKKDGLRMDAAIKRMKEYASKDDHRDFAGELEVLCKLGHHPNIINLLGACEHRGYLYLAIEYAPHGNLLDFLRKSRVLETDPAFAIANSTASTLSSQQLLHFAADVARGMDYLSQKQFIHRDLAARNILVGENYVAKIADFGLSRGQEVYVKKTMGRLPVRWMAIESLNYSVYTTNSDVWSYGVLLWEIVSLGGTPYCGMTCAELYEKLPQGYRLEKPLNCDDEVYDLMRQCWREKPYERPSFAQILVSLNRMLEERKTYVNTTLYEKFTYAGIDCSAEEAA",
         "31",
         "1124",
         "MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIASGWRPHEPITIGRDFEALMNQHQDPLEVTQDVTREWAKKVVWKREKASKINGAYFCEGRVRGEAIRIRTMKMRQQASFLPATLTMTVDKGDNVNISFKKVLIKEEDAVIYKNGSFIHSVPRHEVPDILEVHLPHAQPQDAGVYSARYIGGNLFTSAFTRLIVRRCEAQKWGPECNHLCTACMNNGVCHEDTGECICPPGFMGRTCEKACELHTFGRTCKERCSGQEGCKSYVFCLPDPYGCSCATGWKGLQCNEACHPGFYGPDCKLRCSCNNGEMCDRFQGCLCSPGWQGLQCEREGIQRMTPKIVDLPDHIEVNSGKFNPICKASGWPLPTNEEMTLVKPDGTVLHPKDFNHTDHFSVAIFTIHRILPPDSGVWVCSVNTVAGMVEKPFNISVKVLPKPLNAPNVIDTGHNFAVINISSEPYFGDGPIKSKKLLYKPVNHYEAWQHIQVTNEIVTLNYLEPRTEYELCVQLVRRGEGGEGHPGPVRRFTTASIGLPPPRGLNLLPKSQTTLNLTWQPIFPSSEDDFYVEVERRSVQKSDQQNIKVPGNLTSVLLNNLHPREQYVVRARVNTKAQGEWSEDLTAWTLSDILPPQPENIKISNITHSSAVISWTILDGYSISSITIRYKVQGKNEDQHVDVKIKNATITQYQLKGLEPETAYQVDIFAENNIGSSNPAFSHELVTLPESQAPADLGGGKMLLIAILGSAGMTCLTVLLAFLIILQLKRANVQRRMAQAFQNVREEPAVQFNSGTLALNRKVKNNPDPTIYPVLDWNDIKFQDVIGEGNFGQVLKARIKKDGLRMDAAIKRMKEYASKDDHRDFAGELEVLCKLGHHPNIINLLGACEHRGYLYLAIEYAPHGNLLDFLRKSRVLETDPAFAIANSTASTLSSQQLLHFAADVARGMDYLSQKQFIHRDLAARNILVGENYVAKIADFGLSRGQEVYVKKTMGRLPVRWMAIESLNYSVYTTNSDVWSYGVLLWEIVSLGGTPYCGMTCAELYEKLPQGYRLEKPLNCDDEVYDLMRQCWREKPYERPSFAQILVSLNRMLEERKTYVNTTLYEKFTYAGIDCSAEEAA",
         "False",
         "1124",
         "True"
        ],
        [
         "4",
         "Q12809",
         "MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCNDGFCELCGYSRAEVMQRPCTCDFLHGPRTQRRAAAQIAQALLGAEERKVEIAFYRKDGSCFLCLVDVVPVKNEDGAVIMFILNFEVVMEKDMVGSPAHDTNHRGPPTSWLAPGRAKTFRLKLPALLALTARESSVRSGGAGGAGAPGAVVVDVDLTPAAPSSESLALDEVTAMDNHVAGLGPAEERRALVGPGSPPRSAPGQLPSPRAHSLNPDASGSSCSLARTRSRESCASVRRASSADDIEAMRAGVLPPPPRHASTGAMHPLRSGLLNSTSDSDLVRYRTISKIPQITLNFVDLKGDPFLASPTSDREIIAPKIKERTHNVTEKVTQVLSLGADVLPEYKLQAPRIHRWTILHYSPFKAVWDWLILLLVIYTAVFTPYSAAFLLKETEEGPPATECGYACQPLAVVDLIVDIMFIVDILINFRTTYVNANEEVVSHPGRIAVHYFKGWFLIDMVAAIPFDLLIFGSGSEELIGLLKTARLLRLVRVARKLDRYSEYGAAVLFLLMCTFALIAHWLACIWYAIGNMEQPHMDSRIGWLHNLGDQIGKPYNSSGLGGPSIKDKYVTALYFTFSSLTSVGFGNVSPNTNSEKIFSICVMLIGSLMYASIFGNVSAIIQRLYSGTARYHTQMLRVREFIRFHQIPNPLRQRLEEYFQHAWSYTNGIDMNAVLKGFPECLQADICLHLNRSLLQHCKPFRGATKGCLRALAMKFKTTHAPPGDTLVHAGDLLTALYFISRGSIEILRGDVVVAILGKNDIFGEPLNLYARPGKSNGDVRALTYCDLHKIHRDDLLEVLDMYPEFSDHFWSSLEITFNLRDTNMIPGSPGSTELEGGFSRQRKRKLSFRRRTDKDTEQPGEVSALGPGRAGAGPSSRGRPGGPWGESPSSGPSSPESSEDEGPGRSSSPLRLVPFSSPRPPGEPPGGEPLMEDCEKSSDTCNPLSGAFSGVSNIFSFWGDSRGRQYQELPRCPAPTPSLLNIPLSSPGRRPRGDVESRLDALQRQLNRLETRLSADMATVLQLLQRQMTLVPPAYSAVTTPGPGPTSTSPLLPVSPLPTLTLDSLSQVSQFMACEELPPGAPELPQEGPTRRLSLPGQLGALTSQPLHRHGSDPGS",
         "161",
         "1159",
         "MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCNDGFCELCGYSRAEVMQRPCTCDFLHGPRTQRRAAAQIAQALLGAEERKVEIAFYRKDGSCFLCLVDVVPVKNEDGAVIMFILNFEVVMEKDMVGSPAHDTNHRGPPTSWLAPGRAKTFRLKLPALLALTARESSVRSGGAGGAGAPGAVVVDVDLTPAAPSSESLALDEVTAMDNHVAGLGPAEERRALVGPGSPPRSAPGQLPSPRAHSLNPDASGSSCSLARTRSRESCASVRRASSADDIEAMRAGVLPPPPRHASTGAMHPLRSGLLNSTSDSDLVRYRTISKIPQITLNFVDLKGDPFLASPTSDREIIAPKIKERTHNVTEKVTQVLSLGADVLPEYKLQAPRIHRWTILHYSPFKAVWDWLILLLVIYTAVFTPYSAAFLLKETEEGPPATECGYACQPLAVVDLIVDIMFIVDILINFRTTYVNANEEVVSHPGRIAVHYFKGWFLIDMVAAIPFDLLIFGSGSEELIGLLKTARLLRLVRVARKLDRYSEYGAAVLFLLMCTFALIAHWLACIWYAIGNMEQPHMDSRIGWLHNLGDQIGKPYNSSGLGGPSIKDKYVTALYFTFSSLTSVGFGNVSPNTNSEKIFSICVMLIGSLMYASIFGNVSAIIQRLYSGTARYHTQMLRVREFIRFHQIPNPLRQRLEEYFQHAWSYTNGIDMNAVLKGFPECLQADICLHLNRSLLQHCKPFRGATKGCLRALAMKFKTTHAPPGDTLVHAGDLLTALYFISRGSIEILRGDVVVAILGKNDIFGEPLNLYARPGKSNGDVRALTYCDLHKIHRDDLLEVLDMYPEFSDHFWSSLEITFNLRDTNMIPGSPGSTELEGGFSRQRKRKLSFRRRTDKDTEQPGEVSALGPGRAGAGPSSRGRPGGPWGESPSSGPSSPESSEDEGPGRSSSPLRLVPFSSPRPPGEPPGGEPLMEDCEKSSDTCNPLSGAFSGVSNIFSFWGDSRGRQYQELPRCPAPTPSLLNIPLSSPGRRPRGDVESRLDALQRQLNRLETRLSADMATVLQLLQRQMTLVPPAYSAVTTPGPGPTSTSPLLPVSPLPTLTLDSLSQVSQFMACEELPPGAPELPQEGPTRRLSLPGQLGALTSQPLHRHGSDPGS",
         "False",
         "1159",
         "True"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target_uniprot_id</th>\n",
       "      <th>sequence</th>\n",
       "      <th>count</th>\n",
       "      <th>seq_len</th>\n",
       "      <th>cleaned_seq</th>\n",
       "      <th>was_cleaned</th>\n",
       "      <th>orig_len</th>\n",
       "      <th>needs_chunk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>O15245</td>\n",
       "      <td>MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...</td>\n",
       "      <td>150</td>\n",
       "      <td>554</td>\n",
       "      <td>MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...</td>\n",
       "      <td>False</td>\n",
       "      <td>554</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>P08183</td>\n",
       "      <td>MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...</td>\n",
       "      <td>328</td>\n",
       "      <td>1280</td>\n",
       "      <td>MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...</td>\n",
       "      <td>False</td>\n",
       "      <td>1280</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>P35367</td>\n",
       "      <td>MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...</td>\n",
       "      <td>67</td>\n",
       "      <td>487</td>\n",
       "      <td>MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...</td>\n",
       "      <td>False</td>\n",
       "      <td>487</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q02763</td>\n",
       "      <td>MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...</td>\n",
       "      <td>31</td>\n",
       "      <td>1124</td>\n",
       "      <td>MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...</td>\n",
       "      <td>False</td>\n",
       "      <td>1124</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q12809</td>\n",
       "      <td>MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...</td>\n",
       "      <td>161</td>\n",
       "      <td>1159</td>\n",
       "      <td>MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...</td>\n",
       "      <td>False</td>\n",
       "      <td>1159</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  target_uniprot_id                                           sequence  count  \\\n",
       "0            O15245  MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...    150   \n",
       "1            P08183  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...    328   \n",
       "2            P35367  MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...     67   \n",
       "3            Q02763  MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...     31   \n",
       "4            Q12809  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...    161   \n",
       "\n",
       "   seq_len                                        cleaned_seq  was_cleaned  \\\n",
       "0      554  MPTVDDILEQVGESGWFQKQAFLILCLLSAAFAPICVGIVFLGFTP...        False   \n",
       "1     1280  MDLEGDRNGGAKKKNFFKLNNKSEKDKKEKKPTVSVFSMFRYSNWL...        False   \n",
       "2      487  MSLPNSSCLLEDKMCEGNKTTMASPQLMPLVVVLSTICLVTVGLNL...        False   \n",
       "3     1124  MDSLASLVLCGVSLLLSGTVEGAMDLILINSLPLVSDAETSLTCIA...        False   \n",
       "4     1159  MPVRRGHVAPQNTFLDTIIRKFEGQSRKFIIANARVENCAVIYCND...        False   \n",
       "\n",
       "   orig_len  needs_chunk  \n",
       "0       554        False  \n",
       "1      1280         True  \n",
       "2       487        False  \n",
       "3      1124         True  \n",
       "4      1159         True  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Allowed amino acids per UniProt standard + X, B, Z, U, O\n",
    "STANDARD_AA = set(list('ACDEFGHIKLMNPQRSTVWY'))\n",
    "ALLOWED_AA = set(list('ACDEFGHIKLMNPQRSTVWYBXZOU'))\n",
    "\n",
    "\n",
    "def clean_sequence(seq: str) -> Tuple[str, bool, int]:\n",
    "    s = (seq or '').strip().upper()\n",
    "    cleaned = False\n",
    "    out_chars = []\n",
    "    for ch in s:\n",
    "        if ch in ALLOWED_AA:\n",
    "            if ch not in STANDARD_AA:\n",
    "                cleaned = True\n",
    "                out_chars.append('X') if ch in {'B', 'Z'} else out_chars.append(ch if ch in STANDARD_AA else 'X')\n",
    "            else:\n",
    "                out_chars.append(ch)\n",
    "        else:\n",
    "            cleaned = True\n",
    "            out_chars.append('X')\n",
    "    return ''.join(out_chars), cleaned, len(s)\n",
    "\n",
    "uniq_df['cleaned_seq'], uniq_df['was_cleaned'], uniq_df['orig_len'] = zip(*uniq_df['sequence'].map(clean_sequence))\n",
    "\n",
    "# Length validation against model max positions (tokenizer adds special tokens)\n",
    "if max_positions is None:\n",
    "    # Fallback: most ESM2 variants support ~1022 tokens including specials\n",
    "    max_positions = 1022\n",
    "\n",
    "# We will mark sequences that likely need chunking if their raw length exceeds (max_positions - specials)\n",
    "# HF tokenizer typically adds 2 specials (CLS/EOS)\n",
    "specials = 2\n",
    "len_limit = max_positions - specials\n",
    "uniq_df['needs_chunk'] = uniq_df['cleaned_seq'].str.len() > len_limit\n",
    "\n",
    "print('Sequences needing chunking:', int(uniq_df['needs_chunk'].sum()))\n",
    "uniq_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a38ae7",
   "metadata": {},
   "source": [
    "## 6) Batch Tokenization and Masked Mean Pooling\n",
    "\n",
    "Implement helper functions to:\n",
    "- tokenize a list of sequences with padding/truncation\n",
    "- forward pass and masked mean pool over last_hidden_state, excluding padding and special tokens using attention_mask\n",
    "- return pooled vectors and flags indicating any truncation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1210ab9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_mean_pool(last_hidden_state: torch.Tensor, input_ids: torch.Tensor, attention_mask: torch.Tensor, tokenizer) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    # last_hidden_state: [B, T, H]\n",
    "    # input_ids: [B, T]\n",
    "    # attention_mask: [B, T]\n",
    "    # Exclude CLS/EOS tokens using input_ids and tokenizer special IDs\n",
    "    cls_id = tokenizer.cls_token_id\n",
    "    eos_id = tokenizer.eos_token_id\n",
    "    special_mask = (input_ids != cls_id) & (input_ids != eos_id)\n",
    "    valid_mask = attention_mask.bool() & special_mask\n",
    "    # Expand to match hidden dim\n",
    "    valid_mask_f = valid_mask.unsqueeze(-1)  # [B, T, 1]\n",
    "    masked = last_hidden_state * valid_mask_f\n",
    "    # Sum and divide by count\n",
    "    sums = masked.sum(dim=1)  # [B, H]\n",
    "    counts = valid_mask_f.sum(dim=1).clamp(min=1)  # [B, 1]\n",
    "    pooled = sums / counts\n",
    "    # A truncation flag can be inferred from tokenizer outputs if available\n",
    "    return pooled, valid_mask.sum(dim=1)\n",
    "\n",
    "\n",
    "def encode_batch(seq_list: List[str]) -> Tuple[torch.Tensor, np.ndarray]:\n",
    "    # Tokenize with padding/truncation; rely on tokenizer/model limits\n",
    "    enc = tokenizer(\n",
    "        seq_list,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        add_special_tokens=True,\n",
    "    ).to(device)\n",
    "\n",
    "    # Choose autocast context based on device\n",
    "    from contextlib import nullcontext\n",
    "    autocast_ctx = torch.cuda.amp.autocast(dtype=dtype) if device.type == 'cuda' else nullcontext()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with autocast_ctx:\n",
    "            outputs = model(**enc)\n",
    "            hs = outputs.last_hidden_state  # [B, T, H]\n",
    "    pooled, valid_counts = masked_mean_pool(hs, enc['input_ids'], enc['attention_mask'], tokenizer)\n",
    "    pooled = pooled.detach().float().cpu()\n",
    "\n",
    "    # Detect truncation: if any sequence length equals the model max (via tokenizer) and original > len_limit\n",
    "    was_truncated = []\n",
    "    for i, s in enumerate(seq_list):\n",
    "        # Token length includes specials; check if attention_mask sum equals tokenizer.model_max_length or close to it\n",
    "        attn_len = int(enc['attention_mask'][i].sum().item())\n",
    "        trunc = attn_len >= (max_positions or attn_len) and len(s) > len_limit\n",
    "        was_truncated.append(trunc)\n",
    "    return pooled, np.array(was_truncated, dtype=bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffeb007c",
   "metadata": {},
   "source": [
    "## 7) Batched Inference with Chunking for Long Sequences\n",
    "\n",
    "- Iterate in batches with a progress bar\n",
    "- For sequences exceeding model max, use sliding-window chunking and average pooled embeddings\n",
    "- Collect results as we go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27639cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_windows(seq: str, max_len: int, overlap: int) -> List[str]:\n",
    "    if len(seq) <= max_len:\n",
    "        return [seq]\n",
    "    windows = []\n",
    "    start = 0\n",
    "    while start < len(seq):\n",
    "        end = min(start + max_len, len(seq))\n",
    "        windows.append(seq[start:end])\n",
    "        if end == len(seq):\n",
    "            break\n",
    "        start = end - overlap\n",
    "        if start < 0:\n",
    "            start = 0\n",
    "    return windows\n",
    "\n",
    "\n",
    "def encode_sequences(ids: List[str], seqs: List[str], batch_size: int = BATCH_SIZE) -> Dict[str, Dict]:\n",
    "    results = {}\n",
    "    for i in tqdm(range(0, len(seqs), batch_size), desc='Encoding batches'):\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        batch_seqs = seqs[i:i+batch_size]\n",
    "\n",
    "        to_encode = []\n",
    "        chunk_map = []  # (orig_index, num_chunks)\n",
    "        for j, s in enumerate(batch_seqs):\n",
    "            if len(s) > len_limit:\n",
    "                chunks = sliding_windows(s, len_limit, CHUNK_OVERLAP)\n",
    "                to_encode.extend(chunks)\n",
    "                chunk_map.append((j, len(chunks)))\n",
    "            else:\n",
    "                to_encode.append(s)\n",
    "                chunk_map.append((j, 1))\n",
    "\n",
    "        # Encode all pieces in this batch (may be more than batch_size due to chunking)\n",
    "        pooled, was_trunc = encode_batch(to_encode)\n",
    "\n",
    "        # Aggregate back to original sequences\n",
    "        idx = 0\n",
    "        agg_vectors = []\n",
    "        agg_trunc = []\n",
    "        for (j, n_chunks) in chunk_map:\n",
    "            vecs = pooled[idx:idx+n_chunks]\n",
    "            flags = was_trunc[idx:idx+n_chunks]\n",
    "            idx += n_chunks\n",
    "            # Average chunk embeddings\n",
    "            agg_vec = vecs.mean(dim=0)\n",
    "            agg_vectors.append(agg_vec)\n",
    "            agg_trunc.append(bool(flags.any()) or (n_chunks > 1))\n",
    "\n",
    "        for bid, vec, trunc_flag, seq in zip(batch_ids, agg_vectors, agg_trunc, batch_seqs):\n",
    "            results[bid] = {\n",
    "                'embedding': vec.numpy(),\n",
    "                'seq_len': len(seq),\n",
    "                'was_truncated': trunc_flag,\n",
    "            }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "37ef9088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output dir: d:\\BracU\\Thesis\\esm-encode\\esm_outputs\n"
     ]
    }
   ],
   "source": [
    "OUT_DIR = Path(os.getenv('ESM_OUT_DIR', r'd:\\BracU\\Thesis\\esm-encode\\esm_outputs'))\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PARQUET_PATH = OUT_DIR / 'esm2_embeddings.parquet'\n",
    "NPZ_PATH = OUT_DIR / 'esm2_embeddings.npz'\n",
    "NPZ_META_CSV = OUT_DIR / 'esm2_embeddings_meta.csv'\n",
    "CHECKPOINT_PATH = OUT_DIR / 'checkpoint_ids.json'\n",
    "\n",
    "print('Output dir:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89da5c31",
   "metadata": {},
   "source": [
    "## 9) Caching and Resume Logic\n",
    "\n",
    "- Load IDs from previous Parquet or checkpoint (if any)\n",
    "- Skip already-encoded IDs\n",
    "- Periodically checkpoint during long runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d3389339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple checkpointing utilities\n",
    "\n",
    "def load_completed_ids(parquet_path: Path, checkpoint_path: Path) -> set:\n",
    "    completed = set()\n",
    "    if parquet_path.exists():\n",
    "        try:\n",
    "            tmp = pd.read_parquet(parquet_path, columns=[ID_COL])\n",
    "            completed.update(tmp[ID_COL].astype(str).tolist())\n",
    "            print('Loaded completed IDs from Parquet:', len(completed))\n",
    "        except Exception as e:\n",
    "            print('Parquet exists but could not be read for checkpoints:', e)\n",
    "    if checkpoint_path.exists():\n",
    "        try:\n",
    "            with open(checkpoint_path, 'r', encoding='utf-8') as f:\n",
    "                completed.update(json.load(f))\n",
    "            print('Loaded completed IDs from checkpoint:', len(completed))\n",
    "        except Exception as e:\n",
    "            print('Checkpoint exists but could not be read:', e)\n",
    "    return completed\n",
    "\n",
    "\n",
    "def save_checkpoint_ids(ids_done: set, checkpoint_path: Path):\n",
    "    with open(checkpoint_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(sorted(list(ids_done)), f)\n",
    "    print('Checkpoint saved with', len(ids_done), 'IDs ->', checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "448bf2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of how to use checkpoints (for long runs, integrate into encode_sequences loop if desired)\n",
    "completed_ids = load_completed_ids(PARQUET_PATH, CHECKPOINT_PATH)\n",
    "if completed_ids:\n",
    "    mask = ~uniq_df[ID_COL].astype(str).isin(completed_ids)\n",
    "    to_process = uniq_df[mask].copy()\n",
    "    print('Skipping', (~mask).sum(), 'already encoded IDs. To process:', len(to_process))\n",
    "else:\n",
    "    to_process = uniq_df.copy()\n",
    "\n",
    "# If you want to use incremental saving, you can iterate batches manually\n",
    "# For simplicity above we encoded all at once; for very large sets, adapt below pattern:\n",
    "# ids = to_process[ID_COL].tolist()\n",
    "# seqs = to_process['cleaned_seq'].tolist()\n",
    "# batch_done = set(completed_ids)\n",
    "# for i in range(0, len(seqs), BATCH_SIZE):\n",
    "#     ... encode and append to Parquet (use pyarrow to append) ...\n",
    "#     batch_done.update(batch_ids)\n",
    "#     save_checkpoint_ids(batch_done, CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67053f7",
   "metadata": {},
   "source": [
    "## 8) Persist Embeddings with Metadata\n",
    "\n",
    "- Save to Parquet and optionally NPZ\n",
    "- Parameterize output paths\n",
    "- Log shapes and sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d12a5d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eb49d94a06c449192642fd40fbf499c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding batches:   0%|          | 0/597 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "C:\\Users\\radit\\AppData\\Local\\Temp\\ipykernel_13372\\3171867018.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_ctx = torch.cuda.amp.autocast(dtype=dtype) if device.type == 'cuda' else nullcontext()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape (new batch): (2385, 1280)\n",
      "Wrote new Parquet: d:\\BracU\\Thesis\\esm-encode\\esm_outputs\\esm2_embeddings.parquet rows: 2385\n",
      "Saved NPZ: d:\\BracU\\Thesis\\esm-encode\\esm_outputs\\esm2_embeddings.npz and meta CSV: d:\\BracU\\Thesis\\esm-encode\\esm_outputs\\esm2_embeddings_meta.csv\n",
      "Checkpoint saved with 2385 IDs -> d:\\BracU\\Thesis\\esm-encode\\esm_outputs\\checkpoint_ids.json\n"
     ]
    }
   ],
   "source": [
    "def save_parquet(ids: List[str], seq_lens: List[int], was_cleaned: List[bool], was_trunc: List[bool], embeds: np.ndarray, path: Path):\n",
    "    table = pd.DataFrame({\n",
    "        ID_COL: ids,\n",
    "        'seq_len': seq_lens,\n",
    "        'was_cleaned': was_cleaned,\n",
    "        'was_truncated': was_trunc,\n",
    "        'embedding': list(embeds.astype('float32')),\n",
    "    })\n",
    "    table.to_parquet(path, index=False)\n",
    "    print('Saved Parquet:', path, 'rows:', len(table))\n",
    "\n",
    "\n",
    "def save_npz_from_parquet(parquet_path: Path, npz_path: Path, meta_csv: Path):\n",
    "    df_all = pd.read_parquet(parquet_path)\n",
    "    # Convert list column to array\n",
    "    embeds = np.stack(df_all['embedding'].to_list(), axis=0).astype('float32')\n",
    "    np.savez_compressed(npz_path, embeddings=embeds)\n",
    "    meta = df_all[[ID_COL, 'seq_len', 'was_cleaned', 'was_truncated']].copy()\n",
    "    meta.to_csv(meta_csv, index=False)\n",
    "    print('Saved NPZ:', npz_path, 'and meta CSV:', meta_csv)\n",
    "\n",
    "# Run encoding for all sequences that still need processing\n",
    "source_df = to_process if 'to_process' in globals() else uniq_df\n",
    "ids = source_df[ID_COL].astype(str).tolist()\n",
    "seqs = source_df['cleaned_seq'].tolist()\n",
    "was_cleaned = source_df['was_cleaned'].astype(bool).tolist()\n",
    "seq_lens = source_df['cleaned_seq'].str.len().tolist()\n",
    "\n",
    "if len(ids) == 0:\n",
    "    print('Nothing to process. All IDs appear to be encoded already.')\n",
    "else:\n",
    "    results = encode_sequences(ids, seqs, batch_size=BATCH_SIZE)\n",
    "\n",
    "    # Assemble arrays in original order\n",
    "    embeddings = np.stack([results[i]['embedding'] for i in ids], axis=0)\n",
    "    was_truncated = [results[i]['was_truncated'] for i in ids]\n",
    "\n",
    "    print('Embeddings shape (new batch):', embeddings.shape)\n",
    "\n",
    "    # Prepare new table\n",
    "    df_new = pd.DataFrame({\n",
    "        ID_COL: ids,\n",
    "        'seq_len': seq_lens,\n",
    "        'was_cleaned': was_cleaned,\n",
    "        'was_truncated': was_truncated,\n",
    "        'embedding': list(embeddings.astype('float32')),\n",
    "    })\n",
    "\n",
    "    # Merge with existing Parquet if present\n",
    "    if PARQUET_PATH.exists():\n",
    "        try:\n",
    "            df_old = pd.read_parquet(PARQUET_PATH)\n",
    "            df_all = pd.concat([df_old, df_new], ignore_index=True)\n",
    "            df_all = df_all.drop_duplicates(subset=[ID_COL], keep='last')\n",
    "            df_all.to_parquet(PARQUET_PATH, index=False)\n",
    "            print('Updated Parquet with merged results:', PARQUET_PATH, 'rows:', len(df_all))\n",
    "        except Exception as e:\n",
    "            print('Failed to merge with existing Parquet, writing new only:', e)\n",
    "            df_new.to_parquet(PARQUET_PATH, index=False)\n",
    "    else:\n",
    "        df_new.to_parquet(PARQUET_PATH, index=False)\n",
    "        print('Wrote new Parquet:', PARQUET_PATH, 'rows:', len(df_new))\n",
    "\n",
    "    # Refresh NPZ + CSV metadata from the Parquet\n",
    "    save_npz_from_parquet(PARQUET_PATH, NPZ_PATH, NPZ_META_CSV)\n",
    "\n",
    "    # Update checkpoint with IDs present in Parquet\n",
    "    try:\n",
    "        all_ids = pd.read_parquet(PARQUET_PATH, columns=[ID_COL])[ID_COL].astype(str)\n",
    "        save_checkpoint_ids(set(all_ids), CHECKPOINT_PATH)\n",
    "    except Exception as e:\n",
    "        print('Could not update checkpoint from Parquet:', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6b0c4d",
   "metadata": {},
   "source": [
    "## 10) Quick Sanity Tests\n",
    "\n",
    "Run a few assertions on a small subset to verify embedding shape, non-zero norms, and similarity between identical sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4696da9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "728e54f759444006abd892122fe5de3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Encoding batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radit\\AppData\\Local\\Temp\\ipykernel_13372\\3171867018.py:33: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  autocast_ctx = torch.cuda.amp.autocast(dtype=dtype) if device.type == 'cuda' else nullcontext()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset embeddings shape: (2, 1280)\n"
     ]
    }
   ],
   "source": [
    "# Take 2-3 sequences (or repeat one) and test shapes\n",
    "subset = uniq_df.head(2).copy()\n",
    "if len(subset) == 1:\n",
    "    subset = pd.concat([subset, subset], ignore_index=True)\n",
    "\n",
    "ids_s = subset[ID_COL].tolist()\n",
    "seqs_s = subset['cleaned_seq'].tolist()\n",
    "\n",
    "embs = encode_sequences(ids_s, seqs_s, batch_size=2)\n",
    "arr = np.stack([embs[i]['embedding'] for i in ids_s])\n",
    "print('Subset embeddings shape:', arr.shape)\n",
    "\n",
    "# Check hidden size\n",
    "assert arr.shape[1] == hidden_size, f\"Expected hidden size {hidden_size}, got {arr.shape[1]}\"\n",
    "\n",
    "# Non-zero norms\n",
    "norms = np.linalg.norm(arr, axis=1)\n",
    "assert np.all(norms > 0), 'Found zero-norm embedding'\n",
    "\n",
    "# Similarity test for identical sequences (if any)\n",
    "if seqs_s[0] == seqs_s[1]:\n",
    "    from numpy.linalg import norm\n",
    "    cos = float(np.dot(arr[0], arr[1]) / (norm(arr[0]) * norm(arr[1]) + 1e-9))\n",
    "    print('Cosine similarity between identical sequences:', cos)\n",
    "    assert cos > 0.99, 'Identical sequences should produce near-identical embeddings'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dti-adr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
